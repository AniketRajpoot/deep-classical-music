{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUngslCPKq0R"
      },
      "source": [
        "## **Initialization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63UfJfdTemNq",
        "outputId": "7048de16-da00-46b3-e020-3dcb7b3b3040"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: better_exceptions in /usr/local/lib/python3.10/dist-packages (0.3.3)\n",
            "Requirement already satisfied: av in /usr/local/lib/python3.10/dist-packages (11.0.0)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (0.12.1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile) (2.21)\n"
          ]
        }
      ],
      "source": [
        "!pip install better_exceptions\n",
        "!pip install av\n",
        "!pip install soundfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rF_49fEx-zT2",
        "outputId": "d6a7e0e4-4771-48d1-92b8-9fa3ef3637ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Feb  4 14:35:36 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jkr-0_TgaT5"
      },
      "outputs": [],
      "source": [
        "import fnmatch\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import threading\n",
        "import librosa,librosa.display\n",
        "import numpy as npx\n",
        "import tensorflow as tf\n",
        "from six.moves import xrange\n",
        "import better_exceptions\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import numpy as np\n",
        "import av\n",
        "import torch as t\n",
        "import tqdm\n",
        "import soundfile\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from time import sleep\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G47XZ52FvAsT",
        "outputId": "794594d7-4a3d-4801-e10d-ba6fea5fad79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ek--x71VMTHU"
      },
      "source": [
        "# **Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnPK3HUI52RF"
      },
      "outputs": [],
      "source": [
        "def audio_preprocess(x, hps):\n",
        "    # Extra layer in case we want to experiment with different preprocessing\n",
        "    # For two channel, blend randomly into mono (standard is .5 left, .5 right)\n",
        "\n",
        "    # x: NTC\n",
        "    x = x.float()\n",
        "    if x.shape[-1]==2:\n",
        "        if hps.aug_blend:\n",
        "            mix=t.rand((x.shape[0],1), device=x.device) #np.random.rand()\n",
        "        else:\n",
        "            mix = 0.5\n",
        "        x=(mix*x[:,:,0]+(1-mix)*x[:,:,1])\n",
        "    elif x.shape[-1]==1:\n",
        "        x=x[:,:,0]\n",
        "    else:\n",
        "        assert False, f'Expected channels {hps.channels}. Got unknown {x.shape[-1]} channels'\n",
        "\n",
        "    # x: NT -> NTC\n",
        "    x = x.unsqueeze(2)\n",
        "    return x\n",
        "\n",
        "def audio_postprocess(x, hps):\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXGNx5-Denu2"
      },
      "outputs": [],
      "source": [
        "def get_duration_sec(file, cache=False):\n",
        "    try:\n",
        "        with open(file + '.dur', 'r') as f:\n",
        "            duration = float(f.readline().strip('\\n'))\n",
        "        return duration\n",
        "    except:\n",
        "        container = av.open(file)\n",
        "        audio = container.streams.get(audio=0)[0]\n",
        "        duration = audio.duration * float(audio.time_base)\n",
        "        if cache:\n",
        "            with open(file + '.dur', 'w') as f:\n",
        "                f.write(str(duration) + '\\n')\n",
        "        return duration\n",
        "\n",
        "def load_audio(file, sr, offset, duration, resample=True, approx=False, time_base='samples', check_duration=True):\n",
        "    if time_base == 'sec':\n",
        "        offset = offset * sr\n",
        "        duration = duration * sr\n",
        "    # Loads at target sr, stereo channels, seeks from offset, and stops after duration\n",
        "    container = av.open(file)\n",
        "    audio = container.streams.get(audio=0)[0] # Only first audio stream\n",
        "    audio_duration = audio.duration * float(audio.time_base)\n",
        "    if approx:\n",
        "        if offset + duration > audio_duration*sr:\n",
        "            # Move back one window. Cap at audio_duration\n",
        "            offset = np.min(audio_duration*sr - duration, offset - duration)\n",
        "    else:\n",
        "        if check_duration:\n",
        "            assert offset + duration <= audio_duration*sr, f'End {offset + duration} beyond duration {audio_duration*sr}'\n",
        "    if resample:\n",
        "        resampler = av.AudioResampler(format='fltp',layout='stereo', rate=sr)\n",
        "    else:\n",
        "        assert sr == audio.sample_rate\n",
        "    offset = int(offset / sr / float(audio.time_base)) #int(offset / float(audio.time_base)) # Use units of time_base for seeking\n",
        "    duration = int(duration) #duration = int(duration * sr) # Use units of time_out ie 1/sr for returning\n",
        "    sig = np.zeros((2, duration), dtype=np.float32)\n",
        "    container.seek(offset, stream=audio)\n",
        "    total_read = 0\n",
        "    for frame in container.decode(audio=0): # Only first audio stream\n",
        "        if resample:\n",
        "            frame.pts = None\n",
        "            frame = resampler.resample(frame)\n",
        "        frame = frame.to_ndarray(format='fltp') # Convert to floats and not int16\n",
        "        read = frame.shape[-1]\n",
        "        if total_read + read > duration:\n",
        "            read = duration - total_read\n",
        "        sig[:, total_read:total_read + read] = frame[:, :read]\n",
        "        total_read += read\n",
        "        if total_read == duration:\n",
        "            break\n",
        "    assert total_read <= duration, f'Expected {duration} frames, got {total_read}'\n",
        "    return sig, sr\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxiOc0spudag"
      },
      "source": [
        "## **Train Loader**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIzvoAP3psWU"
      },
      "outputs": [],
      "source": [
        "class AudioDataset(Dataset):\n",
        "  # Added path variable for passing generalized path\n",
        "  def  __init__(self, path):\n",
        "    #data loading\n",
        "    self.sr = 11000\n",
        "    self.channels = 2\n",
        "    self.min_duration = 17.0\n",
        "    self.max_duration = 30.0\n",
        "    self.sample_length = 24.0\n",
        "    self.aug_shift = False\n",
        "    self.init_dataset(path)\n",
        "\n",
        "\n",
        "  def get_index_offset(self, item):\n",
        "    # For a given dataset item and shift, return song index and offset within song\n",
        "    half_interval = self.sample_length//2\n",
        "    shift = np.random.randint(-half_interval, half_interval) if self.aug_shift else 0\n",
        "    offset = item * self.sample_length + shift # Note we centred shifts, so adding now\n",
        "    midpoint = offset + half_interval\n",
        "    assert 0 <= midpoint < self.cumsum[-1], f'Midpoint {midpoint} of item beyond total length {self.cumsum[-1]}'\n",
        "    index = np.searchsorted(self.cumsum, midpoint)  # index <-> midpoint of interval lies in this song\n",
        "    start, end = self.cumsum[index - 1] if index > 0 else 0.0, self.cumsum[index] # start and end of current song\n",
        "    assert start <= midpoint <= end, f\"Midpoint {midpoint} not inside interval [{start}, {end}] for index {index}\"\n",
        "    if offset > end - self.sample_length: # Going over song\n",
        "        offset = max(start, offset - half_interval)  # Now should fit\n",
        "    elif offset < start: # Going under song\n",
        "        offset = min(end - self.sample_length, offset + half_interval)  # Now should fit\n",
        "    #assert start <= offset <= end - self.sample_length, f\"Offset {offset} not in [{start}, {end - self.sample_length}]. End: {end}, SL: {self.sample_length}, Index: {index}\"\n",
        "    if not(start <= offset and offset <= end - self.sample_length):\n",
        "      offset = end - self.sample_length\n",
        "    offset = offset - start\n",
        "    return index, offset\n",
        "\n",
        "  def init_dataset(self, path):\n",
        "    files = librosa.util.find_files(path, ext = ['mp3', 'm4a', 'opus','wav'])\n",
        "    print(f'Found {len(files)} files!')\n",
        "\n",
        "    self.files = files\n",
        "    self.durations = [int(get_duration_sec(file)) for file in tqdm.tqdm(files)]\n",
        "    self.cumsum = np.cumsum(self.durations)\n",
        "\n",
        "  def __getitem__(self,item):\n",
        "    index, offset = self.get_index_offset(item)\n",
        "    filename, total_length = self.files[index], self.durations[index]\n",
        "    data, sr = load_audio(filename, sr=self.sr, offset=offset, duration=self.sample_length,time_base='sec')\n",
        "    assert data.shape == (self.channels, self.sample_length*self.sr), f'Expected {(self.channels, self.sample_length)}, got {data.shape}'\n",
        "    return data.T\n",
        "\n",
        "  def __len__(self):\n",
        "    return int(np.floor(self.cumsum[-1] / self.sample_length))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPZsCL4KenxP",
        "outputId": "2bc46ac0-7a99-439e-ae39-875feb044e47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25 files!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [00:00<00:00, 40.10it/s]\n"
          ]
        }
      ],
      "source": [
        "collate_fn = lambda batch: t.stack([t.from_numpy(b) for b in batch], dim=0)\n",
        "\n",
        "dataset = AudioDataset('/content/drive/MyDrive/Indian_Classical_Music_Generation/datasets/flute_dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXcGQks29gxQ",
        "outputId": "325bab47-e0f3-45eb-b316-36a7943e5adf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of dataset is  3123\n"
          ]
        }
      ],
      "source": [
        "print(\"Length of dataset is \", len(dataset))\n",
        "\n",
        "train_loader = DataLoader(dataset, batch_size=1, num_workers=2, pin_memory=False,shuffle = True,drop_last=True, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0PXfvj8MKFN"
      },
      "source": [
        "# **Encoding**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1RK5UvYeoci"
      },
      "source": [
        "## **VQ-VAE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-feu0xd8ALI"
      },
      "outputs": [],
      "source": [
        "import torch.distributed as dist\n",
        "from enum import Enum\n",
        "\n",
        "class ReduceOp(Enum):\n",
        "    SUM = 0,\n",
        "    PRODUCT = 1,\n",
        "    MIN = 2,\n",
        "    MAX = 3\n",
        "\n",
        "    def ToDistOp(self):\n",
        "        return {\n",
        "            self.SUM: dist.ReduceOp.SUM,\n",
        "            self.PRODUCT: dist.ReduceOp.PRODUCT,\n",
        "            self.MIN: dist.ReduceOp.MIN,\n",
        "            self.MAX: dist.ReduceOp.MAX\n",
        "        }[self]\n",
        "\n",
        "def is_available():\n",
        "    return dist.is_available()\n",
        "\n",
        "def get_rank():\n",
        "    if is_available():\n",
        "        return _get_rank()\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "def get_world_size():\n",
        "    if is_available():\n",
        "        return _get_world_size()\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "def barrier():\n",
        "    if is_available():\n",
        "        return _barrier()\n",
        "    #else: do nothing\n",
        "\n",
        "def all_gather(tensor_list, tensor):\n",
        "    if is_available():\n",
        "        return _all_gather(tensor_list, tensor)\n",
        "    else:\n",
        "        tensor_list[0] = tensor\n",
        "\n",
        "def all_reduce(tensor, op=ReduceOp.SUM):\n",
        "    if is_available():\n",
        "        return _all_reduce(tensor, op)\n",
        "    #else: do nothing\n",
        "\n",
        "def reduce(tensor, dst, op=ReduceOp.SUM):\n",
        "    if is_available():\n",
        "        return _reduce(tensor, dst, op)\n",
        "    #else: do nothing\n",
        "\n",
        "def broadcast(tensor, src):\n",
        "    if is_available():\n",
        "        return _broadcast(tensor, src)\n",
        "    #else: do nothing\n",
        "\n",
        "def init_process_group(backend, init_method):\n",
        "    if is_available():\n",
        "        return _init_process_group(backend, init_method)\n",
        "    #else: do nothing\n",
        "\n",
        "def _get_rank():\n",
        "    return dist.get_rank()\n",
        "\n",
        "def _barrier():\n",
        "    return dist.barrier()\n",
        "\n",
        "def _get_world_size():\n",
        "    return dist.get_world_size()\n",
        "\n",
        "def _all_gather(tensor_list, tensor):\n",
        "    return dist.all_gather(tensor_list, tensor)\n",
        "\n",
        "def _all_reduce(tensor, op):\n",
        "    return dist.all_reduce(tensor, op.ToDistOp())\n",
        "\n",
        "def _reduce(tensor, dst, op):\n",
        "    return dist.reduce(tensor, dst, op.ToDistOp())\n",
        "\n",
        "def _broadcast(tensor, src):\n",
        "    return dist.broadcast(tensor, src)\n",
        "\n",
        "def _init_process_group(backend, init_method):\n",
        "    return dist.init_process_group(backend, init_method)\n",
        "\n",
        "\n",
        "import gc\n",
        "\n",
        "def freeze_model(model):\n",
        "    model.eval()\n",
        "    for params in model.parameters():\n",
        "        params.requires_grad = False\n",
        "\n",
        "\n",
        "def unfreeze_model(model):\n",
        "    model.train()\n",
        "    for params in model.parameters():\n",
        "        params.requires_grad = True\n",
        "\n",
        "def zero_grad(model):\n",
        "    for p in model.parameters():\n",
        "        if p.requires_grad and p.grad is not None:\n",
        "            p.grad = None\n",
        "\n",
        "def empty_cache():\n",
        "    gc.collect()\n",
        "    t.cuda.empty_cache()\n",
        "\n",
        "def assert_shape(x, exp_shape):\n",
        "    # assert x.shape == exp_shape, f\"Expected {exp_shape} got {x.shape}\"\n",
        "    return\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def count_state(model):\n",
        "    return sum(s.numel() for s in model.state_dict().values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8QpTUfa9FfA"
      },
      "outputs": [],
      "source": [
        "# Simple gradient checkpointing. Works with distributed data parallel\n",
        "\n",
        "def checkpoint(func, inputs, params, flag):\n",
        "    if flag:\n",
        "        args = inputs + tuple(params)\n",
        "        return CheckpointFunction.apply(func, len(inputs), *args)\n",
        "    else:\n",
        "        return func(*inputs)\n",
        "\n",
        "class CheckpointFunction(t.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, run_function, length, *args):\n",
        "        ctx.run_function = run_function\n",
        "        ctx.input_tensors = list(args[:length])\n",
        "        ctx.input_params = list(args[length:])\n",
        "        with t.no_grad():\n",
        "            output_tensors = ctx.run_function(*ctx.input_tensors)\n",
        "        return output_tensors\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, *output_grads):\n",
        "        for i in range(len(ctx.input_tensors)):\n",
        "            temp = ctx.input_tensors[i]\n",
        "            ctx.input_tensors[i] = temp.detach()\n",
        "            ctx.input_tensors[i].requires_grad = temp.requires_grad\n",
        "        with t.enable_grad():\n",
        "            output_tensors = ctx.run_function(*ctx.input_tensors)\n",
        "        input_grads = t.autograd.grad(output_tensors, ctx.input_tensors + ctx.input_params, output_grads, allow_unused=True)\n",
        "        del ctx.input_tensors\n",
        "        del output_tensors\n",
        "        return (None, None) + input_grads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIwmwSNeen19"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch as t\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BottleneckBlock(nn.Module):\n",
        "    def __init__(self, k_bins, emb_width, mu):\n",
        "        super().__init__()\n",
        "        self.k_bins = k_bins\n",
        "        self.emb_width = emb_width\n",
        "        self.mu = mu\n",
        "        self.reset_k()\n",
        "        self.threshold = 1.0\n",
        "\n",
        "    def reset_k(self):\n",
        "        self.init = False\n",
        "        self.k_sum = None\n",
        "        self.k_elem = None\n",
        "        self.register_buffer('k', t.zeros(self.k_bins, self.emb_width).cuda())\n",
        "\n",
        "    def _tile(self, x):\n",
        "        d, ew = x.shape\n",
        "        if d < self.k_bins:\n",
        "            n_repeats = (self.k_bins + d - 1) // d\n",
        "            std = 0.01 / np.sqrt(ew)\n",
        "            x = x.repeat(n_repeats, 1)\n",
        "            x = x + t.randn_like(x) * std\n",
        "        return x\n",
        "\n",
        "    def init_k(self, x):\n",
        "        mu, emb_width, k_bins = self.mu, self.emb_width, self.k_bins\n",
        "        self.init = True\n",
        "        # init k_w using random vectors from x\n",
        "        y = self._tile(x)\n",
        "        _k_rand = y[t.randperm(y.shape[0])][:k_bins]\n",
        "        #dist.broadcast(_k_rand, 0)\n",
        "        self.k = _k_rand\n",
        "        assert self.k.shape == (k_bins, emb_width)\n",
        "        self.k_sum = self.k\n",
        "        self.k_elem = t.ones(k_bins, device=self.k.device)\n",
        "\n",
        "    def restore_k(self, num_tokens=None, threshold=1.0):\n",
        "        mu, emb_width, k_bins = self.mu, self.emb_width, self.k_bins\n",
        "        self.init = True\n",
        "        assert self.k.shape == (k_bins, emb_width)\n",
        "        self.k_sum = self.k.clone()\n",
        "        self.k_elem = t.ones(k_bins, device=self.k.device)\n",
        "        if num_tokens is not None:\n",
        "            expected_usage = num_tokens / k_bins\n",
        "            self.k_elem.data.mul_(expected_usage)\n",
        "            self.k_sum.data.mul_(expected_usage)\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def update_k(self, x, x_l):\n",
        "        mu, emb_width, k_bins = self.mu, self.emb_width, self.k_bins\n",
        "        with t.no_grad():\n",
        "            # Calculate new centres\n",
        "            x_l_onehot = t.zeros(k_bins, x.shape[0], device=x.device)  # k_bins, N * L\n",
        "            x_l_onehot.scatter_(0, x_l.view(1, x.shape[0]), 1)\n",
        "\n",
        "            _k_sum = t.matmul(x_l_onehot, x)  # k_bins, w\n",
        "            _k_elem = x_l_onehot.sum(dim=-1)  # k_bins\n",
        "            y = self._tile(x)\n",
        "            _k_rand = y[t.randperm(y.shape[0])][:k_bins]\n",
        "\n",
        "            # dist.broadcast(_k_rand, 0)\n",
        "            # dist.all_reduce(_k_sum)\n",
        "            # dist.all_reduce(_k_elem)\n",
        "\n",
        "            # Update centres\n",
        "            old_k = self.k\n",
        "            self.k_sum = mu * self.k_sum + (1. - mu) * _k_sum  # w, k_bins\n",
        "            self.k_elem = mu * self.k_elem + (1. - mu) * _k_elem  # k_bins\n",
        "            usage = (self.k_elem.view(k_bins, 1) >= self.threshold).float()\n",
        "            self.k = usage * (self.k_sum.view(k_bins, emb_width) / self.k_elem.view(k_bins, 1)) \\\n",
        "                     + (1 - usage) * _k_rand\n",
        "            _k_prob = _k_elem / t.sum(_k_elem)  # x_l_onehot.mean(dim=-1)  # prob of each bin\n",
        "            entropy = -t.sum(_k_prob * t.log(_k_prob + 1e-8))  # entropy ie how diverse\n",
        "            used_curr = (_k_elem >= self.threshold).sum()\n",
        "            usage = t.sum(usage)\n",
        "            dk = t.norm(self.k - old_k) / np.sqrt(np.prod(old_k.shape))\n",
        "        return dict(entropy=entropy,\n",
        "                    used_curr=used_curr,\n",
        "                    usage=usage,\n",
        "                    dk=dk)\n",
        "\n",
        "    def preprocess(self, x):\n",
        "        # NCT -> NTC -> [NT, C]\n",
        "        x = x.permute(0, 2, 1).contiguous()\n",
        "        x = x.view(-1, x.shape[-1])  # x_en = (N * L, w), k_j = (w, k_bins)\n",
        "\n",
        "        if x.shape[-1] == self.emb_width:\n",
        "            prenorm = t.norm(x - t.mean(x)) / np.sqrt(np.prod(x.shape))\n",
        "        elif x.shape[-1] == 2 * self.emb_width:\n",
        "            x1, x2 = x[...,:self.emb_width], x[...,self.emb_width:]\n",
        "            prenorm = (t.norm(x1 - t.mean(x1)) / np.sqrt(np.prod(x1.shape))) + (t.norm(x2 - t.mean(x2)) / np.sqrt(np.prod(x2.shape)))\n",
        "\n",
        "            # Normalise\n",
        "            x = x1 + x2\n",
        "        else:\n",
        "            assert False, f\"Expected {x.shape[-1]} to be (1 or 2) * {self.emb_width}\"\n",
        "        return x, prenorm\n",
        "\n",
        "    def postprocess(self, x_l, x_d, x_shape):\n",
        "        # [NT, C] -> NTC -> NCT\n",
        "        N, T = x_shape\n",
        "        x_d = x_d.view(N, T, -1).permute(0, 2, 1).contiguous()\n",
        "        x_l = x_l.view(N, T)\n",
        "        return x_l, x_d\n",
        "\n",
        "    def quantise(self, x):\n",
        "        # Calculate latent code x_l\n",
        "        k_w = self.k.t()\n",
        "        distance = t.sum(x ** 2, dim=-1, keepdim=True) - 2 * t.matmul(x, k_w) + t.sum(k_w ** 2, dim=0,\n",
        "                                                                                            keepdim=True)  # (N * L, b)\n",
        "        min_distance, x_l = t.min(distance, dim=-1)\n",
        "        fit = t.mean(min_distance)\n",
        "        return x_l, fit\n",
        "\n",
        "    def dequantise(self, x_l):\n",
        "        x = F.embedding(x_l, self.k)\n",
        "        return x\n",
        "\n",
        "    def encode(self, x):\n",
        "        N, width, T = x.shape\n",
        "\n",
        "        # Preprocess.\n",
        "        x, prenorm = self.preprocess(x)\n",
        "\n",
        "        # Quantise\n",
        "        x_l, fit = self.quantise(x)\n",
        "\n",
        "        # Postprocess.\n",
        "        x_l = x_l.view(N, T)\n",
        "        return x_l\n",
        "\n",
        "    def decode(self, x_l):\n",
        "        N, T = x_l.shape\n",
        "        width = self.emb_width\n",
        "\n",
        "        # Dequantise\n",
        "        x_d = self.dequantise(x_l)\n",
        "\n",
        "        # Postprocess\n",
        "        x_d = x_d.view(N, T, width).permute(0, 2, 1).contiguous()\n",
        "        return x_d\n",
        "\n",
        "    def forward(self, x, update_k=True):\n",
        "        N, width, T = x.shape\n",
        "\n",
        "        # Preprocess\n",
        "        x, prenorm = self.preprocess(x)\n",
        "\n",
        "        # Init k if not inited\n",
        "        if update_k and not self.init:\n",
        "            self.init_k(x)\n",
        "\n",
        "        # Quantise and dequantise through bottleneck\n",
        "        x_l, fit = self.quantise(x)\n",
        "        x_d = self.dequantise(x_l)\n",
        "\n",
        "        # Update embeddings\n",
        "        if update_k:\n",
        "            update_metrics = self.update_k(x, x_l)\n",
        "        else:\n",
        "            update_metrics = {}\n",
        "\n",
        "        # Loss\n",
        "        commit_loss = t.norm(x_d.detach() - x) ** 2 / np.prod(x.shape)\n",
        "\n",
        "        # Passthrough\n",
        "        x_d = x + (x_d - x).detach()\n",
        "\n",
        "        # Postprocess\n",
        "        x_l, x_d = self.postprocess(x_l, x_d, (N,T))\n",
        "        return x_l, x_d, commit_loss, dict(fit=fit,\n",
        "                                           pn=prenorm,\n",
        "                                           **update_metrics)\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, l_bins, emb_width, mu, levels):\n",
        "        super().__init__()\n",
        "        self.levels = levels\n",
        "        level_block = lambda level: BottleneckBlock(l_bins, emb_width, mu)\n",
        "        self.level_blocks = nn.ModuleList()\n",
        "        for level in range(self.levels):\n",
        "            self.level_blocks.append(level_block(level))\n",
        "\n",
        "    def encode(self, xs):\n",
        "        zs = [level_block.encode(x) for (level_block, x) in zip(self.level_blocks, xs)]\n",
        "        return zs\n",
        "\n",
        "    def decode(self, zs, start_level=0, end_level=None):\n",
        "        if end_level is None:\n",
        "            end_level = self.levels\n",
        "        xs_quantised = [level_block.decode(z) for (level_block, z) in zip(self.level_blocks[start_level:end_level], zs)]\n",
        "        return xs_quantised\n",
        "\n",
        "    def forward(self, xs):\n",
        "        zs, xs_quantised, commit_losses, metrics = [], [], [], []\n",
        "        for level in range(self.levels):\n",
        "            level_block = self.level_blocks[level]\n",
        "            x = xs[level]\n",
        "            z, x_quantised, commit_loss, metric = level_block(x, update_k=self.training)\n",
        "            zs.append(z)\n",
        "            if not self.training:\n",
        "                # Be extra paranoid and make sure the encoder weights can't\n",
        "                # change from straight-through estimator\n",
        "                x_quantised = x_quantised.detach()\n",
        "            xs_quantised.append(x_quantised)\n",
        "            commit_losses.append(commit_loss)\n",
        "            if self.training:\n",
        "                metrics.append(metric)\n",
        "        return zs, xs_quantised, commit_losses, metrics\n",
        "\n",
        "class NoBottleneckBlock(nn.Module):\n",
        "    def restore_k(self):\n",
        "        pass\n",
        "\n",
        "class NoBottleneck(nn.Module):\n",
        "    def __init__(self, levels):\n",
        "        super().__init__()\n",
        "        self.level_blocks = nn.ModuleList()\n",
        "        self.levels = levels\n",
        "        for level in range(levels):\n",
        "            self.level_blocks.append(NoBottleneckBlock())\n",
        "\n",
        "    def encode(self, xs):\n",
        "        return xs\n",
        "\n",
        "    def decode(self, zs, start_level=0, end_level=None):\n",
        "        if end_level is None:\n",
        "            end_level = self.levels\n",
        "        return zs\n",
        "\n",
        "    def forward(self, xs):\n",
        "        zero = t.zeros(()).cuda()\n",
        "        commit_losses = [zero for _ in range(self.levels)]\n",
        "        metrics = [dict(entropy=zero, usage=zero, used_curr=zero, pn=zero, dk=zero) for _ in range(self.levels)]\n",
        "        return xs, xs, commit_losses, metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnHQpbF37hvD"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch.nn as nn\n",
        "\n",
        "class ResConvBlock(nn.Module):\n",
        "    def __init__(self, n_in, n_state):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(n_in, n_state, 3, 1, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(n_state, n_in, 1, 1, 0),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.model(x)\n",
        "\n",
        "class Resnet(nn.Module):\n",
        "    def __init__(self, n_in, n_depth, m_conv=1.0):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(*[ResConvBlock(n_in, int(m_conv * n_in)) for _ in range(n_depth)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class ResConv1DBlock(nn.Module):\n",
        "    def __init__(self, n_in, n_state, dilation=1, zero_out=False, res_scale=1.0):\n",
        "        super().__init__()\n",
        "        padding = dilation\n",
        "        self.model = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(n_in, n_state, 3, 1, padding, dilation),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(n_state, n_in, 1, 1, 0),\n",
        "        )\n",
        "        if zero_out:\n",
        "            out = self.model[-1]\n",
        "            nn.init.zeros_(out.weight)\n",
        "            nn.init.zeros_(out.bias)\n",
        "        self.res_scale = res_scale\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.res_scale * self.model(x)\n",
        "\n",
        "class Resnet1D(nn.Module):\n",
        "    def __init__(self, n_in, n_depth, m_conv=1.0, dilation_growth_rate=1, dilation_cycle=None, zero_out=False, res_scale=False, reverse_dilation=False, checkpoint_res=False):\n",
        "        super().__init__()\n",
        "        def _get_depth(depth):\n",
        "            if dilation_cycle is None:\n",
        "                return depth\n",
        "            else:\n",
        "                return depth % dilation_cycle\n",
        "        blocks = [ResConv1DBlock(n_in, int(m_conv * n_in),\n",
        "                                 dilation=dilation_growth_rate ** _get_depth(depth),\n",
        "                                 zero_out=zero_out,\n",
        "                                 res_scale=1.0 if not res_scale else 1.0 / math.sqrt(n_depth))\n",
        "                  for depth in range(n_depth)]\n",
        "        if reverse_dilation:\n",
        "            blocks = blocks[::-1]\n",
        "        self.checkpoint_res = checkpoint_res\n",
        "        if self.checkpoint_res == 1:\n",
        "            if dist.get_rank() == 0:\n",
        "                print(\"Checkpointing convs\")\n",
        "            self.blocks = nn.ModuleList(blocks)\n",
        "        else:\n",
        "            self.model = nn.Sequential(*blocks)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.checkpoint_res == 1:\n",
        "            for block in self.blocks:\n",
        "                x = checkpoint(block, (x, ), block.parameters(), True)\n",
        "            return x\n",
        "        else:\n",
        "            return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lnm_NC3Z7hxI"
      },
      "outputs": [],
      "source": [
        "class EncoderConvBlock(nn.Module):\n",
        "    def __init__(self, input_emb_width, output_emb_width, down_t,\n",
        "                 stride_t, width, depth, m_conv,\n",
        "                 dilation_growth_rate=1, dilation_cycle=None, zero_out=False,\n",
        "                 res_scale=False):\n",
        "        super().__init__()\n",
        "        blocks = []\n",
        "        filter_t, pad_t = stride_t * 2, stride_t // 2\n",
        "        if down_t > 0:\n",
        "            for i in range(down_t):\n",
        "                block = nn.Sequential(\n",
        "                    nn.Conv1d(input_emb_width if i == 0 else width, width, filter_t, stride_t, pad_t),    #filter_t is kernel size\n",
        "                    Resnet1D(width, depth, m_conv, dilation_growth_rate, dilation_cycle, zero_out, res_scale),\n",
        "                )\n",
        "                blocks.append(block)\n",
        "            block = nn.Conv1d(width, output_emb_width, 3, 1, 1)\n",
        "            blocks.append(block)\n",
        "        self.model = nn.Sequential(*blocks)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class DecoderConvBock(nn.Module):\n",
        "    def __init__(self, input_emb_width, output_emb_width, down_t,\n",
        "                 stride_t, width, depth, m_conv, dilation_growth_rate=1, dilation_cycle=None, zero_out=False, res_scale=False, reverse_decoder_dilation=False, checkpoint_res=False):\n",
        "        super().__init__()\n",
        "        blocks = []\n",
        "        if down_t > 0:\n",
        "            filter_t, pad_t = stride_t * 2, stride_t // 2\n",
        "            block = nn.Conv1d(output_emb_width, width, 3, 1, 1)\n",
        "            blocks.append(block)\n",
        "            for i in range(down_t):\n",
        "                block = nn.Sequential(\n",
        "                    Resnet1D(width, depth, m_conv, dilation_growth_rate, dilation_cycle, zero_out=zero_out, res_scale=res_scale, reverse_dilation=reverse_decoder_dilation, checkpoint_res=checkpoint_res),\n",
        "                    nn.ConvTranspose1d(width, input_emb_width if i == (down_t - 1) else width, filter_t, stride_t, pad_t)  #filter_t is kernel size\n",
        "                )\n",
        "                blocks.append(block)\n",
        "        self.model = nn.Sequential(*blocks)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_emb_width, output_emb_width, levels, downs_t,\n",
        "                 strides_t, **block_kwargs):\n",
        "        super().__init__()\n",
        "        self.input_emb_width = input_emb_width\n",
        "        self.output_emb_width = output_emb_width\n",
        "        self.levels = levels\n",
        "        self.downs_t = downs_t\n",
        "        self.strides_t = strides_t\n",
        "\n",
        "        block_kwargs_copy = dict(**block_kwargs)\n",
        "        if 'reverse_decoder_dilation' in block_kwargs_copy:\n",
        "            del block_kwargs_copy['reverse_decoder_dilation']\n",
        "        level_block = lambda level, down_t, stride_t: EncoderConvBlock(input_emb_width if level == 0 else output_emb_width,\n",
        "                                                           output_emb_width,\n",
        "                                                           down_t, stride_t,\n",
        "                                                           **block_kwargs_copy)\n",
        "        self.level_blocks = nn.ModuleList()\n",
        "        iterator = zip(list(range(self.levels)), downs_t, strides_t)\n",
        "        for level, down_t, stride_t in iterator:\n",
        "            self.level_blocks.append(level_block(level, down_t, stride_t))\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, T = x.shape[0], x.shape[-1]\n",
        "        emb = self.input_emb_width\n",
        "        assert_shape(x, (N, emb, T))\n",
        "        xs = []\n",
        "\n",
        "        # 64, 32, ...\n",
        "        iterator = zip(list(range(self.levels)), self.downs_t, self.strides_t)\n",
        "        for level, down_t, stride_t in iterator:\n",
        "            level_block = self.level_blocks[level]\n",
        "            x = level_block(x)\n",
        "            emb, T = self.output_emb_width, T // (stride_t ** down_t)\n",
        "            assert_shape(x, (N, emb, T))\n",
        "            xs.append(x)\n",
        "\n",
        "        return xs\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_emb_width, output_emb_width, levels, downs_t,\n",
        "                 strides_t, **block_kwargs):\n",
        "        super().__init__()\n",
        "        self.input_emb_width = input_emb_width\n",
        "        self.output_emb_width = output_emb_width\n",
        "        self.levels = levels\n",
        "\n",
        "        self.downs_t = downs_t\n",
        "\n",
        "        self.strides_t = strides_t\n",
        "\n",
        "        level_block = lambda level, down_t, stride_t: DecoderConvBock(output_emb_width,\n",
        "                                                          output_emb_width,\n",
        "                                                          down_t, stride_t,\n",
        "                                                          **block_kwargs)\n",
        "        self.level_blocks = nn.ModuleList()\n",
        "        iterator = zip(list(range(self.levels)), downs_t, strides_t)\n",
        "        for level, down_t, stride_t in iterator:\n",
        "            self.level_blocks.append(level_block(level, down_t, stride_t))\n",
        "\n",
        "        self.out = nn.Conv1d(output_emb_width, input_emb_width, 3, 1, 1)\n",
        "\n",
        "    def forward(self, xs, all_levels=True):\n",
        "        if all_levels:\n",
        "            assert len(xs) == self.levels\n",
        "        else:\n",
        "            assert len(xs) == 1\n",
        "        x = xs[-1]\n",
        "        N, T = x.shape[0], x.shape[-1]\n",
        "        emb = self.output_emb_width\n",
        "        assert_shape(x, (N, emb, T))\n",
        "\n",
        "        # 32, 64 ...\n",
        "        iterator = reversed(list(zip(list(range(self.levels)), self.downs_t, self.strides_t)))\n",
        "        for level, down_t, stride_t in iterator:\n",
        "            level_block = self.level_blocks[level]\n",
        "            x = level_block(x)\n",
        "            emb, T = self.output_emb_width, T * (stride_t ** down_t)\n",
        "            assert_shape(x, (N, emb, T))\n",
        "            if level != 0 and all_levels:\n",
        "                x = x + xs[level - 1]\n",
        "\n",
        "        x = self.out(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa-TYQai9WYI"
      },
      "outputs": [],
      "source": [
        "class DefaultSTFTValues:\n",
        "    def __init__(self, hps):\n",
        "        self.sr = 44100\n",
        "        self.n_fft = 2048\n",
        "        self.hop_length = 256\n",
        "        self.window_size = 6 * self.hop_length\n",
        "\n",
        "class STFTValues:\n",
        "    def __init__(self, hps, n_fft, hop_length, window_size):\n",
        "        self.sr = 44100\n",
        "        self.n_fft = n_fft\n",
        "        self.hop_length = hop_length\n",
        "        self.window_size = window_size\n",
        "\n",
        "# not sure about this but used in loss decode!\n",
        "def calculate_bandwidth(dataset, hps, duration=600):\n",
        "    hps = DefaultSTFTValues(hps)\n",
        "    n_samples = int(dataset.sr * duration)\n",
        "    l1, total, total_sq, n_seen = 0.0, 0.0, 0.0, 0.0\n",
        "    spec_norm_total, spec_nelem = 0.0, 0.0\n",
        "    idx = 0\n",
        "    while n_seen < n_samples:\n",
        "        x = dataset[idx]\n",
        "        if isinstance(x, (tuple, list)):\n",
        "            x, y = x\n",
        "        samples = x.astype(np.float64)\n",
        "        stft = librosa.core.stft(np.mean(samples, axis=1), hps.n_fft, hop_length=hps.hop_length, win_length=hps.window_size)\n",
        "        spec = np.absolute(stft)\n",
        "        spec_norm_total += np.linalg.norm(spec)\n",
        "        spec_nelem += 1\n",
        "        n_seen += int(np.prod(samples.shape))\n",
        "        l1 += np.sum(np.abs(samples))\n",
        "        total += np.sum(samples)\n",
        "        total_sq += np.sum(samples ** 2)\n",
        "        idx += 1\n",
        "\n",
        "    # if dist.is_available():\n",
        "    #     from jukebox.utils.dist_utils import allreduce\n",
        "    #     n_seen = allreduce(n_seen)\n",
        "    #     total = allreduce(total)\n",
        "    #     total_sq = allreduce(total_sq)\n",
        "    #     l1 = allreduce(l1)\n",
        "    #     spec_nelem = allreduce(spec_nelem)\n",
        "    #     spec_norm_total = allreduce(spec_norm_total)\n",
        "\n",
        "    mean = total / n_seen\n",
        "    bandwidth = dict(l2 = total_sq / n_seen - mean ** 2,\n",
        "                     l1 = l1 / n_seen,\n",
        "                     spec = spec_norm_total / spec_nelem)\n",
        "    print(bandwidth)\n",
        "    return bandwidth\n",
        "\n",
        "############################################### above are important values try to debug ##################################\n",
        "\n",
        "def stft(sig, hps):\n",
        "    return t.stft(sig, hps.n_fft, hps.hop_length, win_length=hps.window_size, window=t.hann_window(hps.window_size, device=sig.device))\n",
        "\n",
        "def spec(x, hps):\n",
        "    return t.norm(stft(x, hps), p=2, dim=-1)\n",
        "\n",
        "def norm(x):\n",
        "    return (x.view(x.shape[0], -1) ** 2).sum(dim=-1).sqrt()\n",
        "\n",
        "def squeeze(x):\n",
        "    if len(x.shape) == 3:\n",
        "        assert x.shape[-1] in [1,2]\n",
        "        x = t.mean(x, -1)\n",
        "    if len(x.shape) != 2:\n",
        "        raise ValueError(f'Unknown input shape {x.shape}')\n",
        "    return x\n",
        "\n",
        "def spectral_loss(x_in, x_out, hps):\n",
        "    hps = DefaultSTFTValues(hps)\n",
        "    spec_in = spec(squeeze(x_in.float()), hps)\n",
        "    spec_out = spec(squeeze(x_out.float()), hps)\n",
        "    return norm(spec_in - spec_out)\n",
        "\n",
        "def multispectral_loss(x_in, x_out, hps):\n",
        "    losses = []\n",
        "    assert len(hps.multispec_loss_n_fft) == len(hps.multispec_loss_hop_length) == len(hps.multispec_loss_window_size)\n",
        "    args = [hps.multispec_loss_n_fft,\n",
        "            hps.multispec_loss_hop_length,\n",
        "            hps.multispec_loss_window_size]\n",
        "    for n_fft, hop_length, window_size in zip(*args):\n",
        "        hps = STFTValues(hps, n_fft, hop_length, window_size)\n",
        "        spec_in = spec(squeeze(x_in.float()), hps)\n",
        "        spec_out = spec(squeeze(x_out.float()), hps)\n",
        "        losses.append(norm(spec_in - spec_out))\n",
        "    return sum(losses) / len(losses)\n",
        "\n",
        "def spectral_convergence(x_in, x_out, hps, epsilon=2e-3):\n",
        "    hps = DefaultSTFTValues(hps)\n",
        "    spec_in = spec(squeeze(x_in.float()), hps)\n",
        "    spec_out = spec(squeeze(x_out.float()), hps)\n",
        "\n",
        "    gt_norm = norm(spec_in)\n",
        "    residual_norm = norm(spec_in - spec_out)\n",
        "    mask = (gt_norm > epsilon).float()\n",
        "    return (residual_norm * mask) / t.clamp(gt_norm, min=epsilon)\n",
        "\n",
        "def log_magnitude_loss(x_in, x_out, hps, epsilon=1e-4):\n",
        "    hps = DefaultSTFTValues(hps)\n",
        "    spec_in = t.log(spec(squeeze(x_in.float()), hps) + epsilon)\n",
        "    spec_out = t.log(spec(squeeze(x_out.float()), hps) + epsilon)\n",
        "    return t.mean(t.abs(spec_in - spec_out))\n",
        "\n",
        "# def load_audio(file, sr, offset, duration, mono=False):\n",
        "#     # Librosa loads more filetypes than soundfile\n",
        "#     x, _ = librosa.load(file, sr=sr, mono=mono, offset=offset/sr, duration=duration/sr)\n",
        "#     if len(x.shape) == 1:\n",
        "#         x = x.reshape((1, -1))\n",
        "#     return x\n",
        "\n",
        "def save_wav(fname, aud, sr):\n",
        "    # clip before saving?\n",
        "    aud = t.clamp(aud, -1, 1).cpu().numpy()\n",
        "    for i in list(range(aud.shape[0])):\n",
        "        soundfile.write(f'{fname}/item_{i}.wav', aud[i], samplerate=sr, format='wav')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rn6oa_hC9qJw"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "def def_tqdm(x):\n",
        "    return tqdm(x, leave=True, file=sys.stdout, bar_format=\"{n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}{postfix}]\")\n",
        "\n",
        "def get_range(x):\n",
        "    return x\n",
        "\n",
        "def init_logging(hps, local_rank, rank):\n",
        "    logdir = f\"{hps.local_logdir}/{hps.name}\"\n",
        "    if local_rank == 0:\n",
        "        if not os.path.exists(logdir):\n",
        "            os.makedirs(logdir)\n",
        "        with open(logdir + 'argv.txt', 'w') as f:\n",
        "            f.write(hps.argv + '\\n')\n",
        "        print(\"Logging to\", logdir)\n",
        "    logger = Logger(logdir, rank)\n",
        "    metrics = Metrics()\n",
        "    logger.add_text('hps', str(hps))\n",
        "    return logger, metrics\n",
        "\n",
        "def get_name(hps):\n",
        "    name = \"\"\n",
        "    for key, value in hps.items():\n",
        "        name += f\"{key}_{value}_\"\n",
        "    return name\n",
        "\n",
        "def average_metrics(_metrics):\n",
        "    metrics = {}\n",
        "    for _metric in _metrics:\n",
        "        for key, val in _metric.items():\n",
        "            if key not in metrics:\n",
        "                metrics[key] = []\n",
        "            metrics[key].append(val)\n",
        "    return {key: sum(vals)//len(vals) for key, vals in metrics.items()}\n",
        "\n",
        "class Metrics:\n",
        "    def __init__(self):\n",
        "        self.sum = {}\n",
        "        self.n = {}\n",
        "\n",
        "    def update(self, tag, val, batch):\n",
        "        # v is average value over batch\n",
        "        # store total value and total batch, returns dist average\n",
        "        sum = t.tensor(val * batch).float().cuda()\n",
        "        n = t.tensor(batch).float().cuda()\n",
        "        dist.all_reduce(sum)\n",
        "        dist.all_reduce(n)\n",
        "        sum = sum.item()\n",
        "        n = n.item()\n",
        "        self.sum[tag] = self.sum.get(tag, 0.0) + sum\n",
        "        self.n[tag] = self.n.get(tag, 0.0) + n\n",
        "        return sum / n\n",
        "\n",
        "    def avg(self, tag):\n",
        "        if tag in self.sum:\n",
        "            return self.sum[tag] / self.n[tag]\n",
        "        else:\n",
        "            return 0.0\n",
        "\n",
        "    def reset(self):\n",
        "        self.sum = {}\n",
        "        self.n = {}\n",
        "\n",
        "class Logger:\n",
        "    def __init__(self, logdir, rank):\n",
        "        if rank == 0:\n",
        "            from tensorboardX import SummaryWriter\n",
        "            self.sw = SummaryWriter(f\"{logdir}/logs\")\n",
        "        self.iters = 0\n",
        "        self.rank = rank\n",
        "        self.works = []\n",
        "        self.logdir = logdir\n",
        "\n",
        "    def step(self):\n",
        "        self.iters += 1\n",
        "\n",
        "    def flush(self):\n",
        "        if self.rank == 0:\n",
        "            self.sw.flush()\n",
        "\n",
        "    def add_text(self, tag, text):\n",
        "        if self.rank == 0:\n",
        "            self.sw.add_text(tag, text, self.iters)\n",
        "\n",
        "    def add_audios(self, tag, auds, sample_rate=22050, max_len=None, max_log=8):\n",
        "        if self.rank == 0:\n",
        "            for i in range(min(len(auds), max_log)):\n",
        "                if max_len:\n",
        "                    self.sw.add_audio(f\"{i}/{tag}\", auds[i][:max_len * sample_rate], self.iters, sample_rate)\n",
        "                else:\n",
        "                    self.sw.add_audio(f\"{i}/{tag}\", auds[i], self.iters, sample_rate)\n",
        "\n",
        "    def add_audio(self, tag, aud, sample_rate=22050):\n",
        "        if self.rank == 0:\n",
        "            self.sw.add_audio(tag, aud, self.iters, sample_rate)\n",
        "\n",
        "    def add_images(self, tag, img, dataformats=\"NHWC\"):\n",
        "        if self.rank == 0:\n",
        "            self.sw.add_images(tag, img, self.iters, dataformats=dataformats)\n",
        "\n",
        "    def add_image(self, tag, img):\n",
        "        if self.rank == 0:\n",
        "            self.sw.add_image(tag, img, self.iters)\n",
        "\n",
        "    def add_scalar(self, tag, val):\n",
        "        if self.rank == 0:\n",
        "            self.sw.add_scalar(tag, val, self.iters)\n",
        "\n",
        "    def get_range(self, loader):\n",
        "        if self.rank == 0:\n",
        "            self.trange = def_tqdm(loader)\n",
        "        else:\n",
        "            self.trange = loader\n",
        "        return enumerate(self.trange)\n",
        "\n",
        "    def close_range(self):\n",
        "        if self.rank == 0:\n",
        "            self.trange.close()\n",
        "\n",
        "    def set_postfix(self, *args, **kwargs):\n",
        "        if self.rank == 0:\n",
        "            self.trange.set_postfix(*args, **kwargs)\n",
        "\n",
        "    # For logging summaries of varies graph ops\n",
        "    def add_reduce_scalar(self, tag, layer, val):\n",
        "        if self.iters % 100 == 0:\n",
        "            with t.no_grad():\n",
        "                val = val.float().norm()/float(val.numel())\n",
        "            work = dist.reduce(val, 0, async_op=True)\n",
        "            self.works.append((tag, layer, val, work))\n",
        "\n",
        "    def finish_reduce(self):\n",
        "        for tag, layer, val, work in self.works:\n",
        "            work.wait()\n",
        "            if self.rank == 0:\n",
        "                val = val.item()/dist.get_world_size()\n",
        "                self.lw[layer].add_scalar(tag, val, self.iters)\n",
        "        self.works = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-J-F9J57hzW"
      },
      "outputs": [],
      "source": [
        "import torch as t\n",
        "import torch.nn as nn\n",
        "\n",
        "def dont_update(params):\n",
        "    for param in params:\n",
        "        param.requires_grad = False\n",
        "\n",
        "def update(params):\n",
        "    for param in params:\n",
        "        param.requires_grad = True\n",
        "\n",
        "def calculate_strides(strides, downs):\n",
        "    return [stride ** down for stride, down in zip(strides, downs)]\n",
        "\n",
        "def _loss_fn(loss_fn, x_target, x_pred, hps):\n",
        "    if loss_fn == 'l1':\n",
        "        return t.mean(t.abs(x_pred - x_target)) / hps.bandwidth['l1']\n",
        "    elif loss_fn == 'l2':\n",
        "        return t.mean((x_pred - x_target) ** 2) / hps.bandwidth['l2']\n",
        "    elif loss_fn == 'linf':\n",
        "        residual = ((x_pred - x_target) ** 2).reshape(x_target.shape[0], -1)\n",
        "        values, _ = t.topk(residual, hps.linf_k, dim=1)\n",
        "        return t.mean(values) / hps.bandwidth['l2']\n",
        "    elif loss_fn == 'lmix':\n",
        "        loss = 0.0\n",
        "        if hps.lmix_l1:\n",
        "            loss += hps.lmix_l1 * _loss_fn('l1', x_target, x_pred, hps)\n",
        "        if hps.lmix_l2:\n",
        "            loss += hps.lmix_l2 * _loss_fn('l2', x_target, x_pred, hps)\n",
        "        if hps.lmix_linf:\n",
        "            loss += hps.lmix_linf * _loss_fn('linf', x_target, x_pred, hps)\n",
        "        return loss\n",
        "    else:\n",
        "        assert False, f\"Unknown loss_fn {loss_fn}\"\n",
        "\n",
        "class VQVAE(nn.Module):\n",
        "    def __init__(self, input_shape, levels, downs_t, strides_t,\n",
        "                 emb_width, l_bins, mu, commit, spectral, multispectral,\n",
        "                 multipliers=None, use_bottleneck=True, **block_kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.sample_length = input_shape[0]\n",
        "        # print(input_shape)\n",
        "        x_shape, x_channels = input_shape[:-1], input_shape[-1]\n",
        "        self.x_shape = x_shape\n",
        "\n",
        "        self.downsamples = calculate_strides(strides_t, downs_t)\n",
        "        self.hop_lengths = np.cumprod(self.downsamples)\n",
        "        self.z_shapes = z_shapes = [(x_shape[0] // self.hop_lengths[level],) for level in range(levels)]\n",
        "        self.levels = levels\n",
        "\n",
        "        if multipliers is None:\n",
        "            self.multipliers = [1] * levels\n",
        "        else:\n",
        "            assert len(multipliers) == levels, \"Invalid number of multipliers\"\n",
        "            self.multipliers = multipliers\n",
        "        def _block_kwargs(level):\n",
        "            this_block_kwargs = dict(block_kwargs)\n",
        "            this_block_kwargs[\"width\"] *= self.multipliers[level]\n",
        "            this_block_kwargs[\"depth\"] *= self.multipliers[level]\n",
        "            return this_block_kwargs\n",
        "\n",
        "        encoder = lambda level: Encoder(x_channels, emb_width, level + 1,\n",
        "                                        downs_t[:level+1], strides_t[:level+1], **_block_kwargs(level))\n",
        "        decoder = lambda level: Decoder(x_channels, emb_width, level + 1,\n",
        "                                        downs_t[:level+1], strides_t[:level+1], **_block_kwargs(level))\n",
        "        self.encoders = nn.ModuleList()\n",
        "        self.decoders = nn.ModuleList()\n",
        "        for level in range(levels):\n",
        "            self.encoders.append(encoder(level))\n",
        "            self.decoders.append(decoder(level))\n",
        "\n",
        "        if use_bottleneck:\n",
        "            self.bottleneck = Bottleneck(l_bins, emb_width, mu, levels)\n",
        "        else:\n",
        "            self.bottleneck = NoBottleneck(levels)\n",
        "\n",
        "        self.downs_t = downs_t\n",
        "        self.strides_t = strides_t\n",
        "        self.l_bins = l_bins\n",
        "        self.commit = commit\n",
        "        self.spectral = spectral\n",
        "        self.multispectral = multispectral\n",
        "\n",
        "    def preprocess(self, x):\n",
        "        # x: NTC [-1,1] -> NCT [-1,1]\n",
        "        assert len(x.shape) == 3\n",
        "        x = x.permute(0,2,1).float()\n",
        "        return x\n",
        "\n",
        "    def postprocess(self, x):\n",
        "        # x: NTC [-1,1] <- NCT [-1,1]\n",
        "        x = x.permute(0,2,1)\n",
        "        return x\n",
        "\n",
        "    def _decode(self, zs, start_level=0, end_level=None):\n",
        "        # Decode\n",
        "        if end_level is None:\n",
        "            end_level = self.levels\n",
        "        assert len(zs) == end_level - start_level\n",
        "        xs_quantised = self.bottleneck.decode(zs, start_level=start_level, end_level=end_level)\n",
        "        assert len(xs_quantised) == end_level - start_level\n",
        "\n",
        "        # Use only lowest level\n",
        "        decoder, x_quantised = self.decoders[start_level], xs_quantised[0:1]\n",
        "        x_out = decoder(x_quantised, all_levels=False)\n",
        "        x_out = self.postprocess(x_out)\n",
        "        return x_out\n",
        "\n",
        "    def decode(self, zs, start_level=0, end_level=None, bs_chunks=1):\n",
        "        z_chunks = [t.chunk(z, bs_chunks, dim=0) for z in zs]\n",
        "        x_outs = []\n",
        "        for i in range(bs_chunks):\n",
        "            zs_i = [z_chunk[i] for z_chunk in z_chunks]\n",
        "            x_out = self._decode(zs_i, start_level=start_level, end_level=end_level)\n",
        "            x_outs.append(x_out)\n",
        "        return t.cat(x_outs, dim=0)\n",
        "\n",
        "    def _encode(self, x, start_level=0, end_level=None):\n",
        "        # Encode\n",
        "        if end_level is None:\n",
        "            end_level = self.levels\n",
        "        x_in = self.preprocess(x)\n",
        "        xs = []\n",
        "        for level in range(self.levels):\n",
        "            encoder = self.encoders[level]\n",
        "            x_out = encoder(x_in)\n",
        "            xs.append(x_out[-1])\n",
        "        zs = self.bottleneck.encode(xs)\n",
        "        return zs[start_level:end_level]\n",
        "\n",
        "    def encode(self, x, start_level=0, end_level=None, bs_chunks=1):\n",
        "        x_chunks = t.chunk(x, bs_chunks, dim=0)\n",
        "        zs_list = []\n",
        "        for x_i in x_chunks:\n",
        "            zs_i = self._encode(x_i, start_level=start_level, end_level=end_level)\n",
        "            zs_list.append(zs_i)\n",
        "        zs = [t.cat(zs_level_list, dim=0) for zs_level_list in zip(*zs_list)]\n",
        "        return zs\n",
        "\n",
        "    def sample(self, n_samples):\n",
        "        zs = [t.randint(0, self.l_bins, size=(n_samples, *z_shape), device='cuda') for z_shape in self.z_shapes]\n",
        "        return self.decode(zs)\n",
        "\n",
        "    def forward(self, x, hps, loss_fn='l1'):\n",
        "        metrics = {}\n",
        "\n",
        "        N = x.shape[0]\n",
        "\n",
        "        # Encode/Decode\n",
        "        x_in = self.preprocess(x)\n",
        "        xs = []\n",
        "        for level in range(self.levels):\n",
        "            encoder = self.encoders[level]\n",
        "            x_out = encoder(x_in)\n",
        "            xs.append(x_out[-1])\n",
        "\n",
        "        zs, xs_quantised, commit_losses, quantiser_metrics = self.bottleneck(xs)\n",
        "        x_outs = []\n",
        "        for level in range(self.levels):\n",
        "            decoder = self.decoders[level]\n",
        "            x_out = decoder(xs_quantised[level:level+1], all_levels=False)\n",
        "            assert_shape(x_out, x_in.shape)\n",
        "            x_outs.append(x_out)\n",
        "\n",
        "        # Loss\n",
        "        def _spectral_loss(x_target, x_out, hps):\n",
        "            if hps.use_nonrelative_specloss:\n",
        "                sl = spectral_loss(x_target, x_out, hps) / hps.bandwidth['spec']\n",
        "            else:\n",
        "                sl = spectral_convergence(x_target, x_out, hps)\n",
        "            sl = t.mean(sl)\n",
        "            return sl\n",
        "\n",
        "        def _multispectral_loss(x_target, x_out, hps):\n",
        "            sl = multispectral_loss(x_target, x_out, hps) / hps.bandwidth['spec']\n",
        "            sl = t.mean(sl)\n",
        "            return sl\n",
        "\n",
        "        recons_loss = t.zeros(()).to(x.device)\n",
        "        spec_loss = t.zeros(()).to(x.device)\n",
        "        multispec_loss = t.zeros(()).to(x.device)\n",
        "        x_target = audio_postprocess(x.float(), hps)\n",
        "\n",
        "        for level in reversed(range(self.levels)):\n",
        "            x_out = self.postprocess(x_outs[level])\n",
        "            x_out = audio_postprocess(x_out, hps)\n",
        "            this_recons_loss = _loss_fn(loss_fn, x_target, x_out, hps)\n",
        "            this_spec_loss = _spectral_loss(x_target, x_out, hps)\n",
        "            this_multispec_loss = _multispectral_loss(x_target, x_out, hps)\n",
        "            metrics[f'recons_loss_l{level + 1}'] = this_recons_loss\n",
        "            metrics[f'spectral_loss_l{level + 1}'] = this_spec_loss\n",
        "            metrics[f'multispectral_loss_l{level + 1}'] = this_multispec_loss\n",
        "            recons_loss += this_recons_loss\n",
        "            spec_loss += this_spec_loss\n",
        "            multispec_loss += this_multispec_loss\n",
        "\n",
        "        commit_loss = sum(commit_losses)\n",
        "        loss = recons_loss + self.spectral * spec_loss + self.multispectral * multispec_loss + self.commit * commit_loss\n",
        "\n",
        "        with t.no_grad():\n",
        "            sc = t.mean(spectral_convergence(x_target, x_out, hps))\n",
        "            l2_loss = _loss_fn(\"l2\", x_target, x_out, hps)\n",
        "            l1_loss = _loss_fn(\"l1\", x_target, x_out, hps)\n",
        "            linf_loss = _loss_fn(\"linf\", x_target, x_out, hps)\n",
        "\n",
        "        quantiser_metrics = average_metrics(quantiser_metrics)\n",
        "\n",
        "        metrics.update(dict(\n",
        "            recons_loss=recons_loss,\n",
        "            spectral_loss=spec_loss,\n",
        "            multispectral_loss=multispec_loss,\n",
        "            spectral_convergence=sc,\n",
        "            l2_loss=l2_loss,\n",
        "            l1_loss=l1_loss,\n",
        "            linf_loss=linf_loss,\n",
        "            commit_loss=commit_loss,\n",
        "            **quantiser_metrics))\n",
        "\n",
        "        for key, val in metrics.items():\n",
        "            metrics[key] = val.detach()\n",
        "\n",
        "        return x_out, loss, metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ta-DrpItLh4Q"
      },
      "source": [
        "# **Generation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZfxHZ8A0vNs"
      },
      "source": [
        "## **Scalable(Sparse) Transformer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leRvf6-2rqJY"
      },
      "outputs": [],
      "source": [
        "def print_once(msg):\n",
        "    if (not dist.is_available()) or dist.get_rank()==0:\n",
        "        print(msg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3j4QXf9keoHe"
      },
      "outputs": [],
      "source": [
        "# jukebox/jukebox/transformer/factored_attention.py /\n",
        "\n",
        "def repeat(x, n, dim):\n",
        "    if dim == -1:\n",
        "        dim = len(x.shape) - 1\n",
        "    return x.view(int(np.prod(x.shape[:dim+1])), 1, int(np.prod(x.shape[dim+1:]))).repeat(1,n,1).view(*x.shape[:dim], n * x.shape[dim], *x.shape[dim+1:])\n",
        "\n",
        "def get_mask(mask, q_l, kv_l, blocks, spread, device, sample, sample_t):\n",
        "    # returns a mask of shape 1 x 1 x q_l x kv_l or None if masking is not needed.\n",
        "    if mask is None or q_l == 1:\n",
        "        return None\n",
        "    offset = sample_t - q_l if sample else max(kv_l - q_l, 0)\n",
        "    if mask == 'autoregressive':\n",
        "        # Masked dense\n",
        "        mask = t.ones(q_l, kv_l, device=device).tril(offset)\n",
        "    elif mask == 'summary':\n",
        "        # Masked summary\n",
        "        mask = t.nn.functional.pad(t.ones(q_l, q_l, device=device).tril().view(q_l, blocks, q_l // blocks)[:,:-1,-kv_l//blocks:],(0,0,1,0),value=1).contiguous().view(q_l, kv_l)\n",
        "    elif mask == 'prime':\n",
        "        mask = t.ones(q_l, kv_l, device=device).tril(offset)\n",
        "    return mask.view(1,1,q_l,kv_l)\n",
        "\n",
        "class FactoredAttention(nn.Module):\n",
        "    def __init__(self, n_in, n_ctx, n_state, n_head,\n",
        "                 attn_dropout=0.0, resid_dropout=0.0,\n",
        "                 scale=True, mask=False,\n",
        "                 zero_out=False, init_scale=1.0,\n",
        "                 checkpoint_attn=0,\n",
        "                 attn_func=0, blocks=None, spread=None,\n",
        "                 encoder_dims=None, prime_len=None):\n",
        "        super().__init__()\n",
        "        self.n_in = n_in\n",
        "        self.n_ctx = n_ctx # NOTE: n_ctx could be different within operations. This is complete n_ctx\n",
        "        self.n_state = n_state\n",
        "        assert n_state % n_head == 0\n",
        "        self.n_head = n_head\n",
        "        self.scale = scale\n",
        "        self.mask = mask\n",
        "        if attn_func == 6:\n",
        "            self.c_attn = Conv1D(n_in, n_state, init_scale=init_scale)\n",
        "            self.c_enc_kv = Conv1D(n_in, n_state * 2, init_scale=init_scale)\n",
        "        else:\n",
        "            self.c_attn = Conv1D(n_in, n_state * 3, init_scale=init_scale)\n",
        "        self.c_proj = Conv1D(n_state, n_in, zero_out, init_scale=init_scale)\n",
        "        self.attn_dropout = nn.Dropout(attn_dropout) if attn_dropout > 0.0 else lambda x: x\n",
        "        self.resid_dropout = nn.Dropout(resid_dropout) if resid_dropout > 0.0 else lambda x: x\n",
        "\n",
        "        # Sequence of length l is factored as [blocks, l // blocks]\n",
        "        self.attn_func = attn_func\n",
        "        self.qkv, self.attn, self.attn_mask = {\n",
        "            0: (self.factored_qkv, self.dense_attn, 'autoregressive'),              # Attend to all positions\n",
        "            1: (self.factored_qkv, self.block_attn, 'autoregressive'),              # Attend to your block\n",
        "            2: (self.factored_qkv, self.transpose_block_attn, 'autoregressive'),    # Attend to transpose block\n",
        "            3: (self.factored_qkv, self.prev_block_attn, None),                     # Attend to previous block\n",
        "            4: (self.factored_qkv, self.summary_attn, 'summary'),                   # Attend to last position of each block\n",
        "            5: (self.factored_qkv, self.summary_spread_attn, 'summary'),\n",
        "            6: (self.decode_qkv, self.decode_attn, None),\n",
        "            7: (self.prime_qkv, self.prime_attn, 'prime')\n",
        "        }[attn_func] # Attend to last k position of each block\n",
        "\n",
        "        self.blocks = blocks\n",
        "        self.spread = spread\n",
        "        if blocks is not None:\n",
        "            assert n_ctx % blocks == 0\n",
        "            self.block_ctx = n_ctx // blocks\n",
        "        self.checkpoint_attn = checkpoint_attn # 0: None, 1: Attn after heads split, 2: Attn\n",
        "\n",
        "        self.sample_t = 0\n",
        "        self.cache = {}\n",
        "        self.encoder_dims = encoder_dims\n",
        "        self.prime_len = prime_len\n",
        "        self.record_attn = False\n",
        "        self.w = None\n",
        "\n",
        "    def _attn(self, q, k, v, sample):\n",
        "        scale = 1. / math.sqrt(math.sqrt(self.n_state // self.n_head))\n",
        "        if self.training:\n",
        "            w = t.matmul(q * scale, k * scale)\n",
        "        else:\n",
        "            w = t.matmul(q, k)\n",
        "            w.mul_(scale*scale)\n",
        "        wtype = w.dtype\n",
        "        w = w.float()\n",
        "        if self.mask:\n",
        "            # Generate appropriate mask to mask out all positions before current\n",
        "            # Might take up lot of memory for dense, so can cache it\n",
        "            mask = get_mask(self.attn_mask, q.size(-2), k.size(-1), self.blocks, self.spread, w.device, sample, self.sample_t)\n",
        "            if mask is not None:\n",
        "                #print(mask)\n",
        "                w = w * mask + -1e9 * (1 - mask)\n",
        "            w = F.softmax(w, dim=-1).type(wtype)\n",
        "        else:\n",
        "            w = F.softmax(w, dim=-1).type(wtype)\n",
        "        if self.record_attn:\n",
        "            self.w = w #.float().cpu().numpy()\n",
        "            if self.attn_func == 7:\n",
        "                # only keep music queries and lyrics keys/values\n",
        "                self.w = self.w[:,:,self.prime_len:,:self.prime_len]\n",
        "        w = self.attn_dropout(w)\n",
        "        a = t.matmul(w, v)\n",
        "        return a\n",
        "\n",
        "    def merge_heads(self, x):\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        new_x_shape = (*x.size()[:-2], x.size(-2) * x.size(-1))\n",
        "        return x.view(*new_x_shape)  # in Tensorflow implem: fct merge_states\n",
        "\n",
        "    def split_heads(self, x, k=False):\n",
        "        new_x_shape = (*x.size()[:-1], self.n_head, x.size(-1) // self.n_head)\n",
        "        x = x.view(*new_x_shape)  # in Tensorflow implem: fct split_states\n",
        "        if k:\n",
        "            return x.permute(0, 2, 3, 1)\n",
        "        else:\n",
        "            return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def dense_attn(self, query, key, value, sample):\n",
        "        query = self.split_heads(query)\n",
        "        key = self.split_heads(key, k=True)\n",
        "        value = self.split_heads(value)\n",
        "        if self.checkpoint_attn == 1 and not sample:\n",
        "            a = checkpoint(lambda q,k,v,s=sample: self._attn(q,k,v,s), (query, key, value),\n",
        "                       (), True)\n",
        "        else:\n",
        "            a = self._attn(query,key,value,sample)\n",
        "        a = self.merge_heads(a)\n",
        "        return a\n",
        "\n",
        "    def block_attn(self, q, k, v, sample):\n",
        "        blocks, block_ctx = self.blocks, self.block_ctx # block_ctx is l // blocks for complete l ie l = n_ctx. Sampling has less l\n",
        "        bs, l, d = v.shape # For sample, q_l = 1, k_l = v_l = sample_t\n",
        "        if sample:\n",
        "            assert l == self._suff_cache_len(), f\"{l} != {self._suff_cache_len()}\"\n",
        "            return self.dense_attn(q, k, v, sample).view(bs, 1, d)\n",
        "        else:\n",
        "            ql = q.shape[1]\n",
        "            q = q.view(bs * ql // block_ctx, block_ctx, d)\n",
        "            if ql < l:\n",
        "                l = ql\n",
        "                k = k[:, -l:].contiguous()\n",
        "                v = v[:, -l:].contiguous()\n",
        "            k = k.view(bs * l // block_ctx, block_ctx, d)\n",
        "            v = v.view(bs * l // block_ctx, block_ctx, d)\n",
        "            return self.dense_attn(q, k, v, sample).view(bs, l, d)\n",
        "\n",
        "    def transpose_block_attn(self, q, k, v, sample):\n",
        "        blocks, block_ctx = self.blocks, self.block_ctx # block_ctx is l // blocks for complete l ie l = n_ctx. Sampling has less l\n",
        "        bs, l, d = v.shape # For sample, q_l = 1, k_l = v_l = sample_t\n",
        "        if sample:\n",
        "            block_l = (l - 1) % block_ctx\n",
        "            k = k[:,block_l::block_ctx,:]\n",
        "            v = v[:,block_l::block_ctx,:]\n",
        "            return self.dense_attn(q, k, v, sample).view(bs, 1, d)\n",
        "        else:\n",
        "            ql = q.shape[1]\n",
        "            q = q.view(bs, ql // block_ctx, block_ctx, d).transpose(1,2).contiguous().view(bs * block_ctx, ql // block_ctx, d)\n",
        "            k = k.view(bs,  l // block_ctx, block_ctx, d).transpose(1,2).contiguous().view(bs * block_ctx,  l // block_ctx, d)\n",
        "            v = v.view(bs,  l // block_ctx, block_ctx, d).transpose(1,2).contiguous().view(bs * block_ctx,  l // block_ctx, d)\n",
        "            return self.dense_attn(q, k, v, sample).view(bs, block_ctx, ql // block_ctx, d).transpose(1,2).contiguous().view(bs, ql, d)\n",
        "\n",
        "    def prev_block_attn(self, q, k, v, sample):\n",
        "        blocks, block_ctx = self.blocks, self.block_ctx # block_ctx is l // blocks for complete l ie l = n_ctx. Sampling has less l\n",
        "        bs, l, d = v.shape # For sample, q_l = 1, k_l = v_l = sample_t\n",
        "        if sample:\n",
        "            assert l == self._suff_cache_len(), f\"{l} != {self._suff_cache_len()}\"\n",
        "            block = (l - 1) // block_ctx\n",
        "            prev_l = (block - 1) * block_ctx\n",
        "            if block > 0:\n",
        "                assert prev_l == 0\n",
        "                k = k[:, prev_l:prev_l + block_ctx, :]\n",
        "                v = v[:, prev_l:prev_l + block_ctx, :]\n",
        "            else:\n",
        "                k = t.zeros(bs, block_ctx, d, device=q.device, dtype=q.dtype)\n",
        "                v = t.zeros(bs, block_ctx, d, device=q.device, dtype=q.dtype)\n",
        "            return self.dense_attn(q, k, v, sample).view(bs, 1, d)\n",
        "        else:\n",
        "            ql = q.shape[1]\n",
        "            q = q.view(bs * ql // block_ctx, block_ctx, d)\n",
        "            k = t.nn.functional.pad(k.view(bs, l // block_ctx, block_ctx, d)[:, :-1, :, :], (0,0,0,0,1,0)).view(bs * l // block_ctx, block_ctx, d)\n",
        "            v = t.nn.functional.pad(v.view(bs, l // block_ctx, block_ctx, d)[:, :-1, :, :], (0,0,0,0,1,0)).view(bs * l // block_ctx, block_ctx, d)\n",
        "            if ql < l:\n",
        "                qb = ql // block_ctx\n",
        "                kb =  l // block_ctx\n",
        "                l = ql\n",
        "                k = k.view(bs, kb, block_ctx, d)[:, -qb:].contiguous().view(bs * qb, block_ctx, d)\n",
        "                v = v.view(bs, kb, block_ctx, d)[:, -qb:].contiguous().view(bs * qb, block_ctx, d)\n",
        "            return self.dense_attn(q, k, v, sample).view(bs, l, d)\n",
        "\n",
        "    def summary_attn(self, q, k, v, sample):\n",
        "        blocks, block_ctx = self.blocks, self.block_ctx # block_ctx is l // blocks for complete l ie l = n_ctx. Sampling has less l\n",
        "        bs, l, d = v.shape # For sample, q_l = 1, k_l = v_l = sample_t\n",
        "        if sample:\n",
        "            k = t.nn.functional.pad(k[:, block_ctx-1:blocks*block_ctx-1:block_ctx, :],(0,0,1,0))\n",
        "            v = t.nn.functional.pad(v[:, block_ctx-1:blocks*block_ctx-1:block_ctx, :],(0,0,1,0))\n",
        "            return self.dense_attn(q, k, v, sample).view(bs, 1, d)\n",
        "        else:\n",
        "            k = t.nn.functional.pad(k.view(bs, blocks, l // blocks, d)[:, :-1, -1, :],(0,0,1,0)) # bs, blocks, d\n",
        "            v = t.nn.functional.pad(v.view(bs, blocks, l // blocks, d)[:, :-1, -1, :],(0,0,1,0)) # bs, blocks, d\n",
        "            return self.dense_attn(q, k, v, sample).view(bs, l, d)\n",
        "\n",
        "    def summary_spread_attn(self, q, k, v, sample):\n",
        "        blocks, block_ctx, spread = self.blocks, self.block_ctx, self.spread # block_ctx is l // blocks for complete l ie l = n_ctx. Sampling has less l\n",
        "        bs, l, d = v.shape # For sample, q_l = 1, k_l = v_l = sample_t\n",
        "        if sample:\n",
        "            assert False, \"Not yet implemented\"\n",
        "            # k = t.nn.functional.pad(k,(0,0,block_ctx,(-l)%block_ctx)).view(bs, -1, block_ctx, d)[:,:-1,-spread:,:].contiguous().view(bs, -1, d)\n",
        "            # v = t.nn.functional.pad(v,(0,0,block_ctx,(-l)%block_ctx)).view(bs, -1, block_ctx, d)[:,:-1,-spread:,:].contiguous().view(bs, -1, d)\n",
        "            # return self.dense_attn(q, k, v, sample).view(bs, 1, d)\n",
        "        else:\n",
        "            k = t.nn.functional.pad(k.view(bs, blocks, l // blocks, d)[:, :-1, -spread:, :],(0,0,0,0,1,0)).contiguous().view(bs, blocks * spread, d)  # bs, blocks * spread, d\n",
        "            v = t.nn.functional.pad(v.view(bs, blocks, l // blocks, d)[:, :-1, -spread:, :],(0,0,0,0,1,0)).contiguous().view(bs, blocks * spread, d)  # bs, blocks * spread, d\n",
        "            return self.dense_attn(q, k, v, sample).view(bs, l, d)\n",
        "\n",
        "    def prime_attn(self, q, k, v, sample):\n",
        "        prime_len = self._prime_len\n",
        "        k = k[:, :prime_len]\n",
        "        v = v[:, :prime_len]\n",
        "        return self.dense_attn(q, k, v, sample)\n",
        "\n",
        "    def decode_attn(self, q, k, v, sample):\n",
        "        assert k.shape[1] == v.shape[1] == self.encoder_dims, f'k: {k.shape}, v: {v.shape}, enc_dims: {self.encoder_dims}'\n",
        "        return self.dense_attn(q, k, v, sample)\n",
        "\n",
        "    def factored_qkv(self, x, encoder_kv=None, sample=False):\n",
        "        curr_ctx = x.shape[1]\n",
        "        assert encoder_kv is None\n",
        "        query, key, value = x.chunk(3, dim=2)\n",
        "        if sample:\n",
        "            self.sample_t += curr_ctx\n",
        "            key, value = self._append_cache(key, value)\n",
        "            l_cache = self._suff_cache_len()\n",
        "            if self._cache_len() > l_cache:\n",
        "                self._slice_cache(-l_cache)\n",
        "            if curr_ctx > 1:\n",
        "                if self.attn_func != 0:\n",
        "                    query = self._pad_to_block_ctx(query, query=True)\n",
        "                    key = self._pad_to_block_ctx(key)\n",
        "                    value = self._pad_to_block_ctx(value)\n",
        "                    assert key.shape[1] % self.block_ctx == 0\n",
        "                    assert query.shape[1] % self.block_ctx == 0\n",
        "                assert key.shape[1] == value.shape[1]\n",
        "                assert query.shape[1] <= key.shape[1]\n",
        "                sample = False\n",
        "            else:\n",
        "                key = self.cache['key']\n",
        "                value = self.cache['value']\n",
        "        return query, key, value, sample\n",
        "\n",
        "    def prime_qkv(self, x, encoder_kv=None, sample=False):\n",
        "        curr_ctx = x.shape[1]\n",
        "        assert encoder_kv is None\n",
        "        query, key, value = x.chunk(3, dim=2)\n",
        "        if sample:\n",
        "            if self._cache_len() < self._prime_len:\n",
        "                self._append_cache(key, value)\n",
        "            if self._cache_len() > self._prime_len:\n",
        "                self._slice_cache(0, self._prime_len)\n",
        "            key, value = self.cache['key'], self.cache['value']\n",
        "            self.sample_t += curr_ctx\n",
        "            assert key.shape[1] == value.shape[1] == self._suff_cache_len(), f'k: {key.shape}, v: {value.shape}, prime_dims: {self._suff_cache_len()}'\n",
        "        else:\n",
        "            assert key.shape[1] == value.shape[1] == self.n_ctx, f'k: {key.shape}, v: {value.shape}, prime_dims: {self.n_ctx}'\n",
        "        assert key.shape[0] == value.shape[0] == query.shape[0], f'k: {key.shape}, v: {value.shape}, q: {query.shape}'\n",
        "        assert key.shape[2] == value.shape[2] == query.shape[2], f'k: {key.shape}, v: {value.shape}, q: {query.shape}'\n",
        "        return query, key, value, sample\n",
        "\n",
        "    def decode_qkv(self, x, encoder_kv=None, sample=False):\n",
        "        curr_ctx = x.shape[1]\n",
        "        assert encoder_kv is not None\n",
        "        query = x\n",
        "        if sample:\n",
        "            if self.sample_t == 0:\n",
        "                self.cache['key'], self.cache['value'] = self.c_enc_kv(encoder_kv.type_as(x)).chunk(2, dim=2)\n",
        "            key, value = self.cache['key'], self.cache['value']\n",
        "            self.sample_t += curr_ctx\n",
        "        else:\n",
        "            key, value = self.c_enc_kv(encoder_kv.type_as(x)).chunk(2, dim=2)\n",
        "        assert key.shape[0] == value.shape[0] == query.shape[0], f'k: {key.shape}, v: {value.shape}, q: {query.shape}'\n",
        "        assert key.shape[1] == value.shape[1] == self.encoder_dims, f'k: {key.shape}, v: {value.shape}, enc_dims: {self.encoder_dims}'\n",
        "        assert key.shape[2] == value.shape[2] == query.shape[2], f'k: {key.shape}, v: {value.shape}, q: {query.shape}'\n",
        "        return query, key, value, sample\n",
        "\n",
        "    def forward(self, x, encoder_kv=None, sample=False):\n",
        "        curr_ctx = x.shape[1]\n",
        "        x = self.c_attn(x)\n",
        "        query, key, value, sample = self.qkv(x, encoder_kv=encoder_kv, sample=sample)\n",
        "        if self.checkpoint_attn == 2 and not sample:\n",
        "            a = checkpoint(lambda q,k,v,s=sample: self.attn(q,k,v,s), (query, key, value), (), True)\n",
        "        else:\n",
        "            a = self.attn(query,key,value,sample)\n",
        "        if a.shape[1] != curr_ctx:\n",
        "            offset = self._offset(curr_ctx)\n",
        "            a = a[:,offset:offset + curr_ctx,:].contiguous()\n",
        "        a = self.c_proj(a)\n",
        "        return self.resid_dropout(a)\n",
        "\n",
        "    @property\n",
        "    def _prime_len(self):\n",
        "        prime_len = self.prime_len\n",
        "        assert prime_len is not None\n",
        "        prime_blocks = (prime_len // self.blocks) + 1\n",
        "        return prime_blocks * self.blocks\n",
        "\n",
        "    def _offset(self, curr_ctx):\n",
        "        if self.attn_func == 0:\n",
        "            return 0\n",
        "        return (self.sample_t - curr_ctx) % self.block_ctx\n",
        "\n",
        "    def _pad_to_block_ctx(self, x, query=False):\n",
        "        l = x.shape[1]\n",
        "        offset = self._offset(l) if query else 0\n",
        "        n_blocks = (l + offset + self.block_ctx - 1) // self.block_ctx\n",
        "        pad = n_blocks * self.block_ctx - l - offset\n",
        "        if pad == 0 and offset == 0:\n",
        "            return x\n",
        "        else:\n",
        "            return F.pad(x, (0, 0, offset, pad))\n",
        "\n",
        "    def _cache_len(self):\n",
        "        return 0 if 'key' not in self.cache else self.cache['key'].shape[1]\n",
        "\n",
        "    def _suff_cache_len(self):\n",
        "        \"\"\"\n",
        "        Precondition:\n",
        "            key and value are appended with the current context and\n",
        "            self.sample_t reflects the 1-indexed sample location in the\n",
        "            context.\n",
        "        \"\"\"\n",
        "        if self.attn_func == 0:\n",
        "            return self.sample_t\n",
        "        elif self.attn_func == 1:\n",
        "            return (self.sample_t - 1) % self.block_ctx + 1\n",
        "        elif self.attn_func == 2:\n",
        "            return self.sample_t\n",
        "        elif self.attn_func == 3:\n",
        "            if self.sample_t <= self.block_ctx:\n",
        "                return self.sample_t\n",
        "            else:\n",
        "                curr_block = (self.sample_t - 1) % self.block_ctx + 1\n",
        "                prev_block = self.block_ctx\n",
        "                return curr_block + prev_block\n",
        "        elif self.attn_func == 6:\n",
        "            return self.encoder_dims\n",
        "        elif self.attn_func == 7:\n",
        "            return min(self.sample_t, self._prime_len)\n",
        "        else:\n",
        "            raise NotImplementedError()\n",
        "\n",
        "    def _slice_cache(self, start, end=None):\n",
        "        self.cache['key'] = self.cache['key'][:, start:end]\n",
        "        self.cache['value'] = self.cache['value'][:, start:end]\n",
        "\n",
        "    def _append_cache(self, key, value):\n",
        "        if 'key' not in self.cache:\n",
        "            self.cache['key'] = key\n",
        "            self.cache['value'] = value\n",
        "        else:\n",
        "            old_key, old_value = key, value\n",
        "            key = t.cat([self.cache['key'], key], dim=1)\n",
        "            value = t.cat([self.cache['value'], value], dim=1)\n",
        "            del self.cache['key']\n",
        "            del self.cache['value']\n",
        "            del old_key\n",
        "            del old_value\n",
        "            self.cache['key'] = key\n",
        "            self.cache['value'] = value\n",
        "        return self.cache['key'], self.cache['value']\n",
        "\n",
        "    def del_cache(self):\n",
        "        self.sample_t = 0\n",
        "        if 'key' in self.cache:\n",
        "            del self.cache['key']\n",
        "        if 'value' in self.cache:\n",
        "            del self.cache['value']\n",
        "        self.cache = {}\n",
        "\n",
        "    def check(self):\n",
        "        blocks = self.blocks or 1\n",
        "        spread = self.spread or 1\n",
        "        bs, l, d = (4, self.n_ctx, self.n_in)\n",
        "        x = t.randn(bs, l, d).cuda()\n",
        "        x.requires_grad = True\n",
        "        x_out = self.forward(x) # bs, l, d\n",
        "        loss = x_out.mean(dim = -1) # bs, l\n",
        "        pos = 60\n",
        "        grad = t.autograd.grad(loss[2, pos], x)[0]\n",
        "\n",
        "        assert grad.shape == (bs, l, d)\n",
        "        assert (grad[:2] == 0).all()\n",
        "        assert (grad[3:] == 0).all()\n",
        "        assert (grad[2, (pos + 1):] == 0).all()\n",
        "        pos_grad = (t.sum(grad[2] ** 2, dim=-1) > 0).nonzero().view(-1).cpu()\n",
        "\n",
        "        block_pos = pos - (pos % (l // blocks))\n",
        "        exp_pos_grad = {0: t.arange(pos),\n",
        "                        1: t.arange(block_pos, pos),\n",
        "                        2: t.arange(pos % (l // blocks), pos, l // blocks),\n",
        "                        3: t.arange(block_pos - l // blocks, block_pos),\n",
        "                        4: t.arange(l // blocks - 1, pos, l // blocks),\n",
        "                        5: ((t.arange(pos) % (l // blocks) >= (l // blocks - spread)) & (t.arange(pos) < block_pos)).nonzero().view(-1)}[self.attn_func]\n",
        "        exp_pos_grad = t.cat([exp_pos_grad, t.tensor([pos])], dim=-1)\n",
        "\n",
        "        assert (len(pos_grad) == len(exp_pos_grad)) and (pos_grad == exp_pos_grad).all(), \\\n",
        "            f\"Expected pos grad {exp_pos_grad} got {pos_grad} for attn_func {self.attn_func} pos {pos} l {l} blocks {blocks}\"\n",
        "\n",
        "    def check_cache(self, n_samples, sample_t, fp16):\n",
        "        assert self.sample_t == sample_t, f\"{self.sample_t} != {sample_t}\"\n",
        "        if sample_t == 0:\n",
        "            assert self.cache == {}\n",
        "        else:\n",
        "            dtype = {True: t.float16, False: t.float32}[fp16]\n",
        "            l_cache = self._suff_cache_len()\n",
        "            assert self.cache['key'].shape == (n_samples, l_cache, self.n_state)\n",
        "            assert self.cache['value'].shape == (n_samples, l_cache, self.n_state)\n",
        "            assert self.cache['key'].dtype == dtype, f\"Expected {dtype}, got {self.cache['key'].dtype}\"\n",
        "            assert self.cache['value'].dtype == dtype, f\"Expected {dtype}, got {self.cache['value'].dtype}\"\n",
        "\n",
        "    def check_sample(self):\n",
        "        t.manual_seed(42)\n",
        "        bs, l, d = (4, self.n_ctx, self.n_in)\n",
        "        prime = 5\n",
        "        x = t.randn(bs, l, d).cuda()\n",
        "        xs = t.chunk(x, l, dim=1)\n",
        "        assert self.sample_t == 0\n",
        "        assert self.cache == {}\n",
        "\n",
        "        with t.no_grad():\n",
        "            enc_l = self.encoder_dims\n",
        "            encoder_kv = None\n",
        "            if self.attn_func == 6:\n",
        "                encoder_kv = t.randn(bs, enc_l, d).cuda()\n",
        "\n",
        "            # Normal path\n",
        "            x_out_normal = self.forward(x, encoder_kv=encoder_kv)\n",
        "\n",
        "            # Sampling path\n",
        "            x_out_sample = t.cat([self.forward(xs[i], encoder_kv=encoder_kv, sample=True) for i in range(l)],dim=1)\n",
        "        max_err = t.max(t.abs(x_out_sample - x_out_normal))\n",
        "        assert max_err < 1e-8, f\"Max sampling err is {max_err} {[i for i in range(l) if t.max(t.abs(x_out_sample - x_out_normal)[:,i,:]) > 1e-8]}\"\n",
        "\n",
        "        with t.no_grad():\n",
        "            x_out_normal = x_out_normal[:,:prime,:]\n",
        "            # Prime sampling path\n",
        "            self.del_cache()\n",
        "            x_out_sample = self.forward(x[:,:prime,:].contiguous(), encoder_kv=encoder_kv, sample=True)\n",
        "            self.check_cache(bs, prime, False)\n",
        "\n",
        "        max_err = t.max(t.abs(x_out_sample - x_out_normal))\n",
        "        assert max_err < 1e-8, f\"Max prime sampling err is {max_err} {[i for i in range(prime) if t.max(t.abs(x_out_sample - x_out_normal)[:,i,:]) > 1e-8]}\"\n",
        "\n",
        "    def check_chunks(self, chunk_size):\n",
        "        t.manual_seed(42)\n",
        "        bs, l, d = (4, self.n_ctx, self.n_in)\n",
        "        enc_l = self.encoder_dims\n",
        "        assert l % chunk_size == 0\n",
        "        n_chunks = l // chunk_size\n",
        "        with t.no_grad():\n",
        "            encoder_kv = None\n",
        "            x = t.randn(bs, l, d).cuda()\n",
        "            if self.attn_func == 6:\n",
        "                encoder_kv = t.randn(bs, enc_l, d).cuda()\n",
        "\n",
        "            self.del_cache()\n",
        "            y_forw = self.forward(x, encoder_kv=encoder_kv, sample=False)\n",
        "            self.del_cache()\n",
        "            y_forw_sample = self.forward(x, encoder_kv=encoder_kv, sample=True)\n",
        "            max_err = t.max(t.abs(y_forw - y_forw_sample))\n",
        "            assert max_err <= 1e-6, f\"Max err is {max_err} {[i for i in range(l) if t.max(t.abs(y_forw - y_forw_sample)[:, i, :]) > 1e-6]}\"\n",
        "\n",
        "            self.del_cache()\n",
        "            x_chunks = t.chunk(x, n_chunks, dim=1)\n",
        "            y_chunks = []\n",
        "            total_len = 0\n",
        "            for x_chunk in x_chunks:\n",
        "                y_chunk = self.forward(x_chunk.contiguous(), encoder_kv=encoder_kv, sample=True)\n",
        "                total_len += x_chunk.shape[1]\n",
        "                self.check_cache(bs, total_len, False)\n",
        "                y_chunks.append(y_chunk)\n",
        "            y_forw_in_chunks = t.cat(y_chunks, dim=1)\n",
        "\n",
        "            max_err = t.max(t.abs(y_forw - y_forw_in_chunks))\n",
        "            assert max_err <= 1e-6, f\"Max err is {max_err} {[i for i in range(l) if t.max(t.abs(y_forw - y_forw_in_chunks)[:, i, :]) > 1e-6]}\"\n",
        "\n",
        "\n",
        "#if __name__ == '__main__':\n",
        "#    from jukebox.utils.dist_utils import setup_dist_from_mpi\n",
        "#    setup_dist_from_mpi(port=29600)\n",
        "#    n_in = 16\n",
        "#    n_state = n_in * 2\n",
        "#    n_ctx = 6144\n",
        "#    n_head = 4\n",
        "#    n_depth = 12\n",
        "#    blocks = 64\n",
        "#    chunk_size = 8\n",
        "#    for attn_func in [0, 1, 2, 3, 6, 7]:\n",
        "#        encoder_dims = {0: 0, 1: 0, 2: 0, 3: 0, 6: 64, 7: 0}[attn_func]\n",
        "#        prime_len = {0: 0, 1: 0, 2: 0, 3: 0, 6: 0, 7: 384}[attn_func]\n",
        "#        attn = FactoredAttention(n_in, n_ctx + prime_len, n_state, n_head, mask=True,\n",
        "#                                 attn_func=attn_func, blocks=blocks,\n",
        "#                                 encoder_dims=encoder_dims, prime_len=prime_len)\n",
        "#        attn.training = False\n",
        "#        attn.check_sample()\n",
        "#        attn.check_chunks(chunk_size)\n",
        "#        print(f\"Checked attn_func: {attn_func}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4BQnEzbeoJg"
      },
      "outputs": [],
      "source": [
        "# jukebox/jukebox/transformer/ops.py /\n",
        "\n",
        "# Import FusedLayerNorm if we have apex, otherwise use regular LayerNorm\n",
        "try:\n",
        "    from apex.normalization import FusedLayerNorm\n",
        "    print(\"Using apex FusedLayerNorm\")\n",
        "except ImportError:\n",
        "    from torch.nn import LayerNorm as FusedLayerNorm\n",
        "\n",
        "class LayerNorm(FusedLayerNorm):\n",
        "    def __init__(self, normalized_shape, eps=1e-5, elementwise_affine=True):\n",
        "        super().__init__(normalized_shape, eps=eps, elementwise_affine=elementwise_affine)\n",
        "        self.width = np.prod(normalized_shape)\n",
        "        self.max_numel = 65535*self.width\n",
        "\n",
        "    def forward(self, input):\n",
        "        if input.numel() > self.max_numel:\n",
        "            return F.layer_norm(input.float(), self.normalized_shape, self.weight, self.bias, self.eps).type_as(input)\n",
        "        else:\n",
        "            return super(LayerNorm, self).forward(input.float()).type_as(input)\n",
        "\n",
        "def gelu(x):\n",
        "    return 0.5 * x * (1 + t.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * t.pow(x, 3))))\n",
        "\n",
        "\n",
        "def swish(x):\n",
        "    return x * t.sigmoid(x)\n",
        "\n",
        "@t.jit.script\n",
        "def quick_gelu(x):\n",
        "    return x * t.sigmoid(1.702 * x)\n",
        "\n",
        "@t.jit.script\n",
        "def quick_gelu_bwd(x, grad_output):\n",
        "    sig = t.sigmoid(1.702 * x)\n",
        "    return grad_output * sig * (1.702 * x * (1 - sig) + 1.)\n",
        "\n",
        "class QuickGelu(t.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x):\n",
        "        ctx.save_for_backward(x)\n",
        "        return quick_gelu(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return quick_gelu_bwd(ctx.saved_tensors[0], grad_output)\n",
        "\n",
        "def memory_efficient_quick_gelu(x):\n",
        "    return QuickGelu.apply(x)\n",
        "\n",
        "ACT_FNS = {\n",
        "    'relu': t.nn.functional.relu,\n",
        "    'swish': swish,\n",
        "    'gelu': gelu,\n",
        "    'quick_gelu': memory_efficient_quick_gelu #quick_gelu\n",
        "}\n",
        "\n",
        "def _move_to_gpu_and_convert_conv_weights_to_fp16(l):\n",
        "    l.cuda()\n",
        "    if isinstance(l, Conv1D):\n",
        "        l.w.data = l.w.data.half()\n",
        "\n",
        "def _convert_conv_weights_to_fp32(l):\n",
        "    if isinstance(l, Conv1D):\n",
        "        l.w.data = l.w.data.float()\n",
        "\n",
        "def _convert_conv_weights_to_fp16(l):\n",
        "    if isinstance(l, Conv1D):\n",
        "        l.w.data = l.w.data.half()\n",
        "\n",
        "def _convert_embedding_weights_to_fp16(l):\n",
        "    if isinstance(l, t.nn.Embedding):\n",
        "        l.weight.data = l.weight.data.half()\n",
        "\n",
        "def _convert_embedding_weights_to_fp32(l):\n",
        "    if isinstance(l, t.nn.Embedding):\n",
        "        l.weight.data = l.weight.data.float()\n",
        "\n",
        "class Conv1D(nn.Module):\n",
        "    def __init__(self, n_in, n_out, zero_out=False, init_scale=1.0):\n",
        "        super(Conv1D, self).__init__()\n",
        "        self.n_in = n_in\n",
        "        self.n_out = n_out\n",
        "        if zero_out:\n",
        "            w = t.zeros(n_in, n_out)\n",
        "        else:\n",
        "            w = t.empty(n_in, n_out)\n",
        "            nn.init.normal_(w, std=0.02 * init_scale)\n",
        "        b = t.zeros(n_out)\n",
        "        self.w = nn.Parameter(w)\n",
        "        self.b = nn.Parameter(b)\n",
        "\n",
        "    def forward(self, x):\n",
        "        size_out = (*x.size()[:-1], self.n_out)\n",
        "        x = t.addmm(self.b.type_as(x), x.view(-1, x.size(-1)), self.w.type_as(x)) # If x if float then float else half\n",
        "        x = x.view(*size_out)\n",
        "        return x\n",
        "\n",
        "# For large contexts, mask's can take up memory, so you can make a single saved mask for all layers\n",
        "class Mask(nn.Module):\n",
        "    def __init__(self, n_ctx):\n",
        "        super().__init__()\n",
        "        self.register_buffer('b', t.tril(t.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n",
        "\n",
        "    def forward(self, w):\n",
        "        w = w * self.b + -1e9 * (1 - self.b)  # For fp16 do w = w.float().masked_fill(self.b, float('-inf')\n",
        "        return w\n",
        "\n",
        "def filter_logits(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
        "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
        "        Args:\n",
        "            logits: logits distribution shape (vocabulary size)\n",
        "            top_k >0: keep only top k tokens with highest probability (top-k filtering).\n",
        "            top_p >0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
        "    \"\"\"\n",
        "    #assert logits.dim() == 2  # batch size 1 for now - could be updated for more but the code would be less clear\n",
        "    logits = logits.clone()\n",
        "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
        "    assert (top_k == 0) or (top_p == 0.0)\n",
        "    if top_k > 0:\n",
        "        # Remove all tokens with a probability less than the last token of the top-k\n",
        "        indices_to_remove = logits < t.topk(logits, top_k, dim=-1)[0][..., -1:]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    if top_p > 0.0:\n",
        "        sorted_logits, sorted_indices = t.sort(logits, descending=True, dim=-1)\n",
        "        cumulative_probs = t.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        # Remove tokens with cumulative probability above the threshold\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        # Shift the indices to the right to keep also the first token above the threshold\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        #indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "        indices_to_remove = t.zeros_like(logits, dtype=t.uint8).scatter_(dim=-1, index=sorted_indices, src=sorted_indices_to_remove)\n",
        "        logits[indices_to_remove] = filter_value\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGAdZvrjeoLv"
      },
      "outputs": [],
      "source": [
        "# jukebox/jukebox/transformer/transformer.py /\n",
        "\n",
        "def _convert_mlp_traced(l):\n",
        "    if isinstance(l, ResAttnBlock):\n",
        "        l.mlp = t.jit.trace(l.mlp, t.randn(1, 1, l.n_in).cuda())\n",
        "\n",
        "def _convert_mlp_traced_fp16(l):\n",
        "    if isinstance(l, ResAttnBlock):\n",
        "        l.mlp = t.jit.trace(l.mlp, t.randn(1, 1, l.n_in).cuda().half())\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, n_in, n_state, resid_dropout=0.0, afn='quick_gelu', zero_out=False, init_scale=1.0):\n",
        "        super().__init__()\n",
        "        self.c_fc = Conv1D(n_in, n_state, init_scale=init_scale)\n",
        "        self.c_proj = Conv1D(n_state, n_in, zero_out, init_scale=init_scale)\n",
        "        self.act = ACT_FNS[afn]\n",
        "        self.resid_dropout = nn.Dropout(resid_dropout) if resid_dropout > 0.0 else lambda x: x\n",
        "\n",
        "    def forward(self, x):\n",
        "        m = self.act(self.c_fc(x))\n",
        "        m = self.c_proj(m)\n",
        "        return self.resid_dropout(m)\n",
        "\n",
        "class ResAttnBlock(nn.Module):\n",
        "    def __init__(self, n_in, n_ctx, n_head,\n",
        "                 attn_dropout=0.0, resid_dropout=0.0,\n",
        "                 afn='quick_gelu', scale=True, mask=False,\n",
        "                 zero_out=False, init_scale=1.0, res_scale=1.0,\n",
        "                 m_attn = 0.25, m_mlp = 1.,\n",
        "                 checkpoint_attn = 0, checkpoint_mlp = 0,\n",
        "                 attn_func=0, blocks=None, spread=None,\n",
        "                 encoder_dims=None, prime_len=None):\n",
        "        super().__init__()\n",
        "        self.attn = FactoredAttention(n_in=n_in, n_ctx=n_ctx, n_state=int(m_attn * n_in), n_head=n_head,\n",
        "                                      attn_dropout=attn_dropout, resid_dropout=resid_dropout,\n",
        "                                      scale=scale, mask=mask,\n",
        "                                      zero_out=zero_out, init_scale=init_scale,\n",
        "                                      checkpoint_attn=checkpoint_attn,\n",
        "                                      attn_func=attn_func, blocks=blocks, spread=spread,\n",
        "                                      encoder_dims=encoder_dims, prime_len=prime_len)\n",
        "        self.ln_0 = LayerNorm(n_in)\n",
        "        self.mlp = MLP(n_in=n_in, n_state=int(m_mlp * n_in),\n",
        "                       resid_dropout=resid_dropout,\n",
        "                       afn=afn,\n",
        "                       zero_out=zero_out, init_scale=init_scale)\n",
        "        self.ln_1 = LayerNorm(n_in)\n",
        "        self.res_scale = res_scale\n",
        "\n",
        "        self.checkpoint_attn = checkpoint_attn\n",
        "        self.checkpoint_mlp = checkpoint_mlp\n",
        "        self.n_in = n_in\n",
        "        self.attn_func = attn_func\n",
        "\n",
        "    def forward(self, x, encoder_kv, sample=False):\n",
        "        if sample:\n",
        "            a = self.attn(self.ln_0(x), encoder_kv, sample)\n",
        "            m = self.mlp(self.ln_1(x + a))\n",
        "        else:\n",
        "            if self.attn_func == 6:\n",
        "                assert encoder_kv is not None\n",
        "                a = checkpoint(lambda _x,_enc_kv,_s=sample: self.attn(self.ln_0(_x),_enc_kv,_s),\n",
        "                               (x,encoder_kv),\n",
        "                               (*self.attn.parameters(), *self.ln_0.parameters()),\n",
        "                               self.checkpoint_attn == 3)  # 2 recomputes after the projections, and 1 recomputes after head splitting.\n",
        "            else:\n",
        "                assert encoder_kv is None\n",
        "                a = checkpoint(lambda _x,_enc_kv=None,_s=sample: self.attn(self.ln_0(_x),_enc_kv,_s),\n",
        "                               (x,),\n",
        "                               (*self.attn.parameters(), *self.ln_0.parameters()),\n",
        "                               self.checkpoint_attn == 3)  # 2 recomputes after the projections, and 1 recomputes after head splitting.\n",
        "            m = checkpoint(lambda _x: self.mlp(self.ln_1(_x)), (x + a,),\n",
        "                           (*self.mlp.parameters(), *self.ln_1.parameters()),\n",
        "                           self.checkpoint_mlp == 1)\n",
        "        if self.res_scale == 1.0:\n",
        "            h = x + a + m\n",
        "        else:\n",
        "            h = x + self.res_scale * (a + m)\n",
        "        return h\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, n_in, n_ctx, n_head, n_depth,\n",
        "                 attn_dropout=0.0, resid_dropout=0.0,\n",
        "                 afn='quick_gelu', scale=True, mask=False,\n",
        "                 zero_out=False, init_scale=1.0, res_scale=False,\n",
        "                 m_attn=0.25, m_mlp=1.,\n",
        "                 checkpoint_attn=0, checkpoint_mlp=0, checkpoint_res=0,\n",
        "                 attn_order=0, blocks=None, spread=None,\n",
        "                 encoder_dims=None, prime_len=None):\n",
        "        super().__init__()\n",
        "        self.n_in = n_in\n",
        "        self.n_ctx = n_ctx\n",
        "        self.encoder_dims = encoder_dims\n",
        "        self.blocks = blocks\n",
        "        if blocks is not None:\n",
        "            assert n_ctx % blocks == 0\n",
        "            self.block_ctx = n_ctx // blocks\n",
        "        self.prime_len = prime_len\n",
        "        self.n_head = n_head\n",
        "\n",
        "        res_scale = 1.0 / n_depth if res_scale else 1.0\n",
        "\n",
        "        # Orders of attn_func\n",
        "        attn_func = {0: lambda d: 0,                    # Complete dense attn\n",
        "                     1: lambda d: [1,2][d%2],           # Alternate row and column attn\n",
        "                     2: lambda d: [1,2,3][d % 3],       # Alternate row, column and previous row attn\n",
        "                     3: lambda d: [1,4][d % 2],         # Alternate row and last column\n",
        "                     4: lambda d: [1,5][d % 2],         # Alternate row and last k columns\n",
        "                     5: lambda d: [1,4,1,1][d % 4],      # Alternate row, last column, row, row\n",
        "                     6: lambda d: [1,2,3,6][d % 4],\n",
        "                     7: lambda d: [*[1,2,3]*5,6][d%16],\n",
        "                     8: lambda d: [1,2,3,1,2,3,1,2,3,6][d%10], # Used by separated_enc_dec model with lyrics\n",
        "                     9: lambda d: [1,2,3,0][d % 4],\n",
        "                     10: lambda d: [*[1,2,3,1,2,3,1,2,3],*[1,2,3,1,2,3,1,2,3,6]*7][d%79], # Used by large separated_enc_dec model with lyrics\n",
        "                     11: lambda d: [6,6,0][d%3] if d%16 == 15 else [1,2,3][d%3],\n",
        "                     12: lambda d: [7,7,0][d%3] if d%16 == 15 else [1,2,3][d%3], # Used by single_enc_dec model with lyrics\n",
        "                     }[attn_order]\n",
        "\n",
        "        attn_cycle = {0:1, 1:2, 2:3, 3:2, 4:2, 5:4, 6:4, 7:16, 8:10, 9:4, 10:79, 11:16, 12:16}[attn_order]\n",
        "        #assert n_depth % attn_cycle == 0, f'Depth {n_depth} not a multiple of cycle {attn_cycle} for attn_order {attn_order}'\n",
        "\n",
        "        attn_block = lambda d: ResAttnBlock(n_in=n_in, n_ctx=n_ctx, n_head=n_head,\n",
        "                                  attn_dropout=attn_dropout, resid_dropout=resid_dropout,\n",
        "                                  afn=afn, scale=scale, mask=mask,\n",
        "                                  zero_out=zero_out if attn_func(d) !=6 else True,\n",
        "                                  init_scale=init_scale, res_scale=res_scale,\n",
        "                                  m_attn=m_attn, m_mlp=m_mlp,\n",
        "                                  checkpoint_attn=checkpoint_attn, checkpoint_mlp=checkpoint_mlp,\n",
        "                                  attn_func=attn_func(d), blocks=blocks, spread=spread,\n",
        "                                  encoder_dims=encoder_dims, prime_len=prime_len)\n",
        "\n",
        "        self.checkpoint_res = checkpoint_res\n",
        "        self._attn_mods = nn.ModuleList()\n",
        "        for d in range(n_depth):\n",
        "            self._attn_mods.append(attn_block(d))\n",
        "        self.ws = []\n",
        "\n",
        "\n",
        "    def set_record_attn(self, record_attn):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            record_attn (bool or set): Makes forward prop dump self-attention\n",
        "                softmaxes to self.ws. Either a set of layer indices indicating\n",
        "                which layers to store, or a boolean value indicating whether to\n",
        "                dump all.\n",
        "        \"\"\"\n",
        "        def _should_record_attn(layer_idx):\n",
        "            if isinstance(record_attn, bool):\n",
        "                return record_attn\n",
        "            return layer_idx in record_attn\n",
        "        for i, l in enumerate(self._attn_mods):\n",
        "            l.attn.record_attn = _should_record_attn(i)\n",
        "        if record_attn:\n",
        "            assert self.ws == []\n",
        "            for l in self._attn_mods:\n",
        "                assert l.attn.w == None\n",
        "        else:\n",
        "            self.ws = []\n",
        "            for l in self._attn_mods:\n",
        "                l.attn.w = None\n",
        "\n",
        "    def forward(self, x, encoder_kv=None, sample=False, fp16=False, fp16_out=False):\n",
        "        if fp16:\n",
        "            x = x.half()\n",
        "\n",
        "        # Blocks\n",
        "        for i,l in enumerate(self._attn_mods):\n",
        "            if self.checkpoint_res == 1 and not sample:\n",
        "                if l.attn_func == 6:\n",
        "                    assert encoder_kv is not None\n",
        "                    f = functools.partial(l, sample=sample)\n",
        "                    x = checkpoint(f, (x, encoder_kv), l.parameters(), True)\n",
        "                else:\n",
        "                    f = functools.partial(l, encoder_kv=None, sample=sample)\n",
        "                    x = checkpoint(f, (x,), l.parameters(), True)\n",
        "            else:\n",
        "                if l.attn_func == 6:\n",
        "                    x = l(x, encoder_kv=encoder_kv, sample=sample)\n",
        "                else:\n",
        "                    x = l(x, encoder_kv=None, sample=sample)\n",
        "            if l.attn.record_attn:\n",
        "                self.ws.append(l.attn.w)\n",
        "        if not fp16_out:\n",
        "            x = x.float()\n",
        "        return x\n",
        "\n",
        "    def check_cache(self, n_samples, sample_t, fp16):\n",
        "        for l in self._attn_mods:\n",
        "            l.attn.check_cache(n_samples, sample_t, fp16)\n",
        "\n",
        "    def del_cache(self):\n",
        "        for l in self._attn_mods:\n",
        "            l.attn.del_cache()\n",
        "\n",
        "    def check_sample(self):\n",
        "        bs, l, s, d = (4, self.n_ctx, self.encoder_dims, self.n_in)\n",
        "        prime = 5\n",
        "        with t.no_grad():\n",
        "            encoder_kv = t.randn(bs, s, d).cuda()\n",
        "            x = t.randn(bs, l, d).cuda()\n",
        "            y_forw = self.forward(x, encoder_kv=encoder_kv, sample=True)\n",
        "\n",
        "            self.del_cache()\n",
        "            x_chunks = t.chunk(x, 4, dim=1)\n",
        "            y_chunks = []\n",
        "            n = 0\n",
        "            for x_chunk in x_chunks:\n",
        "                self.check_cache(bs, n, False)\n",
        "                y_chunk = self.forward(x_chunk, encoder_kv=encoder_kv, sample=True)\n",
        "                y_chunks.append(y_chunk)\n",
        "                n += x_chunk.shape[1]\n",
        "            self.check_cache(bs, n, False)\n",
        "            y_forw_in_chunks = t.cat(y_chunks, dim=1)\n",
        "\n",
        "            max_err = t.max(t.abs(y_forw - y_forw_in_chunks))\n",
        "            assert max_err <= 1e-6, f\"Max err is {max_err} {[i for i in range(l) if t.max(t.abs(y_forw - y_forw_in_chunks)[:, i, :]) > 1e-6]}\"\n",
        "\n",
        "\n",
        "#if __name__ == '__main__':\n",
        "#    from jukebox.utils.dist_utils import setup_dist_from_mpi\n",
        "#    setup_dist_from_mpi(port=29600)\n",
        "#    n_in = 16\n",
        "#    n_ctx = 192\n",
        "#    n_head = 4\n",
        "#    n_depth = 12\n",
        "#    blocks = 16\n",
        "#    for attn_order in [0,2,6]:\n",
        "#        encoder_dims = {0: 0, 2: 0, 6: 64}[attn_order]\n",
        "#        prior = Transformer(n_in, n_ctx, n_head, n_depth, mask=True, attn_order=attn_order, encoder_dims=encoder_dims, blocks=blocks).cuda()\n",
        "#        prior.training = False\n",
        "#        prior.check_sample()\n",
        "#        print(f\"Checked attn_order: {attn_order}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1xkGdWnNep7"
      },
      "outputs": [],
      "source": [
        "# jukebox/jukebox/data/labels.py /\n",
        "\n",
        "class EmptyLabeller():\n",
        "    def get_label(self, artist=None, genre=None, lyrics=None, total_length=None, offset=None):\n",
        "        y = np.array([], dtype=np.int64)\n",
        "        info = dict(artist=\"n/a\", genre=\"n/a\", lyrics=[], full_tokens=[])\n",
        "        return dict(y=y, info=info)\n",
        "\n",
        "    def get_batch_labels(self, metas, device='cpu'):\n",
        "        ys, infos = [], []\n",
        "        for meta in metas:\n",
        "            label = self.get_label()\n",
        "            y, info = label['y'], label['info']\n",
        "            ys.append(y)\n",
        "            infos.append(info)\n",
        "\n",
        "        ys = t.stack([t.from_numpy(y) for y in ys], dim=0).to(device).long()\n",
        "        assert ys.shape[0] == len(metas)\n",
        "        assert len(infos) == len(metas)\n",
        "        return dict(y=ys, info=infos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwKkI54pIPCE"
      },
      "source": [
        "## **ConditionalAutoregressive2D - The main prior**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKLTNO4veoNr"
      },
      "outputs": [],
      "source": [
        "# jukebox/jukebox/prior/autoregressive.py /\n",
        "\n",
        "def get_normal(*shape, std=0.01):\n",
        "    w = t.empty(shape)\n",
        "    nn.init.normal_(w, std=std)\n",
        "    return w\n",
        "\n",
        "def roll(x, n):\n",
        "    return t.cat((x[:, -n:], x[:, :-n]), dim=1)\n",
        "\n",
        "def split_chunks(length, chunk_size):\n",
        "    n_passes = (length + chunk_size - 1) // chunk_size\n",
        "    chunk_sizes = [*[chunk_size] * (n_passes - 1), (length - 1) % chunk_size + 1]\n",
        "    assert sum(chunk_sizes) == length\n",
        "    return chunk_sizes\n",
        "\n",
        "class PositionEmbedding(nn.Module):\n",
        "    def __init__(self, input_shape, width, init_scale=1.0, pos_init=False):\n",
        "        super().__init__()\n",
        "        self.input_shape = input_shape\n",
        "        self.input_dims = input_dims = np.prod(input_shape)\n",
        "        self.pos_init = pos_init\n",
        "        if pos_init:\n",
        "            self.register_buffer('pos', t.tensor(get_pos_idx(input_shape)).long())\n",
        "            self._pos_embs = nn.ModuleList()\n",
        "            for i in range(len(input_shape)):\n",
        "                emb = nn.Embedding(input_shape[i], width)\n",
        "                nn.init.normal_(emb.weight, std=0.02)\n",
        "                self._pos_embs.append(emb)\n",
        "        else:\n",
        "            self.pos_emb = nn.Parameter(get_normal(input_dims, width, std=0.01 * init_scale))\n",
        "\n",
        "    def forward(self):\n",
        "        if self.pos_init:\n",
        "            pos_emb = sum([self._pos_embs[i](self.pos[:,i]) for i in range(len(self.input_shape))])\n",
        "        else:\n",
        "            pos_emb = self.pos_emb\n",
        "        return pos_emb\n",
        "\n",
        "class ConditionalAutoregressive2D(nn.Module):\n",
        "    def __init__(self, input_shape, bins,\n",
        "                 width=128, depth=2, heads=1,\n",
        "                 attn_dropout=0.0, resid_dropout=0.0, emb_dropout=0.0, mask=True,\n",
        "                 zero_out=False, init_scale=1.0, res_scale=False, pos_init=False,\n",
        "                 m_attn=0.25, m_mlp=1,\n",
        "                 checkpoint_res=0, checkpoint_attn=0, checkpoint_mlp=0,\n",
        "                 attn_order=0, blocks=None, spread=None, x_cond=False, y_cond=False,\n",
        "                 encoder_dims=0, only_encode=False, merged_decoder=False, prime_len=None):\n",
        "        super().__init__()\n",
        "        self.input_shape = input_shape\n",
        "        self.input_dims = input_dims = np.prod(input_shape)\n",
        "        self.encoder_dims = encoder_dims\n",
        "        self.bins = bins\n",
        "        self.width = width\n",
        "        self.depth = depth\n",
        "\n",
        "        # A very simple token embedding which converts tokens into real valued vector with embedding dimension size\n",
        "        self.x_emb = nn.Embedding(bins, width)\n",
        "        nn.init.normal_(self.x_emb.weight, std=0.02 * init_scale)\n",
        "        self.x_emb_dropout = nn.Dropout(emb_dropout)\n",
        "\n",
        "        # Mostly none in our case therefore we prepend a start token\n",
        "        self.y_cond = y_cond\n",
        "        self.x_cond = x_cond\n",
        "        if not y_cond:\n",
        "            self.start_token = nn.Parameter(get_normal(1, width, std=0.01 * init_scale))\n",
        "\n",
        "        # Positional encoding for obious reasons\n",
        "        self.pos_emb = PositionEmbedding(input_shape=input_shape, width=width, init_scale=init_scale, pos_init=pos_init)\n",
        "        self.pos_emb_dropout = nn.Dropout(emb_dropout)\n",
        "\n",
        "        # The main scalable Transformer : the point to note is that it takes up most of the params passed to Conditional Autoregressive\n",
        "        self.transformer = Transformer(n_in=width, n_ctx=input_dims, n_head=heads, n_depth=depth,\n",
        "                                       attn_dropout=attn_dropout, resid_dropout=resid_dropout,\n",
        "                                       afn='quick_gelu', scale=True, mask=mask,\n",
        "                                       zero_out=zero_out, init_scale=init_scale, res_scale=res_scale,\n",
        "                                       m_attn=m_attn, m_mlp=m_mlp,\n",
        "                                       checkpoint_attn=checkpoint_attn, checkpoint_mlp=checkpoint_mlp, checkpoint_res=checkpoint_res,\n",
        "                                       attn_order=attn_order, blocks=blocks, spread=spread,\n",
        "                                       encoder_dims=encoder_dims, prime_len=prime_len)\n",
        "\n",
        "        # This setup might be for lyrical pretraining ??\n",
        "        self.only_encode = only_encode\n",
        "        self.prime_len = prime_len\n",
        "        if merged_decoder:\n",
        "            # Merged piped model uses this setup\n",
        "            self.add_cond_after_transformer = False\n",
        "            self.share_x_emb_x_out = False\n",
        "        else:\n",
        "            self.add_cond_after_transformer = True\n",
        "            self.share_x_emb_x_out = True\n",
        "\n",
        "        # This is not clear for now\n",
        "        if not only_encode:\n",
        "            self.x_out = nn.Linear(width, bins, bias=False)\n",
        "            if self.share_x_emb_x_out:\n",
        "                self.x_out.weight = self.x_emb.weight\n",
        "            self.loss = t.nn.CrossEntropyLoss()\n",
        "\n",
        "    def preprocess(self, x):\n",
        "        # Input: x is NHWC and uint8. Converted to NL and long\n",
        "        # Can include stuff like bitpacking, reordering here.\n",
        "        N = x.shape[0]\n",
        "        return x.view(N, -1).long()\n",
        "\n",
        "    def postprocess(self, x, sample_tokens=None):\n",
        "        # Convert back from NL and long to NHWC\n",
        "        N = x.shape[0]\n",
        "        assert (0 <= x).all() and (x < self.bins).all()\n",
        "        if sample_tokens is None or sample_tokens==self.input_dims:\n",
        "            return x.view(N, *self.input_shape)\n",
        "        else:\n",
        "            return x.view(N, -1)\n",
        "\n",
        "    def forward(self, x, x_cond=None, y_cond=None, encoder_kv=None, fp16=False, loss_full=False,\n",
        "                encode=False, get_preds=False, get_acts=False, get_sep_loss=False):\n",
        "        # Preprocess.\n",
        "        with t.no_grad():\n",
        "            x = self.preprocess(x)\n",
        "\n",
        "        N, D = x.shape\n",
        "        assert isinstance(x, t.cuda.LongTensor)\n",
        "        assert (0 <= x).all() and (x < self.bins).all()\n",
        "\n",
        "        if self.y_cond:\n",
        "            assert y_cond is not None\n",
        "            assert y_cond.shape == (N, 1, self.width)\n",
        "        else:\n",
        "            assert y_cond is None\n",
        "\n",
        "        if self.x_cond:\n",
        "            assert x_cond is not None\n",
        "            assert x_cond.shape == (N, D, self.width) or x_cond.shape == (N, 1, self.width), f\"{x_cond.shape} != {(N, D, self.width)} nor {(N, 1, self.width)}. Did you pass the correct --sample_length?\"\n",
        "        else:\n",
        "            assert x_cond is None\n",
        "            x_cond = t.zeros((N, 1, self.width), device=x.device, dtype=t.float)\n",
        "\n",
        "        x_t = x # Target\n",
        "        x = self.x_emb(x) # X emb\n",
        "        x = roll(x, 1) # Shift by 1, and fill in start token\n",
        "        if self.y_cond:\n",
        "            x[:,0] = y_cond.view(N, self.width)\n",
        "        else:\n",
        "            x[:,0] = self.start_token\n",
        "\n",
        "        # print(x.shape)\n",
        "        # x = x[:,:2048,:]\n",
        "        # print(x.shape)\n",
        "\n",
        "        x = self.x_emb_dropout(x) + self.pos_emb_dropout(self.pos_emb()) + x_cond # Pos emb and dropout\n",
        "\n",
        "        #adding encoder kv(input) manually don't do it!\n",
        "        # encode_kv = x\n",
        "\n",
        "        x = self.transformer(x, encoder_kv=encoder_kv, fp16=fp16) # Transformer\n",
        "        if self.add_cond_after_transformer: # Piped doesnt add x_cond\n",
        "            x = x + x_cond\n",
        "\n",
        "        acts = x\n",
        "        if self.only_encode:\n",
        "            return x\n",
        "        x = self.x_out(x) # Predictions\n",
        "\n",
        "        if get_sep_loss:\n",
        "            assert self.prime_len is not None\n",
        "            x_prime = x[:, :self.prime_len].reshape(-1, self.bins)\n",
        "            x_gen = x[:, self.prime_len:].reshape(-1, self.bins)\n",
        "\n",
        "            prime_loss = F.cross_entropy(x_prime, x_t[:, :self.prime_len].reshape(-1)) / np.log(2.)\n",
        "            gen_loss = F.cross_entropy(x_gen, x_t[:, self.prime_len:].reshape(-1)) / np.log(2.)\n",
        "\n",
        "            loss = (prime_loss, gen_loss) # Note order! Prime is first\n",
        "        else:\n",
        "            loss = F.cross_entropy(x.view(-1, self.bins), x_t.view(-1)) / np.log(2.)  # Loss\n",
        "\n",
        "        if get_preds:\n",
        "            return loss, x\n",
        "        elif get_acts:\n",
        "            return loss, acts\n",
        "        else:\n",
        "            return loss, None\n",
        "\n",
        "    def get_emb(self, sample_t, n_samples, x, x_cond, y_cond):\n",
        "        N, D = n_samples, self.input_dims\n",
        "        if sample_t == 0:\n",
        "            # Fill in start token\n",
        "            x = t.empty(n_samples, 1, self.width).cuda()\n",
        "            if self.y_cond:\n",
        "                x[:, 0] = y_cond.view(N, self.width)\n",
        "            else:\n",
        "                x[:, 0] = self.start_token\n",
        "        else:\n",
        "            assert isinstance(x, t.cuda.LongTensor)\n",
        "            assert (0 <= x).all() and (x < self.bins).all()\n",
        "            x = self.x_emb(x)\n",
        "        assert x.shape == (n_samples, 1, self.width)\n",
        "        if x_cond.shape == (N, D, self.width):\n",
        "            cond = x_cond[:, sample_t:sample_t + 1, :]\n",
        "        else:\n",
        "            cond = x_cond\n",
        "        x = x + self.pos_emb()[sample_t:sample_t + 1] + cond  # Pos emb, dropout is identity at eval time\n",
        "        assert x.shape == (n_samples, 1, self.width)\n",
        "        return x, cond\n",
        "\n",
        "    def sample(self, n_samples, x_cond=None, y_cond=None, encoder_kv=None, fp16=False, temp=1.0, top_k=0, top_p=0.0,\n",
        "               get_preds=False, sample_tokens=None):\n",
        "        assert self.training == False\n",
        "\n",
        "        print(self.input_dims)\n",
        "\n",
        "        if sample_tokens is None: sample_tokens=self.input_dims\n",
        "        N, D = n_samples, self.input_dims\n",
        "        if self.y_cond:\n",
        "            assert y_cond is not None\n",
        "            assert y_cond.shape == (N, 1, self.width)\n",
        "        else:\n",
        "            assert y_cond is None\n",
        "\n",
        "        if self.x_cond:\n",
        "            assert x_cond is not None\n",
        "            assert x_cond.shape == (N, D, self.width) or x_cond.shape == (N, 1, self.width), f\"Got {x_cond.shape}, expected ({N}, {D}/{1}, {self.width})\"\n",
        "        else:\n",
        "            assert x_cond is None\n",
        "            x_cond = t.zeros((N, 1, self.width), dtype=t.float).cuda()\n",
        "\n",
        "        with t.no_grad():\n",
        "            xs, x = [], None\n",
        "            if get_preds:\n",
        "                preds = []\n",
        "            for sample_t in get_range(range(0, sample_tokens)):\n",
        "                x, cond = self.get_emb(sample_t, n_samples, x, x_cond, y_cond)\n",
        "                self.transformer.check_cache(n_samples, sample_t, fp16)\n",
        "                x = self.transformer(x, encoder_kv=encoder_kv, sample=True, fp16=fp16) # Transformer\n",
        "                if self.add_cond_after_transformer:\n",
        "                    x = x + cond\n",
        "                assert x.shape == (n_samples, 1, self.width)\n",
        "                x = self.x_out(x) # Predictions\n",
        "                if get_preds:\n",
        "                    preds.append(x.clone())\n",
        "                # Adjust logits\n",
        "                x = x / temp\n",
        "                x = filter_logits(x, top_k=top_k, top_p=top_p)\n",
        "                x = t.distributions.Categorical(logits=x).sample() # Sample and replace x\n",
        "                assert x.shape == (n_samples, 1)\n",
        "                xs.append(x.clone())\n",
        "\n",
        "            del x\n",
        "            self.transformer.del_cache()\n",
        "\n",
        "            x = t.cat(xs, dim=1)\n",
        "            if get_preds:\n",
        "                preds = t.cat(preds, dim=1)\n",
        "            x = self.postprocess(x, sample_tokens)\n",
        "        if get_preds:\n",
        "            return x, preds\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "    def primed_sample(self, n_samples, x, x_cond=None, y_cond=None, encoder_kv=None, fp16=False, temp=1.0, top_k=0,\n",
        "                      top_p=0.0, get_preds=False, chunk_size=None, sample_tokens=None):\n",
        "        assert self.training == False\n",
        "\n",
        "        if sample_tokens is None: sample_tokens=self.input_dims\n",
        "        # Preprocess.\n",
        "        with t.no_grad():\n",
        "            x = self.preprocess(x)\n",
        "        assert isinstance(x, t.cuda.LongTensor)\n",
        "        assert (0 <= x).all() and (x < self.bins).all()\n",
        "        assert x.shape[0] == n_samples\n",
        "        xs = t.split(x, 1, dim=1)\n",
        "        xs = list(xs)\n",
        "\n",
        "        # print(xs)\n",
        "        # print(len(xs))\n",
        "\n",
        "        assert len(xs) < sample_tokens\n",
        "\n",
        "        N, D = n_samples, self.input_dims\n",
        "        if self.y_cond:\n",
        "            assert y_cond is not None\n",
        "            assert y_cond.shape == (N, 1, self.width)\n",
        "        else:\n",
        "            assert y_cond is None\n",
        "\n",
        "        if self.x_cond:\n",
        "            assert x_cond is not None\n",
        "            assert x_cond.shape == (N, D, self.width) or x_cond.shape == (N, 1, self.width), f\"Got {x_cond.shape}, expected ({N}, {D}/{1}, {self.width})\"\n",
        "        else:\n",
        "            assert x_cond is None\n",
        "            x_cond = t.zeros((N, 1, self.width), dtype=t.float).cuda()\n",
        "\n",
        "        with t.no_grad():\n",
        "            if get_preds:\n",
        "                preds = []\n",
        "\n",
        "            # Fill up key/value cache for past context by runing forward pass.\n",
        "            # We do so in chunks instead of doing the whole past in one forward pass to reduce max memory usage.\n",
        "            if chunk_size is None:\n",
        "                chunk_size = len(xs)\n",
        "            #assert len(xs) % chunk_size == 0, f'expected {len(xs)} to be divisible by {chunk_size}'\n",
        "            chunk_sizes = split_chunks(len(xs), chunk_size)\n",
        "            x_primes = []\n",
        "            start = 0\n",
        "            x = None\n",
        "            for current_chunk_size in get_range(chunk_sizes):\n",
        "                xs_prime, conds_prime = [], []\n",
        "                for sample_t in range(start, start + current_chunk_size):\n",
        "                    x_prime, cond_prime = self.get_emb(sample_t, n_samples, x, x_cond, y_cond)\n",
        "                    x = xs[sample_t]\n",
        "                    xs_prime.append(x_prime)\n",
        "                    conds_prime.append(cond_prime)\n",
        "                start = start + current_chunk_size\n",
        "\n",
        "                x_prime, cond_prime = t.cat(xs_prime, dim=1), t.cat(conds_prime, dim=1)\n",
        "                assert x_prime.shape == (n_samples, current_chunk_size, self.width)\n",
        "                assert cond_prime.shape == (n_samples, current_chunk_size, self.width)\n",
        "                del xs_prime\n",
        "                del conds_prime\n",
        "                if not get_preds:\n",
        "                    del cond_prime\n",
        "                x_prime = self.transformer(x_prime, encoder_kv=encoder_kv, sample=True, fp16=fp16)\n",
        "\n",
        "                if get_preds:\n",
        "                    if self.add_cond_after_transformer:\n",
        "                        x_prime = x_prime + cond_prime\n",
        "                    assert x_prime.shape == (n_samples, current_chunk_size, self.width)\n",
        "                    del cond_prime\n",
        "                    x_primes.append(x_prime)\n",
        "                else:\n",
        "                    del x_prime\n",
        "\n",
        "            if get_preds:\n",
        "                x_prime = t.cat(x_primes, dim=1)\n",
        "                assert x_prime.shape == (n_samples, len(xs), self.width)\n",
        "                x_prime = self.x_out(x_prime)  # Predictions\n",
        "                preds.append(x_prime)\n",
        "\n",
        "            empty_cache()\n",
        "            self.transformer.check_cache(n_samples, len(xs), fp16)\n",
        "\n",
        "            x = xs[-1]\n",
        "            assert x.shape == (n_samples, 1)\n",
        "            empty_cache()\n",
        "            for sample_t in get_range(range(len(xs), sample_tokens)):\n",
        "                x, cond = self.get_emb(sample_t, n_samples, x, x_cond, y_cond)\n",
        "                self.transformer.check_cache(n_samples, sample_t, fp16)\n",
        "                x = self.transformer(x, encoder_kv=encoder_kv, sample=True, fp16=fp16) # Transformer\n",
        "                if self.add_cond_after_transformer:\n",
        "                    x = x + cond\n",
        "                assert x.shape == (n_samples, 1, self.width)\n",
        "                x = self.x_out(x) # Predictions\n",
        "                if get_preds:\n",
        "                    preds.append(x)\n",
        "                # Adjust logits\n",
        "                x = x / temp\n",
        "                x = filter_logits(x, top_k=top_k, top_p=top_p)\n",
        "                x = t.distributions.Categorical(logits=x).sample() # Sample and replace x\n",
        "                assert x.shape == (n_samples, 1)\n",
        "                xs.append(x.clone())\n",
        "\n",
        "            del x\n",
        "            self.transformer.del_cache()\n",
        "\n",
        "            x = t.cat(xs, dim=1)\n",
        "            if get_preds:\n",
        "                preds = t.cat(preds, dim=1)\n",
        "            x = self.postprocess(x, sample_tokens)\n",
        "        if get_preds:\n",
        "            return x, preds\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "    def check_sample(self, chunk_size):\n",
        "        bs, l, d = (4, self.input_dims, self.width)\n",
        "        prime = int(self.input_dims//8*7)\n",
        "        enc_l = self.encoder_dims\n",
        "        with t.no_grad():\n",
        "            y_cond = t.randn(bs, 1, d).cuda() if self.y_cond else None\n",
        "            x_cond = t.randn(bs, l, d).cuda() if self.x_cond else None\n",
        "            encoder_kv = t.randn(bs, enc_l, d).cuda()\n",
        "\n",
        "            x, preds_sample = self.sample(bs, x_cond, y_cond, encoder_kv, get_preds=True)\n",
        "            loss, preds_forw = self.forward(x, x_cond, y_cond, encoder_kv, get_preds=True)\n",
        "            max_err = t.max(t.abs(preds_sample - preds_forw))\n",
        "            assert max_err <= 1e-6, f\"Max err is {max_err} {[i for i in range(l) if t.max(t.abs(preds_sample - preds_forw)[:, i, :]) > 1e-6]}\"\n",
        "\n",
        "            x_prime = x.view(bs, -1)[:,:prime]\n",
        "            # unchunked\n",
        "            x, preds_sample = self.primed_sample(bs, x_prime.clone(), x_cond, y_cond, encoder_kv, get_preds=True)\n",
        "            assert (x.view(bs, -1)[:,:prime] == x_prime).all(), \"Priming samples don't match\"\n",
        "            loss, preds_forw = self.forward(x, x_cond, y_cond, encoder_kv, get_preds=True)\n",
        "            max_err = t.max(t.abs(preds_sample - preds_forw))\n",
        "            assert max_err <= 1e-6, f\"Max err is {max_err} {[i for i in range(l) if t.max(t.abs(preds_sample - preds_forw)[:, i, :]) > 1e-6]}\"\n",
        "\n",
        "            # chunked\n",
        "            x, preds_sample = self.primed_sample(bs, x_prime.clone(), x_cond, y_cond, encoder_kv, get_preds=True, chunk_size=chunk_size)\n",
        "            assert (x.view(bs, -1)[:,:prime] == x_prime).all(), \"Priming samples don't match\"\n",
        "            loss, preds_forw = self.forward(x, x_cond, y_cond, encoder_kv, get_preds=True)\n",
        "            max_err = t.max(t.abs(preds_sample - preds_forw))\n",
        "            assert max_err <= 1e-6, f\"Max err is {max_err} {[i for i in range(l) if t.max(t.abs(preds_sample - preds_forw)[:, i, :]) > 1e-6]}\"\n",
        "\n",
        "\n",
        "def test_prior(input_shape, encoder_dims, blocks, heads, chunk_size):\n",
        "    bins = 512\n",
        "    width = 32\n",
        "    depth = 2\n",
        "    prime_len = encoder_dims\n",
        "    for x_cond in [True, False]:\n",
        "        for y_cond in [True, False]:\n",
        "            for attn_order in [0,2,6,12]:\n",
        "                prior = ConditionalAutoregressive2D(input_shape, bins,\n",
        "                                                    width=width, depth=depth, heads=heads,\n",
        "                                                    attn_order=attn_order, blocks=blocks,\n",
        "                                                    x_cond=x_cond, y_cond=y_cond,\n",
        "                                                    encoder_dims=encoder_dims, prime_len=prime_len).cuda()\n",
        "                prior.training = False\n",
        "                prior.check_sample(chunk_size)\n",
        "                print(f\"Checked x_cond: {x_cond}, y_cond: {y_cond}, attn_order: {attn_order}\")\n",
        "            # prior.apply(_convert_mlp_traced)\n",
        "            # prior.check_sample()\n",
        "            # print(f\"Checked traced x_cond: {x_cond}, y_cond: {y_cond}\")\n",
        "\n",
        "\n",
        "#if __name__ == '__main__':\n",
        "#    from jukebox.utils.dist_utils import setup_dist_from_mpi\n",
        "#    setup_dist_from_mpi(port=29600)\n",
        "#    test_cases = [\n",
        "#        ((6144,), 384, 64, 2, 23),\n",
        "#        ((6144,), 384, 64, 2, 8),\n",
        "#        ((8192,), 512, 128, 2, 16),\n",
        "#    ]\n",
        "#    for test_case in test_cases:\n",
        "#        test_prior(*test_case)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_w25XT6N49i"
      },
      "outputs": [],
      "source": [
        "# jukebox/jukebox/prior/conditioners.py /\n",
        "\n",
        "class Conditioner(nn.Module):\n",
        "    def __init__(self, input_shape, bins, down_t, stride_t, out_width, init_scale, zero_out, res_scale, **block_kwargs):\n",
        "        super().__init__()\n",
        "        self.x_shape = input_shape\n",
        "\n",
        "        # Embedding\n",
        "        self.width = out_width\n",
        "        self.x_emb = nn.Embedding(bins, out_width)\n",
        "        nn.init.normal_(self.x_emb.weight, std=0.02 * init_scale)\n",
        "\n",
        "        # Conditioner\n",
        "        self.cond = DecoderConvBock(self.width, self.width, down_t, stride_t, **block_kwargs, zero_out=zero_out, res_scale=res_scale)\n",
        "        self.ln = LayerNorm(self.width)\n",
        "\n",
        "    def preprocess(self, x):\n",
        "        x = x.permute(0,2,1) # NTC -> NCT\n",
        "        return x\n",
        "\n",
        "    def postprocess(self, x):\n",
        "        x = x.permute(0,2,1) # NCT -> NTC\n",
        "        return x\n",
        "\n",
        "    def forward(self, x, x_cond=None):\n",
        "        N = x.shape[0]\n",
        "        # assert_shape(x, (N, *self.x_shape))\n",
        "        if x_cond is not None:\n",
        "            assert_shape(x_cond, (N, *self.x_shape, self.width))\n",
        "        else:\n",
        "            x_cond = 0.0\n",
        "        # Embed x\n",
        "        x = x.long()\n",
        "        x = self.x_emb(x)\n",
        "        assert_shape(x, (N, *self.x_shape, self.width))\n",
        "        x = x + x_cond\n",
        "\n",
        "        # Run conditioner\n",
        "        x = self.preprocess(x)\n",
        "        x = self.cond(x)\n",
        "        x = self.postprocess(x)\n",
        "        x = self.ln(x)\n",
        "        return x\n",
        "\n",
        "def flip(x):\n",
        "    def _flip(x):\n",
        "        return x.permute(0,2,1).contiguous()\n",
        "    if isinstance(x, (list, tuple)):\n",
        "        return [flip(z) for z in x]\n",
        "    return _flip(x)\n",
        "\n",
        "class SimpleEmbedding(nn.Module):\n",
        "    def __init__(self, bins, out_width, init_scale):\n",
        "        super().__init__()\n",
        "        self.bins = bins\n",
        "        self.emb = nn.Embedding(bins, out_width)\n",
        "        nn.init.normal_(self.emb.weight, std=0.01 * init_scale)\n",
        "\n",
        "    def forward(self, y):\n",
        "        assert len(y.shape) == 2, f\"Expected shape with 2 dims, got {y.shape}\"\n",
        "        assert isinstance(y, t.cuda.LongTensor), f\"Expected dtype {t.cuda.LongTensor}, got {y.dtype}\"\n",
        "        assert (0 <= y).all() and (y < self.bins).all(), f\"Bins {self.bins}, got label {y}\"\n",
        "        return self.emb(y)\n",
        "\n",
        "class RangeEmbedding(nn.Module):\n",
        "    # Interpolating\n",
        "    # Interpolate so that [pos_start, pos_end] <-> position tensor of length n_ctx\n",
        "    #\n",
        "    # Binning\n",
        "    # For each pos in position tensor, find its bin\n",
        "    # [start,end) mapped to [0,1,...,bins-1]\n",
        "    # [start,end) -> [0,1) -> [0, bins) -> floor -> [0,...,bins-1]\n",
        "    # NOTE: Open ended interval on right, so start <= pos < end, not <= end\n",
        "    def __init__(self, n_time, bins, range, out_width, init_scale, clamp=False):\n",
        "        super().__init__()\n",
        "        self.n_time = n_time\n",
        "        self.bins = bins\n",
        "        self.emb = nn.Embedding(bins, out_width)\n",
        "        nn.init.normal_(self.emb.weight, std=0.01 * init_scale)\n",
        "        self.pos_min, self.pos_max = range\n",
        "        self.clamp = clamp\n",
        "\n",
        "    def forward(self, pos_start, pos_end=None):\n",
        "        # Check if [pos_start,pos_end] in [pos_min, pos_max)\n",
        "        assert len(pos_start.shape) == 2, f\"Expected shape with 2 dims, got {pos_start.shape}\"\n",
        "        assert (self.pos_min <= pos_start).all() and (pos_start < self.pos_max).all(), f\"Range is [{self.pos_min},{self.pos_max}), got {pos_start}\"\n",
        "        pos_start = pos_start.float()\n",
        "        if pos_end is not None:\n",
        "            assert len(pos_end.shape) == 2, f\"Expected shape with 2 dims, got {pos_end.shape}\"\n",
        "            if self.clamp:\n",
        "                pos_end = pos_end.clamp(self.pos_min, self.pos_max)\n",
        "            assert (self.pos_min <= pos_end).all() and (pos_end <= self.pos_max).all(), f\"Range is [{self.pos_min},{self.pos_max}), got {pos_end}\"\n",
        "            pos_end = pos_end.float()\n",
        "        # Interpolate so that [pos_start, ..., pos_end] <-> position tensor of length n_ctx\n",
        "        n_time = self.n_time\n",
        "        if n_time != 1:\n",
        "            assert pos_end is not None\n",
        "            interpolation  = (t.arange(0, n_time, dtype=t.float, device='cuda').view(1,n_time)/n_time)\n",
        "            position = pos_start + (pos_end - pos_start)*interpolation\n",
        "        else:\n",
        "            position = pos_start\n",
        "\n",
        "        # Bin each value to bins\n",
        "        normalised_position = (position - self.pos_min) / (self.pos_max - self.pos_min) # [0,1)\n",
        "        bins = (self.bins * normalised_position).floor().long().detach() # [0,1) -> [0,1..,bins) -> [0,1...,bins-1]\n",
        "        return self.emb(bins)\n",
        "\n",
        "# class LabelConditioner(nn.Module):\n",
        "#    def __init__(self, y_bins, t_bins, sr, min_duration, max_duration, n_time, out_width, init_scale, max_bow_genre_size, include_time_signal):\n",
        "#        super().__init__()\n",
        "#        self.n_time = n_time\n",
        "#        self.out_width = out_width\n",
        "#        assert len(y_bins) == 2, f\"Expecting (genre, artist) bins, got {y_bins}\"\n",
        "#        bow_genre_bins, artist_bins = y_bins\n",
        "#        self.max_bow_genre_size = max_bow_genre_size\n",
        "#        self.bow_genre_emb = SimpleEmbedding(bow_genre_bins, out_width, init_scale)\n",
        "#        self.artist_emb = SimpleEmbedding(artist_bins, out_width, init_scale)\n",
        "#        self.include_time_signal = include_time_signal\n",
        "#        if self.include_time_signal:\n",
        "#            t_ranges = ((min_duration * sr, max_duration * sr),  # Total length\n",
        "#                        (0.0, max_duration * sr),                # Absolute pos\n",
        "#                        (0.0, 1.0))                              # Relative pos\n",
        "#            assert len(t_ranges) == 3, f\"Expecting (total, absolute, relative) ranges, got {t_ranges}\"\n",
        "#            total_length_range, absolute_pos_range, relative_pos_range = t_ranges\n",
        "#            self.total_length_emb = RangeEmbedding(1, t_bins, total_length_range, out_width, init_scale)\n",
        "#            self.absolute_pos_emb = RangeEmbedding(n_time, t_bins, absolute_pos_range, out_width, init_scale)\n",
        "#            self.relative_pos_emb = RangeEmbedding(n_time, t_bins, relative_pos_range, out_width, init_scale, clamp=True)#\n",
        "\n",
        "#    def forward(self, y):\n",
        "#        assert len(y.shape) == 2, f\"Expected shape with 2 dims, got {y.shape}\"\n",
        "#       assert y.shape[-1] == 4 + self.max_bow_genre_size, f\"Expected shape (N,{4 + self.max_bow_genre_size}), got {y.shape}\"\n",
        "#        assert isinstance(y, t.cuda.LongTensor), f\"Expected dtype {t.cuda.LongTensor}, got {y.dtype}\"\n",
        "#        N = y.shape[0]\n",
        "#        total_length, offset, length, artist, genre = y[:,0:1], y[:,1:2], y[:,2:3], y[:,3:4], y[:,4:]\n",
        "\n",
        "#        # Start embedding of length 1\n",
        "#        artist_emb = self.artist_emb(artist)\n",
        "#        # Empty genre slots are denoted by -1. We mask these out.\n",
        "#        mask = (genre >= 0).float().unsqueeze(2)\n",
        "#        genre_emb = (self.bow_genre_emb(genre.clamp(0)) * mask).sum(dim=1, keepdim=True)\n",
        "#        start_emb = genre_emb + artist_emb\n",
        "#        assert_shape(start_emb, (N, 1, self.out_width))\n",
        "\n",
        "#        # Pos embedding of length n_ctx\n",
        "#        if self.include_time_signal:\n",
        "#            start, end = offset, offset + length\n",
        "#            total_length, start, end = total_length.float(), start.float(), end.float()\n",
        "#            pos_emb = self.total_length_emb(total_length) + self.absolute_pos_emb(start, end) + self.relative_pos_emb(start/total_length, end/total_length)\n",
        "#            assert_shape(pos_emb, (N, self.n_time, self.out_width))\n",
        "#        else:\n",
        "#            pos_emb = None\n",
        "#        return start_emb, pos_emb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztnq56ojKg5J"
      },
      "source": [
        "## **Prior**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehakgKAsOOhP"
      },
      "outputs": [],
      "source": [
        "# jukebox/jukebox/prior/prior.py /\n",
        "\n",
        "\"\"\"\n",
        "Model the prior on vq codes conditioned on timing, artist, genre, lyrics and codes from levels above.\n",
        "To condition on the timing, genre and artist, we use the LabelConditioner class\n",
        "To condition on the codes from the level above, we use the Conditioner class\n",
        "To condition on lyrics, we allow two types of priors:\n",
        "- Separate Encoder Decoder: This is the usual encoder-decoder style transformer. The encoder transformer autoregressively\n",
        "models the lyrics, and we use its last layer to produce keys/values that are attened to by the decoder transformer\n",
        "- Single Encoder Decoder: This is a simplification where we combine them into a single model. We merge the text vocab\n",
        "and VQ vocab into a single large vocab, and the lyric tokens and VQ tokens into a single longer sequence of tokens which\n",
        "we autoregressively model together.\n",
        "\"\"\"\n",
        "class SimplePrior(nn.Module):\n",
        "    def __init__(self, z_shapes, l_bins, encoder, decoder, level,\n",
        "                 downs_t, strides_t, labels, prior_kwargs, x_cond_kwargs, y_cond_kwargs,\n",
        "                 prime_kwargs, copy_input, labels_v3=False,\n",
        "                 merged_decoder=False, single_enc_dec=False):\n",
        "        super().__init__()\n",
        "\n",
        "        #This is trash all values are False or 0 (Assuming this is used for lyrics in the encoder part)\n",
        "        self.use_tokens = prime_kwargs.pop('use_tokens')\n",
        "        self.n_tokens = prime_kwargs.pop('n_tokens')\n",
        "        self.prime_loss_fraction = prime_kwargs.pop('prime_loss_fraction')\n",
        "        self.copy_input = copy_input\n",
        "\n",
        "        # print(self.use_tokens)\n",
        "        # print(self.n_tokens)\n",
        "        # print(self.prime_loss_fraction)\n",
        "\n",
        "        if self.copy_input:                   #No copy input no prime_kwargs\n",
        "            prime_kwargs['bins'] = l_bins\n",
        "\n",
        "        # A list containing encoded dimesion of input at each level of vq-vae\n",
        "        self.z_shapes = z_shapes\n",
        "        self.levels = len(self.z_shapes)        # Length of such a list would obiously provide total no of levels\n",
        "\n",
        "        # Finally z_shape specifies the encoded input dimension of current level (In our case mostly level 0 would be used)\n",
        "        self.z_shape = self.z_shapes[level]\n",
        "\n",
        "        self.level = level\n",
        "        assert level < self.levels, f\"Total levels {self.levels}, got level {level}\"\n",
        "\n",
        "        #obiously codebook size in the vq-vae\n",
        "        self.l_bins = l_bins\n",
        "\n",
        "        # vq-vae encoder decoder\n",
        "        # Passing functions instead of the vqvae module to avoid getting params\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "        # X conditioning and even Y conditioning is False for now in our case\n",
        "        # X conditioning\n",
        "        self.x_cond = (level != (self.levels - 1))          # True if level is not the top one i.e 2 in general case\n",
        "        self.cond_level = level + 1\n",
        "\n",
        "        # Y conditioning\n",
        "        self.y_cond = labels\n",
        "\n",
        "        self.single_enc_dec = single_enc_dec\n",
        "        # X conditioning\n",
        "        if self.x_cond:\n",
        "            print('x conditoning')\n",
        "            self.conditioner_blocks = nn.ModuleList()\n",
        "            conditioner_block = lambda _level: Conditioner(input_shape=z_shapes[_level],\n",
        "                                                          bins=l_bins,\n",
        "                                                          down_t=downs_t[_level],\n",
        "                                                          stride_t=strides_t[_level],\n",
        "                                                          **x_cond_kwargs)\n",
        "            #if dist.get_rank() == 0: print(f\"Conditioning on 1 above level(s)\")\n",
        "            self.conditioner_blocks.append(conditioner_block(self.cond_level))\n",
        "\n",
        "        # Y conditioning\n",
        "        if self.y_cond:\n",
        "            print('y conditoning')\n",
        "            self.n_time = self.z_shape[0] # Assuming STFT=TF order and raw=T1 order, so T is first dim\n",
        "            self.y_emb = LabelConditioner(n_time=self.n_time,include_time_signal=not self.x_cond,**y_cond_kwargs)\n",
        "\n",
        "        # Lyric conditioning\n",
        "        if single_enc_dec:\n",
        "            # Single encoder-decoder transformer\n",
        "            self.prior_shapes = [(self.n_tokens,), prior_kwargs.pop('input_shape')]\n",
        "            self.prior_bins = [prime_kwargs['bins'], prior_kwargs.pop('bins')]\n",
        "            self.prior_dims = [np.prod(shape) for shape in self.prior_shapes]\n",
        "            self.prior_bins_shift = np.cumsum([0, *self.prior_bins])[:-1]\n",
        "            self.prior_width = prior_kwargs['width']\n",
        "            print_once(f'Creating cond. autoregress with prior bins {self.prior_bins}, ')\n",
        "            print_once(f'dims {self.prior_dims}, ')\n",
        "            print_once(f'shift {self.prior_bins_shift}')\n",
        "            print_once(f'input shape {sum(self.prior_dims)}')\n",
        "            print_once(f'input bins {sum(self.prior_bins)}')\n",
        "            print_once(f'Self copy is {self.copy_input}')\n",
        "\n",
        "            self.prime_loss_dims, self.gen_loss_dims = self.prior_dims[0], self.prior_dims[1]\n",
        "            self.total_loss_dims = self.prime_loss_dims + self.gen_loss_dims\n",
        "            self.prior = ConditionalAutoregressive2D(input_shape=(sum(self.prior_dims),),\n",
        "                                                     bins=sum(self.prior_bins),\n",
        "                                                     x_cond=(self.x_cond or self.y_cond), y_cond=True,\n",
        "                                                     prime_len=self.prime_loss_dims,\n",
        "                                                     **prior_kwargs)\n",
        "\n",
        "        else:\n",
        "            # Separate encoder-decoder transformer\n",
        "            if self.n_tokens != 0 and self.use_tokens:\n",
        "                from jukebox.transformer.ops import Conv1D\n",
        "                prime_input_shape = (self.n_tokens,)\n",
        "                self.prime_loss_dims = np.prod(prime_input_shape)\n",
        "                self.prime_acts_width, self.prime_state_width = prime_kwargs['width'], prior_kwargs['width']\n",
        "                self.prime_prior = ConditionalAutoregressive2D(input_shape=prime_input_shape, x_cond=False, y_cond=False,\n",
        "                                                               only_encode=True,\n",
        "                                                               **prime_kwargs)\n",
        "                self.prime_state_proj = Conv1D(self.prime_acts_width, self.prime_state_width, init_scale=prime_kwargs['init_scale'])\n",
        "                self.prime_state_ln = LayerNorm(self.prime_state_width)\n",
        "                self.prime_bins = prime_kwargs['bins']\n",
        "                self.prime_x_out = nn.Linear(self.prime_state_width, self.prime_bins, bias=False)\n",
        "                nn.init.normal_(self.prime_x_out.weight, std=0.02 * prior_kwargs['init_scale'])\n",
        "            else:\n",
        "                self.prime_loss_dims = 0\n",
        "            # This part is used in our case no Lyric conditioning is done therfore prime_loss_dims is zero\n",
        "            self.gen_loss_dims = np.prod(self.z_shape)\n",
        "            self.total_loss_dims = self.prime_loss_dims + self.gen_loss_dims\n",
        "            self.prior = ConditionalAutoregressive2D(x_cond=(self.x_cond or self.y_cond), y_cond=self.y_cond,\n",
        "                                                     encoder_dims = self.prime_loss_dims, merged_decoder=merged_decoder,\n",
        "                                                     **prior_kwargs)\n",
        "\n",
        "        # This is true as self.gen_loss_dims = np.prod(self.z_shape) which gives n_ctx for current level\n",
        "        # This part below mostly looks like calculation involving compression\n",
        "        # For now this part calculates the total compression and tries to figure out the initial sample length before compression vq_vae\n",
        "        self.n_ctx = self.gen_loss_dims\n",
        "        self.downsamples = calculate_strides(strides_t, downs_t)\n",
        "        self.cond_downsample = self.downsamples[level+1] if level != self.levels - 1 else None\n",
        "        self.raw_to_tokens = np.prod(self.downsamples[:level+1])\n",
        "        self.sample_length = self.n_ctx*self.raw_to_tokens\n",
        "\n",
        "        if labels:\n",
        "            self.labels_v3 = labels_v3\n",
        "            self.labeller = Labeller(self.y_emb.max_bow_genre_size, self.n_tokens, self.sample_length, v3=self.labels_v3)\n",
        "        else:\n",
        "            self.labeller = EmptyLabeller()\n",
        "\n",
        "        print(f\"Level:{level}, Cond downsample:{self.cond_downsample}, Raw to tokens:{self.raw_to_tokens}, Sample length:{self.sample_length}\")\n",
        "\n",
        "\n",
        "    def get_y(self, labels, start, get_indices=False):\n",
        "        if isinstance(self.labeller, EmptyLabeller):\n",
        "            return None\n",
        "        y = labels['y'].clone()\n",
        "\n",
        "        # Set sample_length to match this level\n",
        "        y[:, 2] = int(self.sample_length)\n",
        "\n",
        "        # Set offset\n",
        "        y[:, 1:2] = y[:, 1:2] + int(start * self.raw_to_tokens)\n",
        "\n",
        "        # Set lyric tokens\n",
        "        indices = self.labeller.set_y_lyric_tokens(y, labels)\n",
        "        if get_indices:\n",
        "            return y, indices\n",
        "        else:\n",
        "            return y\n",
        "\n",
        "    def get_z_conds(self, zs, start, end):\n",
        "        if self.level != self.levels - 1:\n",
        "            assert start % self.cond_downsample == end % self.cond_downsample == 0\n",
        "            z_cond = zs[self.level + 1][:,start//self.cond_downsample:end//self.cond_downsample]\n",
        "            assert z_cond.shape[1] == self.n_ctx//self.cond_downsample\n",
        "            z_conds = [z_cond]\n",
        "        else:\n",
        "            z_conds = None\n",
        "        return z_conds\n",
        "\n",
        "    def prior_preprocess(self, xs, conds):\n",
        "        N = xs[0].shape[0]\n",
        "        for i in range(len(xs)):\n",
        "            x, shape, dims = xs[i], self.prior_shapes[i], self.prior_dims[i]\n",
        "            bins, bins_shift = int(self.prior_bins[i]), int(self.prior_bins_shift[i])\n",
        "            assert isinstance(x, t.cuda.LongTensor), x\n",
        "            assert (0 <= x).all() and (x < bins).all()\n",
        "            #assert_shape(x, (N, *shape))\n",
        "            xs[i] = (xs[i] + bins_shift).view(N, -1)\n",
        "\n",
        "        for i in range(len(conds)):\n",
        "            cond, shape, dims = conds[i], self.prior_shapes[i], self.prior_dims[i]\n",
        "            if cond is not None:\n",
        "                assert_shape(cond, (N, dims, self.prior_width))\n",
        "            else:\n",
        "                conds[i] = t.zeros((N, dims, self.prior_width), dtype=t.float, device='cuda')\n",
        "\n",
        "        return t.cat(xs, dim=1), t.cat(conds, dim=1)\n",
        "\n",
        "    def prior_postprocess(self, z):\n",
        "        N = z.shape[0]\n",
        "        dims = (self.prior_dims[0], z.shape[1] - self.prior_dims[0])\n",
        "        # xs = list(t.split(z, self.prior_dims, dim=1))\n",
        "        xs = list(t.split(z, dims, dim=1))\n",
        "\n",
        "        for i in range(len(xs)):\n",
        "            # x, shape, dims, bins, bins_shift = xs[i], self.prior_shapes[i], self.prior_dims[i], self.prior_bins[i], self.prior_bins_shift[i]\n",
        "            # assert_shape(x, (N, dims))\n",
        "            shape = self.prior_shapes[i]\n",
        "            bins, bins_shift = int(self.prior_bins[i]), int(self.prior_bins_shift[i])\n",
        "            # xs[i] = (xs[i] - bins_shift).view(N, *shape) #view(N, -1, *shape[1:])\n",
        "            xs[i] = (xs[i] - bins_shift).view(N, -1, *shape[1:])\n",
        "            xs[i] = t.clamp(xs[i], min=0)  # If not masking loss, model may have generated lyric/midi tokens which are now shifted <0 by bin_shift\n",
        "            assert (xs[i] < bins).all(), f'rank: , bins: {bins}, dims {dims}, shape {shape}, prior_shape {self.prior_shapes}, bins_shift {bins_shift}, xs[i]: {xs[i]}'\n",
        "\n",
        "        return xs[-1]\n",
        "\n",
        "    def x_emb(self, z_conds):\n",
        "        z_conds = z_conds[:self.cond_level - self.level]\n",
        "        assert len(z_conds) == len(self.conditioner_blocks) == self.cond_level - self.level, f\"Expected {len(z_conds)} == {len(self.conditioner_blocks)} == {self.cond_level} - {self.level}\"\n",
        "        x_cond = None\n",
        "        for z_cond, conditioner_block in reversed(list(zip(z_conds, self.conditioner_blocks))):\n",
        "            x_cond = conditioner_block(z_cond, x_cond)\n",
        "        return x_cond\n",
        "\n",
        "    def encode(self, x, start_level=None, end_level=None, bs_chunks=1):\n",
        "        if start_level == None:\n",
        "            start_level = self.level\n",
        "        if end_level == None:\n",
        "            end_level = self.levels\n",
        "        # Get latents\n",
        "        with t.no_grad():\n",
        "            zs = self.encoder(x, start_level=start_level, end_level=end_level, bs_chunks=bs_chunks)\n",
        "        return zs\n",
        "\n",
        "    def decode(self, zs, start_level=None, end_level=None, bs_chunks=1):\n",
        "        if start_level == None:\n",
        "            start_level = self.level\n",
        "        if end_level == None:\n",
        "            end_level = self.levels\n",
        "\n",
        "        assert len(zs) == end_level - start_level\n",
        "        with t.no_grad():\n",
        "            x_out = self.decoder(zs, start_level=start_level, end_level=end_level, bs_chunks=bs_chunks)\n",
        "        return x_out\n",
        "\n",
        "    def get_cond(self, z_conds, y):\n",
        "        if y is not None:\n",
        "            assert y.shape[1] == 4 + self.y_emb.max_bow_genre_size + self.n_tokens, f\"Expected {4} + {self.y_emb.max_bow_genre_size} + {self.n_tokens}, got {y.shape[1]}\"\n",
        "            n_labels = y.shape[1] - self.n_tokens\n",
        "            y, prime = y[:,:n_labels], y[:,n_labels:]\n",
        "        else:\n",
        "            y, prime = None, None\n",
        "        y_cond, y_pos = self.y_emb(y) if self.y_cond else (None, None)\n",
        "        x_cond = self.x_emb(z_conds) if self.x_cond else y_pos\n",
        "        return x_cond, y_cond, prime\n",
        "\n",
        "    def sample(self, n_samples, z=None, z_conds=None, y=None, fp16=False, temp=1.0, top_k=0, top_p=0.0,\n",
        "               chunk_size=None, sample_tokens=None):\n",
        "        N = n_samples\n",
        "        if z is not None: assert z.shape[0] == N, f\"Expected shape ({N},**), got shape {z.shape}\"\n",
        "        if y is not None: assert y.shape[0] == N, f\"Expected shape ({N},**), got shape {y.shape}\"\n",
        "        if z_conds is not None:\n",
        "            for z_cond in z_conds:\n",
        "                assert z_cond.shape[0] == N,  f\"Expected shape ({N},**), got shape {z_cond.shape}\"\n",
        "\n",
        "\n",
        "        no_past_context = (z is None or z.shape[1] == 0)\n",
        "        #if dist.get_rank() == 0:\n",
        "        #    name = {True: 'Ancestral', False: 'Primed'}[no_past_context]\n",
        "        #    print(f\"{name} sampling {n_samples} samples with temp={temp}, top_k={top_k}, top_p={top_p}\")\n",
        "\n",
        "        with t.no_grad():\n",
        "            # Currently x_cond only uses immediately above layer\n",
        "            x_cond, y_cond, prime = self.get_cond(z_conds, y)\n",
        "            if self.single_enc_dec:\n",
        "                # assert chunk_size % self.prime_loss_dims == 0. TODO: Check if needed\n",
        "                if no_past_context:\n",
        "                    z, x_cond = self.prior_preprocess([prime], [None, x_cond])\n",
        "                else:\n",
        "                    z, x_cond = self.prior_preprocess([prime, z], [None, x_cond])\n",
        "                if sample_tokens is not None:\n",
        "                    sample_tokens += self.n_tokens\n",
        "                z = self.prior.primed_sample(n_samples, z, x_cond, y_cond, fp16=fp16, temp=temp,\n",
        "                                             top_k=top_k, top_p=top_p, chunk_size=chunk_size, sample_tokens=sample_tokens)\n",
        "                z = self.prior_postprocess(z)\n",
        "            else:\n",
        "                # This part is used in our case and z decides wheter it is prime or not for now\n",
        "                encoder_kv = self.get_encoder_kv(prime, fp16=fp16, sample=True)\n",
        "                if no_past_context:\n",
        "                    z = self.prior.sample(n_samples, x_cond, y_cond, encoder_kv, fp16=fp16, temp=temp, top_k=top_k,\n",
        "                                          top_p=top_p, sample_tokens=sample_tokens)\n",
        "                else:\n",
        "                    z = self.prior.primed_sample(n_samples, z, x_cond, y_cond, encoder_kv, fp16=fp16, temp=temp,\n",
        "                                             top_k=top_k, top_p=top_p, chunk_size=chunk_size, sample_tokens=sample_tokens)\n",
        "            if sample_tokens is None:\n",
        "                assert_shape(z, (N, *self.z_shape))\n",
        "        return z\n",
        "\n",
        "    # imporatant\n",
        "    def get_encoder_kv(self, prime, fp16=False, sample=False):\n",
        "        if self.n_tokens != 0 and self.use_tokens:\n",
        "            if sample:\n",
        "                self.prime_prior.cuda()\n",
        "            N = prime.shape[0]\n",
        "            prime_acts = self.prime_prior(prime, None, None, None, fp16=fp16)\n",
        "            assert_shape(prime_acts, (N, self.prime_loss_dims, self.prime_acts_width))\n",
        "            assert prime_acts.dtype == t.float, f'Expected t.float, got {prime_acts.dtype}'\n",
        "            encoder_kv = self.prime_state_ln(self.prime_state_proj(prime_acts))\n",
        "            assert encoder_kv.dtype == t.float, f'Expected t.float, got {encoder_kv.dtype}'\n",
        "            if sample:\n",
        "                self.prime_prior.cpu()\n",
        "                if fp16:\n",
        "                    encoder_kv = encoder_kv.half()\n",
        "        else:\n",
        "            encoder_kv = None\n",
        "        return encoder_kv\n",
        "\n",
        "    def get_prime_loss(self, encoder_kv, prime_t):\n",
        "        if self.use_tokens:\n",
        "            encoder_kv = encoder_kv.float()\n",
        "            encoder_kv = self.prime_x_out(encoder_kv)\n",
        "            prime_loss = nn.functional.cross_entropy(encoder_kv.view(-1, self.prime_bins), prime_t.view(-1)) / np.log(2.)\n",
        "        else:\n",
        "            prime_loss = t.tensor(0.0, device='cuda')\n",
        "        return prime_loss\n",
        "\n",
        "\n",
        "    # important note search here\n",
        "    def z_forward(self, z, z_conds=[], y=None, fp16=False, get_preds=False, get_attn_weights=False):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            get_attn_weights (bool or set): Makes forward prop dump\n",
        "                self-attention softmaxes to self.prior.transformer.ws. Either a\n",
        "                set of layer indices indicating which layers to store, or a\n",
        "                boolean value indicating whether to dump all.\n",
        "        \"\"\"\n",
        "        assert isinstance(get_attn_weights, (bool, set))\n",
        "        if get_attn_weights:\n",
        "            self.prior.transformer.set_record_attn(get_attn_weights)\n",
        "        x_cond, y_cond, prime = self.get_cond(z_conds, y)\n",
        "        if self.copy_input:\n",
        "            prime = z[:,:self.n_tokens]\n",
        "        if self.single_enc_dec:\n",
        "            z, x_cond = self.prior_preprocess([prime, z], [None, x_cond])\n",
        "            (prime_loss, gen_loss), preds = self.prior(z, x_cond, y_cond, fp16=fp16, get_sep_loss=True, get_preds=get_preds)\n",
        "        else:\n",
        "            # The part below is used here in our prior with encoder_kv and prime_loss to be not taken in account\n",
        "            encoder_kv = self.get_encoder_kv(prime, fp16=fp16)\n",
        "            prime_loss = self.get_prime_loss(encoder_kv, prime)\n",
        "            gen_loss, preds = self.prior(z, x_cond, y_cond, encoder_kv, fp16=fp16, get_preds=get_preds)\n",
        "        loss = (self.prime_loss_fraction*prime_loss*self.prime_loss_dims/self.total_loss_dims) + \\\n",
        "                   (gen_loss*self.gen_loss_dims/self.total_loss_dims)\n",
        "        metrics=dict(bpd=gen_loss.clone().detach(), prime_loss=prime_loss.clone().detach(),\n",
        "                     gen_loss=gen_loss.clone().detach())\n",
        "        if get_preds:\n",
        "            metrics[\"preds\"] = preds.clone().detach()\n",
        "        if get_attn_weights:\n",
        "            ws = self.prior.transformer.ws\n",
        "            self.prior.transformer.set_record_attn(False)\n",
        "            return ws\n",
        "        else:\n",
        "            return loss, metrics\n",
        "\n",
        "    def forward(self, x, y=None, fp16=False, decode=False, get_preds=False):\n",
        "        bs = x.shape[0]\n",
        "        z, *z_conds = self.encode(x, bs_chunks=bs)\n",
        "        loss, metrics = self.z_forward(z=z, z_conds=z_conds, y=y, fp16=fp16, get_preds=get_preds)\n",
        "        if decode:\n",
        "            x_out = self.decode([z, *z_conds])\n",
        "        else:\n",
        "            x_out = None\n",
        "        return x_out, loss, metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e_UGC9kPs2W"
      },
      "source": [
        "## **hps and checkpoints**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "HPARAMS_REGISTRY = {}\n",
        "\n",
        "class Hyperparams(dict):\n",
        "    def __getattr__(self, attr):\n",
        "        return self[attr]\n",
        "\n",
        "    def __setattr__(self, attr, value):\n",
        "        self[attr] = value\n",
        "\n",
        "def setup_hparams(hparam_set_names, kwargs):\n",
        "    H = Hyperparams()\n",
        "    if not isinstance(hparam_set_names, tuple):\n",
        "        hparam_set_names = hparam_set_names.split(\",\")\n",
        "    hparam_sets = [HPARAMS_REGISTRY[x.strip()] for x in hparam_set_names if x] + [kwargs]\n",
        "    for k, v in DEFAULTS.items():\n",
        "        H.update(v)\n",
        "    for hps in hparam_sets:\n",
        "        for k in hps:\n",
        "            if k not in H:\n",
        "                raise ValueError(f\"{k} not in default args\")\n",
        "        H.update(**hps)\n",
        "    H.update(**kwargs)\n",
        "    return H"
      ],
      "metadata": {
        "id": "eUdrLXuvf_BA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iT2Wctm-PvSh"
      },
      "outputs": [],
      "source": [
        "# Model hps\n",
        "pr = Hyperparams(\n",
        "    # Prior\n",
        "    prior = False,\n",
        "    n_vocab=1024,\n",
        "    restore_prior='/content/drive/MyDrive/Indian_Classical_Music_Generation/checkpoints/prior/flute_checkpoint_latest.pth.tar',\n",
        "    restore_prior_ddp=False,\n",
        "    max_bow_genre_size=None,\n",
        "    y_bins=0,\n",
        "    level=0,\n",
        "    cond_levels=None,\n",
        "    t_bins=64,\n",
        "    y_cond_as_bias=False,\n",
        "    copy_input=False,\n",
        "    merged_decoder=True,\n",
        "    single_enc_dec=False,\n",
        "    alignment_layer=None,\n",
        "    alignment_head=None,\n",
        "\n",
        "    # context length\n",
        "    n_ctx=8250,\n",
        "    prior_depth=16,         # prior_depth=3\n",
        "    prior_width=2048,      # prior_width=128\n",
        "    heads=8,               # heads = 1\n",
        "    attn_order=2,\n",
        "    blocks=110,            #blocks = 15\n",
        "    spread=None,\n",
        "    attn_dropout=0.0,\n",
        "    resid_dropout=0.0,\n",
        "    emb_dropout=0.0,\n",
        "    zero_out=False,\n",
        "    res_scale=False,\n",
        "    pos_init=False,\n",
        "    init_scale=0.7,\n",
        "    m_attn=0.25,\n",
        "    m_mlp=1.0,\n",
        "    c_res=0,\n",
        "    c_attn=0,\n",
        "    c_mlp=0,\n",
        "    cond_depth=3,\n",
        "    cond_width=128,\n",
        "    cond_m_conv=1.0,\n",
        "    cond_zero_out=False,\n",
        "    cond_res_scale=False,\n",
        "    cond_dilation_growth_rate=1,\n",
        "    cond_dilation_cycle=None,\n",
        "    cond_c_res=0,\n",
        "    min_duration = 23,\n",
        "    max_duration = 600,\n",
        "    use_tokens=False,\n",
        "    n_tokens=0,\n",
        "    prime_loss_fraction=0.0,\n",
        "    fp16_params=False,\n",
        "    labels = False,\n",
        "    labels_v3 = False,\n",
        "    iters_before_update=1,\n",
        "    bs_sample=1,\n",
        "\n",
        "    # VQVAE\n",
        "    sr = 11000,\n",
        "    levels = 1,\n",
        "    downs_t = (5, 5),\n",
        "    strides_t = (2, 2),\n",
        "    emb_width = 64,\n",
        "    l_bins = 1024,\n",
        "    l_mu = 0.99,\n",
        "    commit = 0.02,\n",
        "    spectral = 0.0,\n",
        "    multispectral = 1.0,\n",
        "    loss_fn = 'l2',\n",
        "    width = 32,\n",
        "    depth = 4,\n",
        "    m_conv = 1.0,\n",
        "    dilation_growth_rate = 3,\n",
        "    revival_threshold=1.0,\n",
        "    hvqvae_multipliers = None,\n",
        "    lmix_l1=0.0,\n",
        "    lmix_l2 = 1.0,\n",
        "    lmix_linf=0.02,\n",
        "    linf_k=2,\n",
        "    use_bottleneck=True,\n",
        "    dilation_cycle=None,\n",
        "    vqvae_reverse_decoder_dilation=True,\n",
        "    sample_length = 24.0*11000,\n",
        "    restore_vqvae='/content/drive/MyDrive/Indian_Classical_Music_Generation/checkpoints/vq_vae/vqvae-flute/1/checkpoint_step_1.pth.tar',\n",
        "    lr=0.000007,\n",
        "    clip=1.0,\n",
        "    beta1=0.9,\n",
        "    beta2=0.999,\n",
        "    ignore_grad_norm=0,\n",
        "    weight_decay=0.0,\n",
        "    eps=1e-08,\n",
        "    lr_warmup=100.0,\n",
        "    lr_decay=10000000000.0,\n",
        "    lr_gamma=1.0,\n",
        "    lr_scale=1.0,\n",
        "    lr_use_linear_decay=False,\n",
        "    lr_start_linear_decay=0,\n",
        "    lr_use_cosine_decay=False,\n",
        "    save_iters = 1000,\n",
        "    save = True,\n",
        "    aug_blend=False,\n",
        "    n_inps=1,\n",
        "    n_hops=2,\n",
        "    n_segment=1,\n",
        "    n_total_segment=1,\n",
        "    n_segment_each=1,\n",
        "    prime_chunks=4,\n",
        "    sample_hop_length=30000,\n",
        "    max_silence_pad_length=0,\n",
        "    ignore_boundaries=False,\n",
        "    use_nonrelative_specloss=True,\n",
        "    multispec_loss_n_fft=(2048,1024,512),\n",
        "    multispec_loss_hop_length=(240,120,50),\n",
        "    multispec_loss_window_size=(1200,600,240),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 44100 x 24 = .....\n",
        "# 12 swaras standing 3 octaves\n",
        "# 21 x 3\n",
        "\n",
        "# 11200 x 30 ->\n",
        "-> 128 -> 11200 / 128 x 30s\n",
        "-> 32\n",
        "\n",
        "x_1 ... x_11200 x 30s -> Amplitude\n",
        "\n",
        "x_1 ... x_11200/128\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UqTU1Bbox8QM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTtIck-99sGh"
      },
      "outputs": [],
      "source": [
        "def load_checkpoint(path):\n",
        "    restore = path\n",
        "    if restore[:5] == 'gs://':\n",
        "        gs_path = restore\n",
        "        local_path = os.path.join(os.path.expanduser(\"~/.cache\"), gs_path[5:])\n",
        "        if dist.get_rank() % 8 == 0:\n",
        "            print(\"Downloading from gce\")\n",
        "            if not os.path.exists(os.path.dirname(local_path)):\n",
        "                os.makedirs(os.path.dirname(local_path))\n",
        "            if not os.path.exists(local_path):\n",
        "                download(gs_path, local_path)\n",
        "        restore = local_path\n",
        "    checkpoint = t.load(restore, map_location=t.device('cpu'))\n",
        "    print(\"Restored from {}\".format(restore))\n",
        "    return checkpoint\n",
        "\n",
        "def save_checkpoint(i,name, model, opt, metrics, hps):\n",
        "    with t.no_grad():\n",
        "        prefix = '/content/drive/My Drive/audio_VAE/checkpoints_prior2/top_level'\n",
        "        # name = time.strftime(prefix + '%m%d_%H_%M_%S.pth.tr')\n",
        "        save_hps = {**hps}\n",
        "        save_hps = {k: v for k,v in save_hps.items() if k not in ['metadata_v2','metadata_v3', 'alignments', 'lyric_processor', 'midi_processor']}\n",
        "        t.save({'hps': save_hps,\n",
        "                'model': model.state_dict(), # should also save bottleneck k's as buffers\n",
        "                'opt': opt.state_dict() if opt is not None else None,\n",
        "                'step': i,\n",
        "                **metrics},f'{prefix}/checkpoint_{name}.pth.tar')\n",
        "    return\n",
        "\n",
        "def restore_model(hps, model, checkpoint_path):\n",
        "    model.step = 0\n",
        "    if checkpoint_path != '':\n",
        "        checkpoint = load_checkpoint(checkpoint_path)\n",
        "        # checkpoint_hps = Hyperparams(**checkpoint['hps'])\n",
        "        # for k in set(checkpoint_hps.keys()).union(set(hps.keys())):\n",
        "        #     if checkpoint_hps.get(k, None) != hps.get(k, None):\n",
        "        #         print(k, \"Checkpoint:\", checkpoint_hps.get(k, None), \"Ours:\", hps.get(k, None))\n",
        "        checkpoint['model'] = {k[7:] if k[:7] == 'module.' else k: v for k, v in checkpoint['model'].items()}\n",
        "        model.load_state_dict(checkpoint['model'])\n",
        "        if 'step' in checkpoint: model.step = checkpoint['step']\n",
        "\n",
        "def restore_opt(opt, shd, checkpoint_path):\n",
        "    if not checkpoint_path:\n",
        "        return\n",
        "    checkpoint = load_checkpoint(checkpoint_path)\n",
        "    if \"opt\" in checkpoint:\n",
        "        opt.load_state_dict(checkpoint['opt'])\n",
        "    if \"step\" in checkpoint:\n",
        "        shd.step(checkpoint['step'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llLoPQXePA-3"
      },
      "source": [
        "## **Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uOwHOWdXtAn"
      },
      "outputs": [],
      "source": [
        "def make_prior(hps, vqvae,train,device='cuda'):\n",
        "    #from jukebox.prior.prior import SimplePrior\n",
        "\n",
        "    prior_kwargs = dict(input_shape=(hps.n_ctx,), bins=vqvae.l_bins,\n",
        "                        width=hps.prior_width, depth=hps.prior_depth, heads=hps.heads,\n",
        "                        attn_order=hps.attn_order, blocks=hps.blocks, spread=hps.spread,\n",
        "                        attn_dropout=hps.attn_dropout, resid_dropout=hps.resid_dropout, emb_dropout=hps.emb_dropout,\n",
        "                        zero_out=hps.zero_out, res_scale=hps.res_scale, pos_init=hps.pos_init,\n",
        "                        init_scale=hps.init_scale,\n",
        "                        m_attn=hps.m_attn, m_mlp=hps.m_mlp,\n",
        "                        checkpoint_res=hps.c_res if train else 0, checkpoint_attn=hps.c_attn if train else 0, checkpoint_mlp=hps.c_mlp if train else 0)\n",
        "\n",
        "    x_cond_kwargs = dict(out_width=hps.prior_width, init_scale=hps.init_scale,\n",
        "                         width=hps.cond_width, depth=hps.cond_depth, m_conv=hps.cond_m_conv,\n",
        "                         dilation_growth_rate=hps.cond_dilation_growth_rate, dilation_cycle=hps.cond_dilation_cycle,\n",
        "                         zero_out=hps.cond_zero_out, res_scale=hps.cond_res_scale,\n",
        "                         checkpoint_res=hps.cond_c_res)  # have to keep this else names wrong\n",
        "\n",
        "    y_cond_kwargs = dict(out_width=hps.prior_width, init_scale=hps.init_scale,\n",
        "                         y_bins=hps.y_bins, t_bins=hps.t_bins, sr= hps.sr, min_duration=hps.min_duration,\n",
        "                         max_duration=hps.max_duration, max_bow_genre_size=hps.max_bow_genre_size)\n",
        "\n",
        "    if hps.use_tokens and not hps.single_enc_dec:\n",
        "        prime_kwargs = dict(use_tokens=hps.use_tokens, prime_loss_fraction=hps.prime_loss_fraction,\n",
        "                            n_tokens=hps.n_tokens, bins=hps.n_vocab,\n",
        "                            width=hps.prime_width, depth=hps.prime_depth, heads=hps.prime_heads,\n",
        "                            attn_order=hps.prime_attn_order, blocks=hps.prime_blocks, spread=hps.prime_spread,\n",
        "                            attn_dropout=hps.prime_attn_dropout, resid_dropout=hps.prime_resid_dropout,\n",
        "                            emb_dropout=hps.prime_emb_dropout,\n",
        "                            zero_out=hps.prime_zero_out, res_scale=hps.prime_res_scale,\n",
        "                            pos_init=hps.prime_pos_init, init_scale=hps.prime_init_scale,\n",
        "                            m_attn=hps.prime_m_attn, m_mlp=hps.prime_m_mlp,\n",
        "                            checkpoint_res=hps.prime_c_res if train else 0, checkpoint_attn=hps.prime_c_attn if train else 0,\n",
        "                            checkpoint_mlp=hps.prime_c_mlp if train else 0)\n",
        "    else:\n",
        "        prime_kwargs = dict(use_tokens=hps.use_tokens, prime_loss_fraction=hps.prime_loss_fraction,\n",
        "                            n_tokens=hps.n_tokens, bins=hps.n_vocab)\n",
        "\n",
        "    # z_shapes for other levels given this level gets n_ctx codes ()\n",
        "    rescale = lambda z_shape: (z_shape[0]*hps.n_ctx//vqvae.z_shapes[hps.level][0],)\n",
        "    z_shapes = [rescale(z_shape) for z_shape in vqvae.z_shapes]\n",
        "\n",
        "    prior = SimplePrior(z_shapes=z_shapes,\n",
        "                        l_bins=vqvae.l_bins,\n",
        "                        encoder=vqvae.encode,\n",
        "                        decoder=vqvae.decode,\n",
        "                        level=hps.level,\n",
        "                        downs_t=vqvae.downs_t,\n",
        "                        strides_t=vqvae.strides_t,\n",
        "                        labels=hps.labels,\n",
        "                        prior_kwargs=prior_kwargs,\n",
        "                        x_cond_kwargs=x_cond_kwargs,\n",
        "                        y_cond_kwargs=y_cond_kwargs,\n",
        "                        prime_kwargs=prime_kwargs,\n",
        "                        copy_input=hps.copy_input,\n",
        "                        labels_v3=hps.labels_v3,\n",
        "                        merged_decoder=hps.merged_decoder,\n",
        "                        single_enc_dec=hps.single_enc_dec)\n",
        "\n",
        "    prior.alignment_head = hps.get('alignment_head', None)\n",
        "    prior.alignment_layer = hps.get('alignment_layer', None)\n",
        "\n",
        "    if hps.fp16_params:\n",
        "        print(\"Converting to fp16 params\")\n",
        "        from jukebox.transformer.ops import _convert_conv_weights_to_fp16\n",
        "        prior.apply(_convert_conv_weights_to_fp16)\n",
        "    prior = prior.to(device)\n",
        "    restore_model(hps, prior, hps.restore_prior)\n",
        "    if train:\n",
        "        print(f\"Loading prior in train mode\")\n",
        "        pass\n",
        "    else:\n",
        "        print(f\"Loading prior in eval mode\")\n",
        "        prior.eval()\n",
        "        freeze_model(prior)\n",
        "    return prior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeCFGt-J9wUc"
      },
      "outputs": [],
      "source": [
        "def make_vqvae(hps, device='cuda',train = False):\n",
        "    block_kwargs = dict(width=hps.width, depth=hps.depth, m_conv=hps.m_conv,\n",
        "                        dilation_growth_rate=hps.dilation_growth_rate,\n",
        "                        dilation_cycle=hps.dilation_cycle,\n",
        "                        reverse_decoder_dilation=hps.vqvae_reverse_decoder_dilation)\n",
        "\n",
        "    if not hps.sample_length:\n",
        "        assert hps.sample_length_in_seconds != 0\n",
        "        downsamples = calculate_strides(hps.strides_t, hps.downs_t)\n",
        "        top_raw_to_tokens = np.prod(downsamples)\n",
        "        hps.sample_length = (hps.sample_length_in_seconds * hps.sr // top_raw_to_tokens) * top_raw_to_tokens\n",
        "        print(f\"Setting sample length to {hps.sample_length} (i.e. {hps.sample_length/hps.sr} seconds) to be multiple of {top_raw_to_tokens}\")\n",
        "\n",
        "    vqvae = VQVAE(input_shape=(hps.sample_length,1), levels=hps.levels, downs_t=hps.downs_t, strides_t=hps.strides_t,\n",
        "                  emb_width=hps.emb_width, l_bins=hps.l_bins,\n",
        "                  mu=hps.l_mu, commit=hps.commit,\n",
        "                  spectral=hps.spectral, multispectral=hps.multispectral,\n",
        "                  multipliers=hps.hvqvae_multipliers, use_bottleneck=hps.use_bottleneck,\n",
        "                  **block_kwargs)\n",
        "\n",
        "    vqvae = vqvae.to(device)\n",
        "    restore_model(hps, vqvae, hps.restore_vqvae)\n",
        "    if train:\n",
        "        print(f\"Loading vqvae in train mode\")\n",
        "        if hps.restore_vqvae != '':\n",
        "            print(\"Reseting bottleneck emas\")\n",
        "            for level, bottleneck in enumerate(vqvae.bottleneck.level_blocks):\n",
        "                num_samples = hps.sample_length\n",
        "                downsamples = calculate_strides(hps.strides_t, hps.downs_t)\n",
        "                raw_to_tokens = np.prod(downsamples[:level + 1])\n",
        "                num_tokens = (num_samples // raw_to_tokens)\n",
        "                bottleneck.restore_k(num_tokens=num_tokens, threshold=hps.revival_threshold)\n",
        "    else:\n",
        "        print(f\"Loading vqvae in eval mode\")\n",
        "        vqvae.eval()\n",
        "        freeze_model(vqvae)\n",
        "    return vqvae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqqtlmNNx7M_"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9QNE50sRKb3",
        "outputId": "0db5b69f-c5dc-48d6-91a1-cca4cc5a6626"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Restored from /content/drive/MyDrive/Indian_Classical_Music_Generation/checkpoints/vq_vae/vqvae-flute/1/checkpoint_step_1.pth.tar\n",
            "Loading vqvae in eval mode\n"
          ]
        }
      ],
      "source": [
        "vqvae = make_vqvae(pr, device = t.device(\"cuda\"), train=False)\n",
        "#print_once(f\"Parameters VQVAE:{count_parameters(vqvae)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDre0KLORR7k",
        "outputId": "bc450fac-df72-4036-e0cd-16da6139f98b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Level:0, Cond downsample:None, Raw to tokens:32, Sample length:264000.0\n",
            "Restored from /content/drive/MyDrive/Indian_Classical_Music_Generation/checkpoints/prior/flute_checkpoint_latest.pth.tar\n",
            "Loading prior in eval mode\n"
          ]
        }
      ],
      "source": [
        "prior = make_prior(pr, vqvae, train = False)\n",
        "#print_once(f\"Parameters Prior:{count_parameters(prior)}\")\n",
        "model = prior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgfqB8ib3eFb",
        "outputId": "4ddb285e-0edd-479e-fa41-0f81ae11f031"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimplePrior(\n",
            "  (prior): ConditionalAutoregressive2D(\n",
            "    (x_emb): Embedding(1024, 2048)\n",
            "    (x_emb_dropout): Dropout(p=0.0, inplace=False)\n",
            "    (pos_emb): PositionEmbedding()\n",
            "    (pos_emb_dropout): Dropout(p=0.0, inplace=False)\n",
            "    (transformer): Transformer(\n",
            "      (_attn_mods): ModuleList(\n",
            "        (0-15): 16 x ResAttnBlock(\n",
            "          (attn): FactoredAttention(\n",
            "            (c_attn): Conv1D()\n",
            "            (c_proj): Conv1D()\n",
            "          )\n",
            "          (ln_0): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): MLP(\n",
            "            (c_fc): Conv1D()\n",
            "            (c_proj): Conv1D()\n",
            "          )\n",
            "          (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (x_out): Linear(in_features=2048, out_features=1024, bias=False)\n",
            "    (loss): CrossEntropyLoss()\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uq2hwy3Nx3dB",
        "outputId": "5395e950-a534-42f9-e092-d7927ff7e4ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model parameters :  0\n"
          ]
        }
      ],
      "source": [
        "print(f'model parameters : ',count_parameters(vqvae))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcOsgyx7y7t5"
      },
      "source": [
        "# **Inference**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxrtKlvsTh8i"
      },
      "outputs": [],
      "source": [
        "def sample_prior(orig_model, x_in, y, hps,i):\n",
        "    #if ema is not None: ema.swap()\n",
        "    orig_model.eval()\n",
        "\n",
        "    x_in = audio_preprocess(x_in, hps)\n",
        "\n",
        "    x_in = x_in[:hps.bs_sample]\n",
        "    bs = x_in.shape[0]\n",
        "    zs_in = orig_model.encode(x_in, start_level=0, bs_chunks=bs)\n",
        "    assert len(zs_in) == hps.levels\n",
        "    x_ds = [orig_model.decode(zs_in[level:], start_level=level, bs_chunks=bs) for level in range(0, hps.levels)]\n",
        "\n",
        "\n",
        "    if not hps.labels:\n",
        "        y = None\n",
        "    elif hps.level == (hps.levels - 1):\n",
        "        # Topmost level labels in order\n",
        "        y = y[:hps.bs_sample]  # t.ones((hps.bs_sample, 1), device=y.device, dtype=t.long) * dist.get_rank()\n",
        "    else:\n",
        "        # Other levels keep labels to match x_cond\n",
        "        y = y[:hps.bs_sample]\n",
        "\n",
        "    # Temp 1.0\n",
        "    z_enc, *z_conds = orig_model.encode(x_in, bs_chunks=bs)\n",
        "\n",
        "    # print(z_enc[:,:512].shape)\n",
        "\n",
        "    z = orig_model.sample(hps.bs_sample,z = z_enc[:,:512],z_conds=z_conds, y=y, fp16=False, temp=1.0)\n",
        "\n",
        "    # print(z_enc.shape)\n",
        "    # print(z.shape)\n",
        "    # print(*z_conds)\n",
        "\n",
        "    # z = t.cat((z_enc,z),dim = 1)\n",
        "\n",
        "    x_sample = orig_model.decode([z, *z_conds], bs_chunks=bs)\n",
        "    x_out = orig_model.decode([z_enc,*z_conds],bs_chunks = bs)\n",
        "\n",
        "    save_to_wav('/content/drive/My Drive/audio_VAE/final_sample/primed_classical/',x_sample,11000,i)\n",
        "    save_to_wav('/content/drive/My Drive/audio_VAE/final_sample/primed_classical/',x_out,11000,i,input = True)\n",
        "    #log_aud(logger, 'sample_x_T1', x_sample, hps)\n",
        "    #if hps.prior and hps.labels:\n",
        "    #    log_labels(logger, orig_model.labeller, f'sample_x_T1', allgather(y.cuda()), hps)\n",
        "\n",
        "    # Recons\n",
        "    #for i in range(len(x_ds)):\n",
        "    #    log_aud(logger, f'x_ds_start_{i}', x_ds[i], hps)\n",
        "    return x_sample\n",
        "    #if ema is not None: ema.swap()\n",
        "    #logger.flush()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VtGFCwa2pjk"
      },
      "outputs": [],
      "source": [
        "import torch as t\n",
        "\n",
        "def split_batch(obj, n_samples, split_size):\n",
        "    n_passes = (n_samples + split_size - 1) // split_size\n",
        "    if isinstance(obj, t.Tensor):\n",
        "        return t.split(obj, split_size, dim=0)\n",
        "    elif isinstance(obj, list):\n",
        "        return list(zip(*[t.split(item, split_size, dim=0) for item in obj]))\n",
        "    elif obj is None:\n",
        "        return [None] * n_passes\n",
        "    else:\n",
        "        raise TypeError('Unknown input type')\n",
        "\n",
        "# Break total_length into hops/windows of size n_ctx separated by hop_length\n",
        "def get_starts(total_length, n_ctx, hop_length):\n",
        "    starts = []\n",
        "    n_ctx = int(n_ctx)\n",
        "    for start in range(0, total_length - n_ctx + hop_length, hop_length):\n",
        "        if start + n_ctx >= total_length:\n",
        "            # Last hop could be smaller, we make it n_ctx to maximise context\n",
        "            start = total_length - n_ctx\n",
        "        starts.append(start)\n",
        "    # print(starts)\n",
        "    return starts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWoO1-eZN4xi"
      },
      "outputs": [],
      "source": [
        "# Sample a partial window of length<n_ctx with tokens_to_sample new tokens on level=level\n",
        "def sample_partial_window(zs, labels, sampling_kwargs, level, prior, tokens_to_sample, hps):\n",
        "    z = zs[level]\n",
        "    n_ctx = prior.n_ctx\n",
        "    current_tokens = z.shape[1]\n",
        "    if current_tokens < n_ctx - tokens_to_sample:\n",
        "        sampling_kwargs['sample_tokens'] = current_tokens + tokens_to_sample\n",
        "        start = 0\n",
        "    else:\n",
        "        sampling_kwargs['sample_tokens'] = n_ctx\n",
        "        start = current_tokens - n_ctx + tokens_to_sample\n",
        "\n",
        "    return sample_single_window(zs, labels, sampling_kwargs, level, prior, start, hps)\n",
        "\n",
        "# Sample a single window of length=n_ctx at position=start on level=level\n",
        "def sample_single_window(zs, labels, sampling_kwargs, level, prior, start, hps):\n",
        "\n",
        "    # 8192\n",
        "    n_samples = 1\n",
        "    n_ctx = prior.n_ctx\n",
        "    end = start + n_ctx\n",
        "\n",
        "    # get z already sampled at current level\n",
        "\n",
        "    start = int(start)\n",
        "    end = int(end)\n",
        "\n",
        "    z = zs[level][:,start:end]\n",
        "\n",
        "    if 'sample_tokens' in sampling_kwargs:\n",
        "        # Support sampling a window shorter than n_ctx\n",
        "        sample_tokens = sampling_kwargs['sample_tokens']\n",
        "    else:\n",
        "        sample_tokens = (end - start)\n",
        "    conditioning_tokens, new_tokens = z.shape[1], sample_tokens - z.shape[1]\n",
        "\n",
        "    print(f'new_tokens : {new_tokens}')\n",
        "    print(f\"Sampling {sample_tokens} tokens for [{start},{start+sample_tokens}]. Conditioning on {conditioning_tokens} tokens\")\n",
        "\n",
        "    if new_tokens <= 0:\n",
        "        # Nothing new to sample\n",
        "        return zs\n",
        "\n",
        "    # get z_conds from level above\n",
        "    z_conds = prior.get_z_conds(zs, start, end)\n",
        "    if(z_conds == None):\n",
        "      print(f'z_conds at level : {level} : {z_conds}')\n",
        "    else:\n",
        "      print(f'z_conds at level : {level} : {z_conds[0].shape}')\n",
        "\n",
        "    # set y offset, sample_length and lyrics tokens\n",
        "    y = prior.get_y(labels, start)\n",
        "\n",
        "    empty_cache()\n",
        "\n",
        "    max_batch_size = sampling_kwargs['max_batch_size']\n",
        "    del sampling_kwargs['max_batch_size']\n",
        "\n",
        "\n",
        "    z_list = split_batch(z, n_samples, max_batch_size)\n",
        "    z_conds_list = split_batch(z_conds, n_samples, max_batch_size)\n",
        "    y_list = split_batch(y, n_samples, max_batch_size)\n",
        "    z_samples = []\n",
        "    for z_i, z_conds_i, y_i in zip(z_list, z_conds_list, y_list):\n",
        "        z_samples_i = prior.sample(n_samples=z_i.shape[0], z=z_i, z_conds=z_conds_i, y=y_i, **sampling_kwargs)\n",
        "        z_samples.append(z_samples_i)\n",
        "\n",
        "    z = t.cat(z_samples, dim=0)\n",
        "    sampling_kwargs['max_batch_size'] = max_batch_size\n",
        "\n",
        "    # Update z with new sample\n",
        "    z_new = z[:,-new_tokens:]\n",
        "    zs[level] = t.cat([zs[level], z_new], dim=1)\n",
        "    return zs\n",
        "\n",
        "# Sample total_length tokens at level=level with hop_length=hop_length\n",
        "def sample_level(zs, labels, sampling_kwargs, level, prior, total_length, hop_length, hps):\n",
        "    print(f\"Sampling level {level}\")\n",
        "    if total_length >= prior.n_ctx:\n",
        "        for start in get_starts(total_length, prior.n_ctx, hop_length):\n",
        "            # print(zs[1].shape)\n",
        "            zs = sample_single_window(zs, labels, sampling_kwargs, level, prior, start, hps)\n",
        "    else:\n",
        "        zs = sample_partial_window(zs, labels, sampling_kwargs, level, prior, total_length, hps)\n",
        "    return zs\n",
        "\n",
        "# Sample multiple levels\n",
        "def _sample(zs, labels, sampling_kwargs, priors, sample_levels, hps):\n",
        "    alignments = None\n",
        "    for level in reversed(sample_levels):\n",
        "        prior = priors[level]\n",
        "        prior.cuda()\n",
        "        empty_cache()\n",
        "\n",
        "        # Set correct total_length, hop_length, labels and sampling_kwargs for level\n",
        "        #\n",
        "        assert hps.sample_length % prior.raw_to_tokens == 0, f\"Expected sample_length {hps.sample_length} to be multiple of {prior.raw_to_tokens}\"\n",
        "        total_length = int(hps.sample_length//prior.raw_to_tokens)\n",
        "\n",
        "        # print(f'total_length at {level} : {total_length}')\n",
        "\n",
        "        hop_length = int(prior.n_ctx)\n",
        "\n",
        "        if(labels == None):\n",
        "            p_labels = None\n",
        "        else:\n",
        "            p_labels = labels[level]\n",
        "\n",
        "        zs = sample_level(zs, p_labels, sampling_kwargs[level], level, prior, total_length, hop_length, hps)\n",
        "\n",
        "        prior.cpu()\n",
        "        empty_cache()\n",
        "\n",
        "        # Decode sample\n",
        "        # x = prior.decode(zs[level:], start_level=level, bs_chunks=zs[level].shape[0])\n",
        "\n",
        "        # if dist.get_world_size() > 1:\n",
        "        #     logdir = f\"{hps.name}_rank_{dist.get_rank()}/level_{level}\"\n",
        "        # else:\n",
        "        #     logdir = f\"{hps.name}/level_{level}\"\n",
        "        # if not os.path.exists(logdir):\n",
        "        #     os.makedirs(logdir)\n",
        "        # t.save(dict(zs=zs, labels=labels, sampling_kwargs=sampling_kwargs, x=x), f\"{logdir}/data.pth.tar\")\n",
        "        # save_wav(logdir, x, hps.sr)\n",
        "        # if alignments is None and priors[-1] is not None and priors[-1].n_tokens > 0 and not isinstance(priors[-1].labeller, EmptyLabeller):\n",
        "        #     alignments = get_alignment(x, zs, labels[-1], priors[-1], sampling_kwargs[-1]['fp16'], hps)\n",
        "        # save_html(logdir, x, zs, labels[-1], alignments, hps)\n",
        "    return zs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfybdrZoN4z6"
      },
      "outputs": [],
      "source": [
        "# Generate ancestral samples given a list of artists and genres\n",
        "def ancestral_sample(labels, sampling_kwargs, priors, hps):\n",
        "    sample_levels = list(range(len(priors)))\n",
        "    zs = [t.zeros(1,0,dtype=t.long, device='cuda') for _ in range(len(priors))]\n",
        "    zs = _sample(zs, labels, sampling_kwargs, priors, sample_levels, hps)\n",
        "    return zs\n",
        "\n",
        "# Continue ancestral sampling from previously saved codes\n",
        "def continue_sample(zs, labels, sampling_kwargs, priors, hps):\n",
        "    sample_levels = list(range(len(priors)))\n",
        "    zs = _sample(zs, labels, sampling_kwargs, priors, sample_levels, hps)\n",
        "    return zs\n",
        "\n",
        "# Upsample given already generated upper-level codes\n",
        "def upsample(zs, labels, sampling_kwargs, priors, hps):\n",
        "    sample_levels = list(range(len(priors) - 1))\n",
        "    zs = _sample(zs, labels, sampling_kwargs, priors, sample_levels, hps)\n",
        "    return zs\n",
        "\n",
        "# Prompt the model with raw audio input (dimension: NTC) and generate continuations\n",
        "def primed_sample(x, labels, sampling_kwargs, priors, hps):\n",
        "    sample_levels = list(range(len(priors)))\n",
        "    zs = priors[-1].encode(x, start_level=0, end_level=len(priors), bs_chunks=x.shape[0])\n",
        "    # zs = priors[-1].encode(x,bs_chunks=x.shape[0])\n",
        "    # zs[1] = zs[1][:,:256]\n",
        "    zs[0] = zs[0][:,:1024]\n",
        "\n",
        "    # 8192 -> 1024 tokens (passed from previous audio)\n",
        "\n",
        "    zs = _sample(zs, labels, sampling_kwargs, priors, sample_levels, hps)\n",
        "    return zs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9_U8peyP3RP"
      },
      "outputs": [],
      "source": [
        "lower_level_chunk_size = 16\n",
        "lower_level_max_batch_size = 1\n",
        "chunk_size = 16\n",
        "max_batch_size = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bv0er4DlN42O"
      },
      "outputs": [],
      "source": [
        "sampling_kwargs = [dict(temp=1.5, fp16=False, chunk_size=lower_level_chunk_size, max_batch_size=lower_level_max_batch_size),\n",
        "                       dict(temp=1.5, fp16=False, chunk_size=lower_level_chunk_size, max_batch_size=lower_level_max_batch_size),\n",
        "                       dict(temp=1.5, fp16=False, chunk_size=chunk_size, max_batch_size=max_batch_size)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLc4mTq9FT8l"
      },
      "outputs": [],
      "source": [
        "def primed_sample_new(x, labels, priors, hps, fp16=False, temp=1.0, chunk_size=None, sample_tokens=None):\n",
        "    sample_levels = list(range(len(priors)))\n",
        "    zs = priors[-1].encode(x, bs_chunks=x.shape[0])\n",
        "    print(zs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdkC4WPtFT_X"
      },
      "outputs": [],
      "source": [
        "model_list = [model]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_to_wav(fname, aud, sr, j, input = False):\n",
        "    # clip before saving\n",
        "\n",
        "    print('converting the audio to wav and saving...')\n",
        "    aud = aud.detach()\n",
        "    aud = t.clamp(aud, -1, 1).cpu().numpy()\n",
        "    print(aud.shape)\n",
        "    for i in list(range(aud.shape[0])):\n",
        "        if(input):\n",
        "          soundfile.write(f'{fname}/item_input_{j}.wav', aud[i], samplerate=sr, format='wav')\n",
        "        else:\n",
        "          soundfile.write(f'{fname}/item_{j}.wav', aud[i], samplerate=sr, format='wav')"
      ],
      "metadata": {
        "id": "-IyulqspGzfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ancestral Sampling**"
      ],
      "metadata": {
        "id": "z46cpulpiTBR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tD1C-g5FN466",
        "outputId": "a2b603f4-8aa7-4c8c-ccb7-1d58f4ed463d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampling level 0\n",
            "new_tokens : 8250\n",
            "Sampling 8250 tokens for [0,8250]. Conditioning on 0 tokens\n",
            "z_conds at level : 0 : None\n",
            "8250\n",
            "converting the audio to wav and saving...\n",
            "(1, 264000, 1)\n",
            "\n",
            "Sampling level 0\n",
            "new_tokens : 8250\n",
            "Sampling 8250 tokens for [0,8250]. Conditioning on 0 tokens\n",
            "z_conds at level : 0 : None\n",
            "8250\n",
            "converting the audio to wav and saving...\n",
            "(1, 264000, 1)\n",
            "\n",
            "Sampling level 0\n",
            "new_tokens : 8250\n",
            "Sampling 8250 tokens for [0,8250]. Conditioning on 0 tokens\n",
            "z_conds at level : 0 : None\n",
            "8250\n",
            "converting the audio to wav and saving...\n",
            "(1, 264000, 1)\n",
            "\n",
            "Sampling level 0\n",
            "new_tokens : 8250\n",
            "Sampling 8250 tokens for [0,8250]. Conditioning on 0 tokens\n",
            "z_conds at level : 0 : None\n",
            "8250\n",
            "converting the audio to wav and saving...\n",
            "(1, 264000, 1)\n",
            "\n",
            "Sampling level 0\n",
            "new_tokens : 8250\n",
            "Sampling 8250 tokens for [0,8250]. Conditioning on 0 tokens\n",
            "z_conds at level : 0 : None\n",
            "8250\n",
            "converting the audio to wav and saving...\n",
            "(1, 264000, 1)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "save_path = '/content/drive/MyDrive/Indian_Classical_Music_Generation/samples/ancestral'\n",
        "\n",
        "# Ancestral Sampling\n",
        "for i in range(5):\n",
        "    z = ancestral_sample(None,\n",
        "                         sampling_kwargs,\n",
        "                         model_list,\n",
        "                         pr)\n",
        "\n",
        "    x_sample = model_list[0].decode(z, bs_chunks=1)\n",
        "\n",
        "    save_to_wav(save_path,\n",
        "                x_sample,\n",
        "                11200,\n",
        "                i)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Primed Sampling**"
      ],
      "metadata": {
        "id": "vgrS3LsSiXIm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oold9KL7t3fC"
      },
      "outputs": [],
      "source": [
        "# # Primed Sampling\n",
        "\n",
        "# count = 0\n",
        "# for i, x in enumerate(train_loader):\n",
        "#   if(count >= 5):\n",
        "#     break\n",
        "\n",
        "#   x = x.to('cuda', non_blocking=True)\n",
        "#   x_in = audio_preprocess(x, pr)\n",
        "\n",
        "#   z_enc, *z_conds = model_list[0].encode(x_in, bs_chunks=1)\n",
        "#   x_out = model_list[0].decode([z_enc,*z_conds],bs_chunks = 1)\n",
        "#   save_to_wav('/content/drive/My Drive/audio_VAE/final_sample/',x_out,11200,i + 5,input = True)\n",
        "\n",
        "#   # main sampling\n",
        "\n",
        "\n",
        "#   z = primed_sample(x_in,None,sampling_kwargs,model_list,up)\n",
        "#   x_sample = model_list[0].decode(z, bs_chunks=1)\n",
        "#   save_to_wav('/content/drive/My Drive/audio_VAE/final_sample/',x_sample,11200,i + 5)\n",
        "#   count += 1"
      ]
    }
  ]
}